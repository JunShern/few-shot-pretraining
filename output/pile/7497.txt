{
    "criteria": [
        {
            "criterion": "AllDocuments",
            "passed": true,
            "reason": "All documents pass."
        },
        {
            "criterion": "Domain",
            "passed": false,
            "reason": "Document source unavailable."
        },
        {
            "criterion": "QuestionAnswerStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "QuestionAnswerStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "FullyStructured",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ListPrefix",
            "passed": false,
            "reason": "Found 0 list prefixes. (Min: 5)"
        },
        {
            "criterion": "ListPrefixV2",
            "passed": true,
            "reason": "Text contains ['-', '-', '-', '-', '*', '-', '-', '-', '-', '*', '-', '-', '-', '-', '-', '-', '1', '2', '3', '*', '*', '*', '*', '*', '*', '*', '-', '-', '*', '-', '-', '*', '*', '*', '*', '*', '+', '*', '1', '2', '3', '-', '*', '*', '-', '*', '*', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-']."
        },
        {
            "criterion": "ExamplesMinimalEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesSynonymsEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesDiverseEmbed",
            "passed": true,
            "reason": "Text contains ['Because the CNN has been trained using numerous images, latent patterns in the AOG are supposed to consistently describe the same part region among different object images, instead of over-fitting to part annotations obtained during the QA process. (0.183)', 'The mined patterns are not over-fitted to the head annotation, but represent generic appearances of different tiger heads. (0.199)', 'In contrast, we believe that a good explanation and transparent representation of parts will create a new possibility of transferring part features. (0.194)', 'Without lots of training samples, \u201csimple\u201d methods are usually insensitive to the over-fitting problem. (0.189)', 'It is because all categories in the two datasets contain the head part. (0.189)', 'We have three reasons to explain the good performance of our method. (0.177)', 'Thus, these patterns reflected generic part appearances and did not over-fit to a few part annotations. (0.197)']."
        }
    ],
    "doc_id": "7497",
    "text": "---\nauthor:\n- 'Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun Zhu *Fellow, IEEE*'\nbibliography:\n- 'TheBib.bib'\ntitle: Mining Interpretable AOG Representations from Convolutional Networks via Active Question Answering\n---\n\n[Shell : Bare Demo of IEEEtran.cls for Computer Society Journals]{}\n\nIntroduction\n============\n\nConvolutional neural networks\u00a0[@CNN; @CNNImageNet; @ResNet; @DenseNet] (CNNs) have achieved superior performance in many visual tasks, such as object detection and segmentation. However, in real-world applications, current neural networks still suffer from low interpretability of their middle-layer representations and data-hungry learning methods.\n\nThus, the objective of this study is to mine thousands of *latent patterns* from the mixed representations in conv-layers. Each latent pattern corresponds to a constituent region or a contextual region of an object part. We use an interpretable graphical model, namely an And-Or graph (AOG), to organize latent patterns hidden in conv-layers. The AOG maps implicit latent patterns to explicit object parts, thereby explaining the hierarchical representation of objects. We use very few (*e.g.* 3\u201320) part annotations to mine latent patterns and construct the AOG to ensure high learning efficiency.\n\nAs shown in Fig.\u00a0\\[fig:rawMapToModel\\], compared to ordinary CNN representations where each filter encodes a mixture of textures and parts (evaluated by [@Interpretability]), we extract clear object-part representations from CNN features. Our weakly-supervised learning method enables people to model objects or object parts on-the-fly, thereby ensuring broad applicability.\n\n![image](rawMapToModel.pdf){width=\"\\linewidth\"}\n\n**And-Or graph representations:**[` `]{} As shown in Fig.\u00a0\\[fig:rawMapToModel\\], the AOG represents a semantic hierarchy on the top of conv-layers, which consists of four layers, *i.e.* the *semantic part*, *part templates*, *latent patterns*, to *CNN units*. In the AOG, AND nodes represent compositional regions of a part, and OR nodes represent a list of alternative template/deformation candidates for a local region.\n\n-   Layer 1: the top *semantic part* node is an OR node, whose children represent template candidates for the part.\n\n-   Layer 2: a *part template* in the second layer describes a certain part appearance with a specific pose, *e.g.* a black sheep head from a side view. A part template is an AND node, which uses its children latent patterns to encode its constituent regions.\n\n-   Layer 3: a *latent pattern* in the third layer represents a constituent region of a part (*e.g.* an eye in the head part) or a contextual region (*e.g.* the neck region *w.r.t.* the head). A latent pattern is an OR node, which naturally corresponds to a group of units within the feature map of a certain CNN filter. The latent pattern selects one of its children *CNN units* as the configuration of the geometric deformation.\n\n-   Layer 4: terminal nodes are *CNN units*, *i.e.* raw activation units on feature maps of a CNN filter.\n\nIn this hierarchy, the AOG maps implicit latent patterns in raw CNN feature maps to explicit semantic parts. We can use the AOG to localize object parts and their constituent regions for hierarchical object parsing. The AOG is interpretable and can be used for communications with human users.\n\n**Weakly-supervised learning via active question-answering:**[` `]{} We propose a new active learning strategy to build an AOG in a weakly-supervised manner. As shown in Fig.\u00a0\\[fig:QA\\], we use an active question-answering (QA) process to mine latent patterns from raw feature maps and gradually grow the AOG.\n\n![image](QA.pdf){width=\"0.99\\linewidth\"}\n\nThe input is a pre-trained CNN and its training samples (*i.e.* object images without part annotations). The QA method actively discovers the missing patterns in the current AOG and asks human users to label object parts for supervision.\n\nIn each step of the QA, we use the current AOG to localize a certain semantic part among all unannotated images. Our method actively identifies object images, which cannot fit well to the AOG. *I.e.* the current AOG cannot explain object parts in these images. Our method estimates the potential gain of asking about each of the unexplained objects, thereby determining an optimal sequence of questions for QA. Note that the QA is implemented based on pre-define ontology, instead of using open-ended questions or answers. As in Fig.\u00a0\\[fig:QA\\], the user is asked to provide five types of answers (*e.g.* labeling the correct part position when the AOG cannot accurately localize the part), in order to guide the growth of the AOG. Given each specific answer, our method may either refine the AOG branch of an existing part template or construct a new AOG branch for a new part template.\n\nBased on human answers, we mine latent patterns for new AOG branches as follows. We require the new latent patterns\n\n-   to represent a region highly related to the annotated object parts,\n\n-   to frequently appear in unannotated objects,\n\n-   to consistently keep stable spatial relationships with other latent patterns.\n\nSimilar requirements were originally proposed in studies of pursuing AOGs, which mined hierarchical object structures from Gabor wavelets on edges\u00a0[@MiningAOG] and HOG features\u00a0[@OurICCV15AoG]. We extend such ideas to feature maps of neural networks.\n\nThe active QA process mines object-part patterns from the CNN with fewer human supervision. There are three mechanisms to ensure the stability of weakly-supervised learning.\n\n-   Instead of learning all representations from scratch, we transfer patterns in a pre-trained CNN to the target object part, which boosts the learning efficiency. Because the CNN has been trained using numerous images, latent patterns in the AOG are supposed to consistently describe the same part region among different object images, instead of over-fitting to part annotations obtained during the QA process. For example, we use the annotation of a specific tiger head to mine latent patterns. The mined patterns are not over-fitted to the head annotation, but represent generic appearances of different tiger heads. In this way, we can use very few (*e.g.* 1\u20133) part annotations to extract latent patterns for each part template.\n\n-   It is important to maintain the generality of the pre-trained CNN during the learning procedure. *I.e.* we do not change/fine-tune the original convolutional weights within the CNN, when we grow new AOGs. This allows us to continuously learn new semantic parts from the same CNN, without the model drift.\n\n-   The active QA strategy reduces the excessive usage of the human labor of annotating object parts that have been well explained by the current AOG.\n\nIn addition, we use object-level annotations for pre-training, considering the following two facts: 1) Only a few datasets\u00a0[@SemanticPart; @CUB200] provide part annotations, and most benchmark datasets\u00a0[@PascalVOC; @ImageNet; @MSCOCO] mainly have annotations of object bounding boxes. 2) More crucially, real-world applications may focus on various object parts on-the-fly, and it is impractical to annotate a large number of parts for each specific task.\n\nThis paper makes the following three contributions.\n\n1\\) From the perspective of object representations, we semanticize a pre-trained CNN by mining reliable latent patterns from noisy feature maps of the CNN. We design an AOG to represent the semantic hierarchy inside conv-layers, which associates implicit neural patterns with explicit semantic parts.\n\n2\\) From the perspective of learning strategies, based on the clear semantic structure of the AOG, we present an active QA method to learn each part template of the object sequentially, thereby incrementally growing AOG branches on a CNN to enrich part representations in the AOG.\n\n3\\) In experiments, our method exhibits superior performance to other baselines of weakly-supervised part localization. For example, our methods with 11 part annotations outperformed fast-RCNNs with 60 annotations on the Pascal VOC Part dataset.\n\nA preliminary version of this paper appeared in [@CNNAoG] and [@DeepQA].\n\nRelated work\n============\n\n**CNN visualization:**[` `]{} Visualization of filters in a CNN is a direct way of exploring the pattern hidden inside a neural unit. Lots of visualization methods have been used in the literature.\n\nGradient-based visualization\u00a0[@CNNVisualization_1; @CNNVisualization_2; @CNNVisualization_3] estimates the input image that maximizes the activation score of a neural unit. Dosovitskiy\u00a0*et al.*\u00a0[@FeaVisual] proposed up-convolutional nets to invert feature maps of conv-layers to images. Unlike gradient-based methods, up-convolutional nets cannot mathematically ensure the visualization result reflects actual neural representations. In recent years, [@olah2017feature] provided a reliable tool to visualize filters in different conv-layers of a CNN.\n\nZhou\u00a0*et al.*\u00a0[@CNNSemanticDeep] proposed a method to accurately compute the image-resolution receptive field of neural activations in a feature map. Theoretically, the actual receptive field of a neural activation is smaller than that computed using the filter size. The accurate estimation of the receptive field is crucial to understand a filter\u2019s representations.\n\nUnlike network visualization, our mining part representations from conv-layers is another choice to interpret CNN representations.\n\n**Active network diagnosis:**[` `]{} Going beyond \u201cpassive\u201d visualization, some methods \u201cactively\u201d diagnose a pre-trained CNN to obtain insight understanding of CNN representations.\n\n[@CNNAnalysis_1] explored semantic meanings of convolutional filters. [@CNNAnalysis_2] evaluated the transferability of filters in intermediate conv-layers. [@CNNAnalysis_3; @CNNVisualization_5] computed feature distributions of different categories in the CNN feature space. Methods of [@visualCNN_grad; @visualCNN_grad_2] propagated gradients of feature maps *w.r.t.* the CNN loss back to the image, in order to estimate the image regions that directly contribute the network output. [@trust] proposed a LIME model to extract image regions that are used by a CNN to predict a label (or an attribute).\n\nNetwork-attack methods\u00a0[@pixelAttack; @CNNInfluence; @CNNAnalysis_1] diagnosed network representations by computing adversarial samples for a CNN. In particular, influence functions\u00a0[@CNNInfluence] were proposed to compute adversarial samples, provide plausible ways to create training samples to attack the learning of CNNs, fix the training set, and further debug representations of a CNN. [@banditUnknown] discovered knowledge blind spots (unknown patterns) of a pre-trained CNN in a weakly-supervised manner.\n\nZhang\u00a0*et al.*\u00a0[@CNNBias] developed a method to examine representations of conv-layers and automatically discover potential, biased representations of a CNN due to the dataset bias. Furthermore, [@wu2007compositional; @yang2009evaluating; @wu2011numerical] mined the local, bottom-up, and top-down information components in a model for prediction.\n\n**CNN semanticization:**[` `]{} Compared to the diagnosis of CNN representations, semanticization of CNN representations is closer to the spirit of building interpretable representations.\n\nHu\u00a0*et al.*\u00a0[@LogicRuleNetwork] designed logic rules for network outputs, and used these rules to regularize neural networks and learn meaningful representations. However, this study has not obtained semantic representations in intermediate layers. Some studies extracted neural units with certain semantics from CNNs for different applications. Given feature maps of conv-layers, Zhou\u00a0*et al.*\u00a0[@CNNSemanticDeep; @CNNSemanticDeep2] extracted scene semantics. Simon\u00a0*et al.* mined objects from feature maps of conv-layers\u00a0[@ObjectDiscoveryCNN_2], and learned explicit object parts\u00a0[@CNNSemanticPart].\n\nUnlike above research, we aim to explore the entire semantic hierarchy hidden inside conv-layers of a CNN. Because the AOG structure\u00a0[@MumfordAOG; @MiningAOG] is suitable for representing the semantic hierarchy of objects, our method uses an AOG to represent the CNN. In our study, we use semantic-level QA to incrementally mine object parts from the CNN and grow the AOG. Such a \u201cwhite-box\u201d representation of the CNN also guided further active QA. With clear semantic structures, the AOG makes it easier to transfer CNN patterns to other part-based tasks.\n\n**Unsupervised/active learning:**[` `]{} Many methods have been developed to learn object models in an unsupervised or weakly supervised manner. Methods of [@Gpt_WeaklyCNN; @WeaklyMIL; @OurICCV15AoG; @ObjectDiscoveryCNN_2] learned with image-level annotations without labeling object bounding boxes. [@UnsuperCNN; @ChoDiscovery] did not require any annotations during the learning process. [@OnlineMetric] collected training data online from videos to incrementally learn models. [@Language2VideoAlign; @Language2ActionAlign] discovered objects and identified actions from language Instructions and videos. Inspired by active learning\u00a0[@Active4; @i13; @Active2], the idea of learning from question-answering has been used to learn object models\u00a0[@KB_Fei_Annotation; @KB_Fei_InteractionLabel; @TuQA]. Branson\u00a0*et al.*\u00a0[@ActivePart] used human-computer interactions to label object parts to learn part models. Instead of directly building new models from active QA, our method uses the QA to mine AOG part representations from CNN representations.\n\n**AOG for knowledge transfer:** Transferring hidden patterns in the CNN to other tasks is important for neural networks. Typical research includes end-to-end fine-tuning and transferring CNN representations between different categories\u00a0[@CNNAnalysis_2; @CNNSemantic] or datasets\u00a0[@UnsuperTransferCNN]. In contrast, we believe that a good explanation and transparent representation of parts will create a new possibility of transferring part features. As in [@AllenAoG; @MiningAOG], the AOG is suitable to represent the semantic hierarchy, which enables semantic-level interactions between human and neural networks.\n\n**Modeling \u201cobjects\u201d vs. modeling \u201c**parts**\u201d in un-/weakly-supervised learning:**[` `]{} Generally speaking, in the scenario of un-/weakly-supervised learning, it is usually more difficult to model object parts than to represent entire objects. For example, object discovery\u00a0[@ObjectDiscoveryCNN_1; @ObjectDiscoveryCNN_2; @ObjectDiscoveryCNN_3] and co-segmentation\u00a0[@InteractiveCoseg] only require image-level labels without object bounding boxes. Object discovery is mainly implemented by identifying common foreground patterns from the noisy background. People usually consider closed boundaries and common object structure as a strong prior for object discovery.\n\nIn contrast to objects, it is difficult to mine true part parsing of objects without sufficient supervision. Up to now, there is no reliable solution to distinguishing semantically meaningful parts from other potential divisions of object parts in an unsupervised manner. In particular, some parts (*e.g.* the abdomen) do not have shape boundaries to determine their shape extent.\n\n**Part localization/detection vs. semanticizing CNN patterns:** There are two key points to differentiate our study from conventional part-detection approaches. First, most detection methods deal with classification problems, but inspired by graph mining\u00a0[@OurICCV15AoG; @OurSAPPAMI; @OurCVPR14Graph], we mainly focus on a mining problem. *I.e.* we aim to discover meaningful latent patterns to clarify CNN representations. Second, instead of summarizing common knowledge from massive annotations, our method requires very limited supervision to mine latent patterns.\n\nMethod\n======\n\nThe overall objective is to sequentially minimize the following three loss terms. $${Loss}={Loss}^{\\textrm{CNN}}+{Loss}^{\\textrm{QA}}+{Loss}^{\\textrm{AOG}}\n\\label{eqn:obj}$$ ${Loss}^{\\textrm{CNN}}$ denotes the classification loss of the CNN.\n\n${Loss}^{\\textrm{QA}}$ is referred as to the loss for active QA. Given the current AOG, we use ${Loss}^{\\textrm{QA}}$ to actively determine a sequence of questions about objects that cannot be explained by the current AOG, and require people to annotate bounding boxes of new object parts for supervision.\n\n${Loss}^{\\textrm{AOG}}$ is designed to learn an AOG for the CNN. ${Loss}^{\\textrm{AOG}}$ penalizes 1) the incompatibility between the AOG and CNN feature maps of unannotated objects and 2) part-location errors *w.r.t.* the annotated ground-truth part locations.\n\nIt is essential to determine the optimization sequence for the three losses in the above equation. We propose to first learn the CNN by minimizing ${Loss}^{\\textrm{CNN}}$ and then build an AOG based on the learned CNN. We use the active QA to obtain new part annotations and use new part annotations to grow the AOG by optimizing ${Loss}^{\\textrm{QA}}$ and ${Loss}^{\\textrm{AOG}}$ alternatively.\n\nWe introduce details of the three losses in the following subsections.\n\nLearning convolutional neural networks\n--------------------------------------\n\nTo simplify the story, in this research, we just consider a CNN for single-category classification, *i.e.* identifying object images of a specific category from random images. We use the log logistic loss to learn the CNN. $${Loss}^{\\textrm{CNN}}=\\mathbb{E}_{I\\in{\\bf I}}\\big[{Loss}(\\hat{y}_{I},y^{*}_{I})\\big]$$ where $\\hat{y}_{I}$ and $y^{*}_{I}$ denote the predicted and ground-truth labels of an image $I$. If the image $I$ belongs to the target category, then $y^{*}_{I}=+1$; otherwise $y^{*}_{I}=-1$.\n\nLearning And-Or graphs\n----------------------\n\nWe are given a pre-trained CNN and its training images without part annotations. We use an active QA process to obtain a small number of annotations of object-part bounding boxes, which will be introduced in Section\u00a0\\[sec:QA\\]. Based on these inputs, in this subsection, we focus on the approach for learning an AOG to represent the object part.\n\n### And-Or graph representations\n\nBefore the introduction of learning AOGs, we first briefly overview the structure of the AOG and the part parsing (inference) based on the AOG.\n\nAs shown in Fig.\u00a0\\[fig:rawMapToModel\\], an AOG represents the semantic structure of a part at four layers.\n\n   Layer  Name                Node type\n  ------- ---------------- ---------------\n     1    semantic part        OR node\n     2    part template       AND node\n     3    latent pattern       OR node\n     4    neural unit       Terminal node\n\nIn the AOG, each OR node encodes a list of alternative appearance (or deformation) candidates as children. Each AND node uses its children to represent its constituent regions.\n\nMore specifically, the top node is an OR node, which represents a certain semantic part, *e.g.* the head or the tail. The semantic part node encodes some part templates as children. Each part template corresponds to a specific part appearance from a certain perspective. During the inference process, the semantic part (an OR node) selects the best part template among all template candidates to represent the object.\n\nThe part template in the second layer is an AND node, which uses its children latent patterns to represent a constituent region or a contextual region *w.r.t.* the part template. The part template encodes spatial relationships between its children.\n\nThe latent pattern in the third layer is an OR node, whose receptive field is a square block within the feature map of a specific convolutional filter. The latent pattern takes neural units inside its receptive field as children. Because the latent pattern may appear at different locations in the feature map, the latent pattern uses these neural units to represent its deformation candidates. During the inference process, the latent pattern selects the strongest activated child unit as its deformation configuration.\n\nGiven an image $I$[^1], we use the CNN to compute feature maps of all conv-layers on image $I$. Then, we can use the AOG for hierarchical part parsing. *I.e.* we use the AOG to semanticize the feature maps and localize the target part and its constituent regions in different layers.\n\nThe parsing result is illustrated as red lines in Fig.\u00a0\\[fig:rawMapToModel\\]. From a top-down perspective, the parsing procedure 1) identifies a part template for the semantic part; 2) parses an image region for the selected part template; 3) for each latent pattern under the part template, it selects a neural unit within a specific deformation range to represent this pattern.\n\n**OR nodes:** Both the top semantic-part node and latent-pattern nodes in the third layer are OR nodes. The parsing process assigns each OR node $u$ with an image region $\\Lambda_{u}$ and an inference score $S_{u}$. $S_{u}$ measures the fitness between the parsed region $\\Lambda_{u}$ and the sub-AOG under $u$. The computation of $\\Lambda_{u}$ and $S_{u}$ for all OR nodes shares the same paradigm. $$S_{u}=\\max_{v\\in Child(u)}S_{v},\\qquad\\Lambda_{u}=\\Lambda_{\\hat{v}}$$ where let $u$ have $m$ children nodes $Child(u)=\\{v_{1},v_{2},\\ldots,v_{m}\\}$. $S_{v}$ denotes the inference score of the child $v$, and $\\Lambda_{v}$ is referred to as the image region assigned to $v$. The OR node selects the child with the highest score $\\hat{v}={\\arg\\!\\max}_{v\\in Child(u)}S_{v}$ as the true parsing configuration. Node $\\hat{v}$ propagates its image region to the parent $u$.\n\nMore specifically, we introduce detailed settings for different OR nodes.\n\n-   The OR node of the top semantic part contains a list of alternative part templates. We use $top$ to denote the top node of the semantic part. The semantic part chooses a part template to describe each input image $I$.\n\n-   The OR node of each latent pattern $u$ in the third layer naturally corresponds to a square deformation range within the feature map of a convolutional filter of a conv-layer. All neural units within the square are used as deformation candidates of the latent pattern. For simplification, we set a constant deformation range (with a center $\\overline{{\\bf p}}_{u}$ and a scale of $\\frac{h}{3}\\times\\frac{w}{3}$ in the feature map where $h$ and $w$ ($h=w$) denote the height and width of the feature map) for each latent pattern. $\\overline{{\\bf p}}_{u}$ is a parameter that needs to be learned. Deformation ranges of different patterns in the same feature map may overlap. Given parsing configurations of children neural units as input, the latent pattern selects the child with the highest inference score as the true deformation configuration.\n\n**AND nodes:** Each part template is an AND node, which uses its children (latent patterns) to represent its constituent or contextual regions. We use $v$ and $Child(v)=\\{u_{1},u_{2},\\ldots,u_{m}\\}$ to denote the part template and its children latent patterns. We learn the average displacement from $\\Lambda_{u}$ to $\\Lambda_{v}$ among different image, denoted by $\\Delta{\\bf p}_{u}$, as a parameter of the AOG. Given parsing results of children latent patterns, we use the image region of each child node $\\Lambda_{u}$ to infer the region for the parent $v$ based on its spatial relationships. Just like a deformable part model, the parsing of $v$ can be given as $$S_{v}\\!=\\!\\!\\!\\!\\!\\!\\!\\sum_{u\\in Child(v)}\\!\\!\\!\\!\\!\\!\\!\\big[S_{u}\\!+\\!S^{\\textrm{inf}}(\\Lambda_{u}|\\Lambda_{v})\\big],\\;\\;\\Lambda_{v}\\!=\\!f(\\Lambda_{u_{1}},\\ldots,\\Lambda_{u_{m}})\\!$$ where we use parsing results of children nodes to infer the parent part template $v$. $S^{\\textrm{inf}}(\\Lambda_{u}|\\Lambda_{v})$ denotes the spatial compatibility between $\\Lambda_{u}$ and $\\Lambda_{v}$ *w.r.t.* their average displacement $\\Delta{\\bf p}_{u}$. Please see the appendix for details of $S^{\\textrm{inf}}(\\Lambda_{u}|\\Lambda_{v})$.\n\nFor the region parsing of the part template $v$, we need to estimate two terms, *i.e.* the center position ${\\bf p}_{v}$ and the scale $scale_{v}$ of $\\Lambda_{v}$. We learn a fixed scale for each part template, which will be introduced in Section\u00a0\\[sec:learnAOG\\]. In this way, we can simply implement region parsing by computing the region position that maximizes the inference score ${\\bf p}_{v}=f(\\Lambda_{u_{1}},\\Lambda_{u_{2}},\\ldots,\\Lambda_{u_{m}})={\\arg\\!\\max}_{{\\bf p}_{v}}S_{v}$.\n\n**Terminal nodes (neural units):** Each terminal node under a latent pattern represents a deformation candidate of the latent pattern. The terminal node has a fixed image region, *i.e.* we propagate the neural unit\u2019s receptive field back to the image plane as its image region. We compute a neural unit\u2019s inference score based on both its neural response value and its displacement *w.r.t.* its parent latent pattern. Please see the appendix for details.\n\nBased on the above node definitions, we can use the AOG to parse each given image $I$ by dynamic programming in a bottom-up manner.\n\n### Learning And-Or graphs {#sec:learnAOG}\n\nThe core of learning AOGs is to distinguish reliable latent patterns from noisy neural responses in conv-layers and select reliable latent patterns to construct the AOG.\n\n**Training data:**[` `]{} Let ${\\bf I}^{\\textrm{obj}}\\subset{\\bf I}$ denote the set of object images of a target category. During the active question-answering, we obtain bounding boxes of the target object part in a small number of images, ${\\bf I}^{\\textrm{ant}}\\!=\\!\\{I_1,I_2,\\ldots,I_{M}\\}\\subset{\\bf I}^{\\textrm{obj}}$ among all objects. The other images without part annotations are denoted by ${\\bf I}^{\\textrm{unant}}={\\bf I}^{\\textrm{obj}}\\setminus{\\bf I}^{\\textrm{ant}}$. In addition, the question-answering process collects a number of part templates. Thus, for each image $I\\in{\\bf I}^{\\textrm{ant}}$, we annotate $(\\Lambda_{top}^{*},v^{*})$, where $\\Lambda_{top}^{*}$ denotes the ground-truth bounding box of the part in $I$, and $v^{*}\\in Child(top)$ specifies the ground-truth template for the part.\n\n**Which AOG parameters to learn:**[` `]{} We can use human annotations to define the first two layers of the AOG. If human annotators specify a total of $m$ different part templates during the annotation process, correspondingly, we can directly connect the top node with $m$ part templates as children. For each part template $v\\in Child(top)$, we fix a constant scale for its region $\\Lambda_{v}$. *I.e.* if there are $n$ ground-truth part boxes that are labeled for $v$, we compute the average scale among the $n$ part boxes as the constant scale $scale_{v}$.\n\nThus, the key to AOG construction is to mine children latent patterns for each part template $v$. We need to mine latent patterns from a total of $K$ conv-layers. We select $n_{k}$ latent patterns from the $k$-th ($k=1,2,\\ldots,K$) conv-layer, where $K$ and $\\{n_{k}\\}$ are hyper-parameters. Let each latent pattern $u$ in the $k$-th conv-layer correspond to a square deformation range, which is located in the $D_{u}$-th slice of the conv-layer\u2019s feature map. $\\overline{\\bf p}_{u}$ denotes the center of the range. As analyzed in the appendix, we only need to estimate the parameters of $D_{u},\\overline{\\bf p}_{u}$ for $u$.\n\n**How to learn:**[` `]{} Just like the pattern pursuing in Fig.\u00a0\\[fig:rawMapToModel\\], we mine the latent patterns by estimating their best locations $D_{u},\\overline{\\bf p}_{u}\\in{\\boldsymbol\\theta}$ that maximize the following objective function, where ${\\boldsymbol\\theta}$ denotes the parameter set of the AOG. $$\\begin{split}\n{Loss}^{\\textrm{AOG}}=\\mathbb{E}_{I\\in{\\bf I}^{\\textrm{ant}}}\\big[-S_{top}+L(\\Lambda_{top},\\Lambda_{top}^{*})\\big]\\qquad\\\\\n+\\lambda^{\\textrm{unant}}\\mathbb{E}_{I\\in{\\bf I}^{\\textrm{obj}}}\\big[-S^{\\textrm{unant}}_{\\textrm{AOG}}+L^{\\textrm{unant}}({\\boldsymbol\\Lambda}_{\\textrm{AOG}})\\big]\n\\end{split}\n\\label{eqn:LossAOG}$$ First, let us focus on the first half of the equation, which learns from part annotations. $S_{top}$ and $L(\\Lambda_{top},\\Lambda_{top}^{*})$ denote the final inference score of the AOG on image $I$ and the loss of part localization, respectively. Given annotations $(\\Lambda_{top}^{*},v^{*})$ on $I$, we get $$\\begin{split}\n&S_{top}=\\max_{v\\in Child(top)}S_{v}\\approx S_{v^{*}}\\\\\n&L(\\Lambda_{top},\\Lambda_{top}^{*})=-\\lambda_{v^{*}}\\Vert{\\bf p}_{top}-{\\bf p}^{*}_{top}\\Vert\n\\end{split}$$ where we approximate the ground-truth part template $v^{*}$ as the selected part template. We ignore the small probability of the AOG assigning an annotated image with an incorrect part template to simplify the computation. The part-localization loss $L(\\Lambda_{top},\\Lambda_{top}^{*})$ measures the localization error between the parsed part region ${\\bf p}_{top}$ and the ground truth ${\\bf p}^{*}_{top}={\\bf p}(\\Lambda_{top}^{*})$.\n\nThe second half of Equation\u00a0(\\[eqn:LossAOG\\]) learns from objects without part annotations. $$\\begin{split}\nS^{\\textrm{unant}}_{\\textrm{AOG}}&={\\sum}_{u\\in Child(v^{*})}S^{\\textrm{unant}}_{u}\\\\\nL^{\\textrm{unant}}({\\boldsymbol\\Lambda}_{\\textrm{AOG}})&={\\sum}_{u\\in Child(v^{*})}\\lambda^{\\textrm{close}}\\Vert\\Delta{\\bf p}_{u}\\Vert^2\n\\end{split}\n\\label{sec:unsuper}$$ where the first term $S^{\\textrm{unant}}_{\\textrm{AOG}}$ denotes the inference score at the level of latent patterns without ground-truth annotations of object parts. Please see the appendix for the computation of $S^{\\textrm{unant}}_{u}$. The second term $L^{\\textrm{unant}}({\\boldsymbol\\Lambda}_{\\textrm{AOG}})$ penalizes latent patterns that are far from their parent $v^{*}$. This loss encourages the assigned neural unit to be close to its parent latent pattern. We assume that 1) latent patterns that frequently appear among unannotated objects may potentially represent stable part appearance and should have higher priorities; and that 2) latent patterns spatially closer to their parent part templates are usually more reliable.\n\nWhen we set $\\lambda_{v^{*}}$ to a constant $\\lambda^{\\textrm{inf}}\\sum_{k=1}^{K}n_{k}$, we can transform the learning objective in Equation\u00a0(\\[eqn:LossAOG\\]) as follows. $$\\forall v\\in Child(top), \\quad\\max_{{\\boldsymbol\\theta}_{v}}{\\bf L}_{v},\\quad {\\bf L}_{v}\\!=\\!\\!\\!\\!\\!\\!\\!\\!\\sum_{u\\in Child(v)}\\!\\!\\!\\!\\!\\!\\!Score(u)\n\\label{eqn:subAOG}$$ where [$Score(u)\\!=\\!\\mathbb{E}_{I\\in{\\bf I}_{v}}[S_{u}+S^{\\textrm{inf}}(\\Lambda_{u}|\\Lambda^{*}_{v})]$ $+\\mathbb{E}_{I'\\in{\\bf I}^{\\textrm{obj}}}$ $\\lambda^{\\textrm{unant}}[S^{\\textrm{unant}}_{u}-\\lambda^{\\textrm{close}}\\Vert\\Delta{\\bf p}_{u}\\Vert^2]$]{}. ${\\boldsymbol\\theta}_{v}\\subset{\\boldsymbol\\theta}$ denotes the parameters for the sub-AOG of the part template $v$. We use ${\\bf I}_{v}\\subset{\\bf I}^{\\textrm{ant}}$ to denote the subset of images that are annotated with $v$ as the ground-truth part template.\n\n**Learning the sub-AOG for each part template:**[` `]{} Based on Equation\u00a0(\\[eqn:subAOG\\]), we can mine the sub-AOG for each part template $v$, which uses this template\u2019s annotations on images $I\\in{\\bf I}_{v}\\subset{\\bf I}^{\\textrm{ant}}$, as follows.\n\n1\\) We first enumerate all possible latent patterns corresponding to the $k$-th CNN conv-layer ($k=1,\\ldots,K$), by sampling all pattern locations *w.r.t.* $D_{u}$ and $\\overline{\\bf p}_{u}$.\n\n2\\) Then, we sequentially compute $\\Lambda_{u}$ and $Score(u)$ for each latent pattern.\n\n3\\) Finally, we sequentially select a total of $n_{k}$ latent patterns. In each step, we select $\\hat{u}\\!=\\!{\\arg\\!\\max}_{u\\in Child(v)}\\Delta{\\bf L}_{v}$. *I.e.* we select latent patterns with top-ranked values of [$Score(u)$]{} as children of part template $v$.\n\nLearning via active question-answering {#sec:QA}\n--------------------------------------\n\nWe propose a new learning strategy, *i.e.* active QA, which is more efficient than conventional batch learning. The QA-based learning algorithm actively detects blind spots in feature representations of the model and ask questions for supervision. In general, blind spots in the AOG include 1) neural-activation patterns in the CNN that have not been encoded in the AOG and 2) inaccurate latent patterns in the AOG. The unmodeled neural patterns potentially reflect new part templates, while inaccurate latent patterns correspond to sub-optimized part templates.\n\nAs an interpretable representation of object parts, the AOG can represent blind spots using linguistic description. We design five types of answers to project these blind spots onto semantic details of objects. Our method selects and asks a series of questions. We then collect answers from human users, in order to incrementally grow new AOG branches to explain new part templates and refine existing AOG branches of part templates.\n\nOur approach repeats the following QA process. As shown in Fig.\u00a0\\[fig:QA\\], at first, we use the current AOG to localize object parts on all unannotated objects of a category. Based on localization results, the algorithm selects and asks about the object $I$, from which the AOG can obtain the most information gain. A question [$q\\!=\\!(I,\\hat{v},\\Lambda_{\\hat{v}})$]{} requires people to determine whether our approach predicts the correct part template $\\hat{v}$ and parses a correct region $\\Lambda_{top}=\\Lambda_{\\hat{v}}$ for the part. Our method expects one of the following answers.\n\n**Answer 1:** the part detection is correct. **Answer 2:** the current AOG predicts the correct part template in the parse graph, but it does not accurately localize the part. **Answer 3:** neither the part template nor the part location is correctly estimated. **Answer 4:** the part belongs to a new part template. **Answer 5:** the target part does not appear in the image. In particular, in case of receiving Answers\u00a02\u20134, our method will ask people to annotate the target part. In case of getting Answer 3, our method will require people to specify its part template and whether the object is flipped. Our method uses new part annotations to refine (for Answers 2\u20133) or create (for Answer 4) an AOG branch of the annotated part template based on Equation\u00a0(\\[eqn:LossAOG\\]).\n\n### Question ranking\n\nThe core of the QA-based learning is to select a sequence of questions that reduce the uncertainty of part localization the most. Therefore, in this section, we design a loss function to measure the incompatibility between the AOG and real part appearances in object samples. Our approach predicts the potential gain (decrease of the loss) of asking about each object. Objects with large gains usually correspond to not well explained CNN neural activations. Note that annotating a part in an object may also help localize parts on other objects, thereby leading to a large gain. Thus, we use a greedy strategy to select a sequence of questions [$\\Omega=\\{q_{i}|i=1,2,\\ldots\\}$]{}, *i.e.* asking about the object that produces the most gain in each step.\n\nFor each object image $I$, we use [${\\bf P}(y|I)$]{} and [${\\bf Q}(y|I)$]{} to denote the prior distribution and the estimated distribution of an object part on $I$, respectively. A label [$y\\in\\{+1,-1\\}$]{} indicates whether $I$ contains the target part. The AOG estimates the probability of object $I$ containing the target part as [${\\bf Q}(y\\!=\\!+1|I)\\!=\\!\\frac{1}{Z}\\exp[\\beta S_{top}]$]{}, where $Z$ and $\\beta$ are parameters for scaling (see Section\u00a0\\[sec:implement\\] for details); [${\\bf Q}(y=-1|I)\\!=\\!1-{\\bf Q}(y=+1|I)$]{}. Let [${\\bf I}^{\\textrm{ant}}$]{} denote the set of objects without being asked during previous QA. For each asked object [$I\\in{\\bf I}^{\\textrm{ant}}$]{}, we set its prior distribution [${\\bf P}(y=+1|I)=1$]{} if $I$ contains the target part; [${\\bf P}(y=+1|I)=0$]{} otherwise. For each un-asked object [$I\\in{\\bf I}^{\\textrm{unant}}$]{}, we set its prior distribution based on statistics of previous answers, [${\\bf P}(y=+1|I)=\\mathbb{E}_{I'\\in{\\bf I}^{\\textrm{ant}}}{\\bf P}(y=+1|I')$]{}. Therefore, we formulate the loss function as the KL divergence between the prior distribution [${\\bf P}$]{} and the estimated distribution [${\\bf Q}$]{}. $$\\begin{split}\n\\!\\!{Loss}^{\\textrm{QA}}\\!\\!\\!=\\!{\\bf KL}({\\bf P}\\Vert{\\bf Q})\\!=\\!&\\sum_{I\\in{\\bf I}^{\\textrm{obj}}}\\sum_{y}{\\bf P}(y,I)\\log\\frac{{\\bf P}(y,I)}{{\\bf Q}(y,I)}\\!\\!\\!\\!\\!\\!\\!\\\\\n=&\\lambda\\sum_{I\\in{\\bf I}^{\\textrm{obj}}}\\sum_{y}{\\bf P}(y|I)\\log\\frac{{\\bf P}(y|I)}{{\\bf Q}(y|I)}\n\\end{split}$$ where [${\\bf P}(y,I)\\!=\\!{\\bf P}(y|I)P(I)$; ${\\bf Q}(y,I)\\!=\\!{\\bf Q}(y|I)P(I)$; $\\lambda=P(I)\\!=\\!1/\\vert{\\bf I}^{\\textrm{obj}}\\vert$]{} is a constant prior probability for $I$.\n\nWe keep modifying both the prior distribution ${\\bf P}$ and the estimated distribution ${\\bf Q}$ during the QA process. Let the algorithm select an unannotated object [$\\tilde{I}\\in{\\bf I}^{\\textrm{unant}}={\\bf I}^{\\textrm{obj}}\\setminus{\\bf I}^{\\textrm{ant}}$]{} and ask people to label its part. The annotation would encode part representations of $\\tilde{I}$ into the AOG and significantly change the estimated distribution for objects that are similar to $\\tilde{I}$. For each object $I'\\in{\\bf I}^{\\textrm{obj}}$, we predict its estimated distribution after a new part annotation as $$\\begin{split}\n\\tilde{\\bf Q}(y=+1|I')=&\\frac{1}{Z}\\exp[\\beta S_{top,I'}^{\\textrm{new}}|_{\\tilde{I}}]\\\\\nS_{top,I'}^{\\textrm{new}}|_{\\tilde{I}}=&S_{top,I'}+\\Delta S_{top,\\tilde{I}}e^{-\\alpha\\cdot dist(I',\\tilde{I})}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n\\end{split}\n\\label{eqn:predict}$$ where $S_{top,I'}$ indicates the current AOG\u2019s inference score of $S_{top}$ on image $I'$. $S_{top,I'}^{\\textrm{new}}|_{\\tilde{I}}$ denotes the predicted inference score of $I'$ when people annotate $\\tilde{I}$. We assume that if object $I'$ is similar to object $\\tilde{I}$, the inference score of $I'$ will have an increase similar to that of $\\tilde{I}$. [$\\Delta S_{top,\\tilde{I}}\\!=\\!\\mathbb{E}_{I\\in{\\bf I}^{\\textrm{ant}}}S_{top,I}-S_{top,\\tilde{I}}$]{} denotes the score increase of $\\tilde{I}$. $\\alpha$ is a scalar weight. We formulate the appearance distance between $I'$ and $\\tilde{I}$ as [$dist(I',\\tilde{I})\\!=\\!1-\\frac{\\phi(I')^{T}\\phi(\\tilde{I})}{\\vert\\phi(I')\\vert\\cdot\\vert\\phi(\\tilde{I})\\vert}$]{}, where [$\\phi(I')\\!=\\!{\\bf M}\\,{\\bf f}_{I'}$]{}. ${\\bf f}_{I'}$ denotes features of $I'$ at the top conv-layer after ReLU operation, and ${\\bf M}$ is a diagonal matrix representing the prior reliability for each feature dimension[^2]. In addition, if $I'$ and $\\tilde{I}$ are assigned with different part templates by the current AOG, we set an infinite distance between $I'$ and $\\tilde{I}$ to achieve better performance. Based on Equation\u00a0(\\[eqn:predict\\]), we can predict the changes of the KL divergence after the new annotation on $\\tilde{I}$ as $$\\Delta{\\bf KL}(\\tilde{I})=\\lambda{\\sum}_{I\\in{\\bf I}^{\\textrm{obj}}}{\\sum}_{y}{\\bf P}(y|I)\\log\\frac{\\tilde{\\bf Q}(y|I)}{{\\bf Q}(y|I)}\n\\label{eqn:delta}$$ Thus, in each step, our method selects and asks about the object that decreases the KL divergence the most. $$\\hat{I}={\\arg\\!\\max}_{I\\in{\\bf I}^{\\textrm{unant}}}\\Delta{\\bf KL}(I)\n\\label{eqn:select}$$\n\n**QA implementations:**[` `]{} In the beginning, for each object $I$, we initialize [${\\bf P}(y\\!=\\!+1|I)\\!=\\!1$]{} and [${\\bf Q}(y\\!=\\!+1|I)\\!=\\!0$]{}. Then, our approach selects and asks about an object $\\hat{I}$ based on Equation\u00a0(\\[eqn:select\\]). We use the answer to update ${\\bf P}$. If a new object part is labeled during the QA process, we apply Equation\u00a0(\\[eqn:LossAOG\\]) to update the AOG. More specifically, if people label a new part template, our method will grow a new AOG branch to encode this template. If people annotate a part for an old part template, our method will update its corresponding AOG branch. Then, we compute the new distribution [${\\bf Q}$]{} based on the new AOG. In this way, the above QA procedure gradually grows the AOG.\n\nExperiments\n===========\n\nImplementation details {#sec:implement}\n----------------------\n\nWe used a 16-layer VGG network (VGG-16)\u00a0[@VGG], which was pre-trained for object classification using 1.3M images in the ImageNet ILSVRC 2012 dataset\u00a0[@ImageNet]. Then, for each testing category, we further fine-tune the VGG-16 using object images in this category to classify target objects from random images. We selected the last nine conv-layers of VGG-16 as valid conv-layers. We extracted neural units from these conv-layers to build the AOG.\n\n**Active question-answering:** Three parameters were involved in our active-QA method, *i.e.* $\\alpha$, $\\beta$, and $Z$. Because most objects of the category contained the target part, we ignored the small probability of ${\\bf P}(y=-1|I)$ in Equation\u00a0(\\[eqn:delta\\]) to simplify the computation. As a result, $Z$ was eliminated in Equation\u00a0(\\[eqn:delta\\]), and the constant weight $\\beta$ did not affect object-selection results in Equation\u00a0(\\[eqn:select\\]). We set $\\alpha=4.0$ in our experiments.\n\n**Learning AOGs:** Multiple latent patterns corresponding to the same convolutional filter may have similar positions $\\overline{\\bf p}_{u}$, and their deformation ranges may highly overlap. Thus, we selected the latent pattern with the highest $Score(u)$ within a small range of $\\epsilon\\times\\epsilon$ in the filter\u2019s feature map and removed other nearby patterns to obtain a spare AOG. Besides, for each part template $v$, we estimated $n_{k}$ latent patterns in the $k$-th conv-layer. We assumed that scores of all latent patterns in the $k$-th conv-layer follow the distribution of [$Score(u)\\sim\\alpha\\exp[-(\\xi\\cdot{rank})^{0.5}]+\\gamma$]{}, where $rank$ denotes the score rank of [$u$]{}. We set [$n_{k}=\\lceil0.5/\\xi\\rceil$]{}, which learned the best AOG.\n\nDatasets\n--------\n\nBecause evaluation of part localization requires ground-truth annotations of part positions, we used the following three benchmark datasets to test our method, *i.e.* the PASCAL VOC Part Dataset\u00a0[@SemanticPart], the CUB200-2011 dataset\u00a0[@CUB200], and the ILSVRC 2013 DET Animal-Part dataset\u00a0[@CNNAoG]. Just like in [@SemanticPart; @CNNAoG], we selected animal categories, which prevalently contain non-rigid shape deformation, for testing. *I.e.* we selected six animal categories\u2014*bird, cat, cow, dog, horse*, and *sheep*\u2014from the PASCAL Part Dataset. The CUB200-2011 dataset contains 11.8K images of 200 bird species. We followed [@ActivePart; @CNNSemanticPart; @CNNAoG] and used all these images as a single bird category for learning. The ILSVRC 2013 DET Animal-Part dataset\u00a0[@CNNAoG] contains part annotations of 30 animal categories among all the 200 categories in the ILSVRC 2013 DET dataset\u00a0[@ImageNet].\n\n![image](energyCurve.pdf){width=\"\\linewidth\"}\n\nThe 2nd column shows the number of part annotations for training. The 3rd column indicates whether the baseline used all object-box annotations in the category to pre-fine-tune a CNN before learning the part (*object-box annotations are more than part annotations*).\n\nBaselines\n---------\n\nWe used the following thirteen baselines for comparison. The first two baselines were based on the Fast-RCNN\u00a0[@FastRCNN]. We fine-tuned the fast-RCNN with a loss of detecting a single class/part for a fair comparison. The first baseline, namely *Fast-RCNN (1 ft)*, fine-tuned the VGG-16 using part annotations to detect parts on well-cropped objects. To enable a more fair comparison, we conducted the second baseline based on two-stage fine-tuning, namely *Fast-RCNN (2 fts)*. This baseline first fine-tuned the VGG-16 using numerous object-box annotations in the target category, and then fine-tuned the VGG-16 using a few part annotations.\n\nThe third baseline was proposed in [@CNNSemanticPart], namely *CNN-PDD*. *CNN-PDD* selected a filter in a CNN (pre-trained using ImageNet ILSVRC 2012 dataset) to represent the part on well-cropped objects. Then, we slightly extended [@CNNSemanticPart] as the fourth baseline *CNN-PDD-ft*. *CNN-PDD-ft* first fine-tuned the VGG-16 using object bounding boxes, and then applied [@CNNSemanticPart] to learn object parts.\n\nThe strongly supervised DPM (*SS-DPM-Part*)\u00a0[@SSDPM] and the approach of [@PLDPM] (*PL-DPM-Part*) were the fifth and sixth baselines. These methods learned DPMs for part localization. The graphical model proposed in [@SemanticPart] was selected as the seventh baseline, namely *Part-Graph*. The eighth baseline was the interactive learning for part localization\u00a0[@ActivePart] (*Interactive-DPM*).\n\nWithout lots of training samples, \u201csimple\u201d methods are usually insensitive to the over-fitting problem. Thus, we designed the last four baselines as follows. We first fine-tuned the VGG-16 using object bounding boxes, and collected image patches from cropped objects based on the selective search\u00a0[@SelectiveSearch]. We used the VGG-16 to extract *fc7* features from image patches. The two baselines (*i.e.* *fc7+linearSVM* and *fc7+RBF-SVM*) used a linear SVM and an RBF-SVM, respectively, to detect object parts. The other baselines *VAE+linearSVM* and *CoopNet+linearSVM* used features of the VAE network\u00a0[@VAE] and the CoopNet\u00a0[@CoopNet], respectively, instead of *fc7* features, for part detection.\n\nThe last baseline\u00a0[@CNNAoG] learned AOGs without QA (*AOG w/o QA*). We randomly selected objects and annotated their parts for training.\n\nBoth object annotations and part annotations are used to learn models in all the thirteen baselines (including those without fine-tuning). *Fast-RCNN (1 ft)* and *CNN-PDD* used the cropped objects as the input of the CNN; *SS-DPM-Part*, *PL-DPM-Part*, *Part-Graph*, and *Interactive-DPM* used object boxes and part boxes to learn models. *CNN-PDD-ft*, *Fast-RCNN (2 fts)*, and methods based on *fc7* features used object bounding boxes for fine-tuning.\n\nEvaluation metric\n-----------------\n\nAs discussed in [@SemanticPart; @CNNAoG], a fair evaluation of part localization requires removing factors of object detection. Thus, we used ground-truth object bounding boxes to crop objects as testing images. Given an object image, some competing methods (*e.g.* *Fast-RCNN (1 ft)*, *Part-Graph*, and *SS-DPM-Part*) estimate several bounding boxes for the part with different confidences. We followed [@CNNSemanticPart; @SemanticPart; @ObjectDiscoveryCNN_1; @CNNAoG] to take the most confident bounding box per image as the part-localization result. Given part-localization results of a category, we applied the *normalized distance*\u00a0[@CNNSemanticPart] and the *percentage of correctly localized parts* (PCP)\u00a0[@fineGrained1; @fineGrained2; @fineGrained3] to evaluate the localization accuracy. We measured the distance between the predicted part center and the ground-truth part center, and then normalized the distance using the diagonal length of the object as the normalized distance. For the PCP, we used the typical metric of \u201c$IoU\\geq0.5$\u201d\u00a0[@FastRCNN] to identify correct part localizations.\n\nSee Table\u00a0\\[tab:imgnet\\] for the introduction of the 2nd and 3rd columns. The 4th column shows the number of questions for training. The 4th column indicates whether the baseline used all object annotations (*more than part annotations*) in the category to pre-fine-tune a CNN before learning the part.\n\nThe 3rd and 4th columns show the number of part annotations and the average number of questions for training.\n\nExperimental results\n--------------------\n\nWe learned AOGs for the head, the neck, and the nose/muzzle/beak parts of the six animal categories in the Pascal VOC Part dataset. For the ILSVRC 2013 DET Animal-Part dataset and the CUB200-2011 dataset, we learned an AOG for the head part[^3] of each category. It is because all categories in the two datasets contain the head part. We did not train human annotators. Shape differences between two part templates were often very vague, so that an annotator could assign a part to either part template.\n\n![image](results.pdf){width=\"\\linewidth\"}\n\n![image](visualization_QA.pdf){width=\"\\linewidth\"}\n\n![Image patches corresponding to different latent patterns.[]{data-label=\"fig:patches\"}](patches.pdf){width=\"0.8\\linewidth\"}\n\nTable\u00a0\\[tab:stat\\] shows how the AOG grew when people annotated more parts during the QA process. Given AOGs learned for the PASCAL VOC Part dataset, we computed the average number of children for each node in different AOG layers. The AOG mainly grew by adding new branches to represent new part templates. The refinement of an existing AOG branch did not significantly change the node number of the AOG.\n\nFig.\u00a0\\[fig:energyCurve\\] analyzes activation states of latent patterns in AOGs that were learned with different numbers of part annotations. Given a testing image $I$ for part parsing, we only focused on the inferred latent patterns and neural units, *i.e.* latent patterns and their inferred neural units under the selected part template. Let ${\\bf V}$ and ${\\bf V'}\\subset{\\bf V}$ denote all units in a specific conv-layer and the inferred units, respectively. $a_{v}$ denotes the activation score of $v\\in{\\bf V}$ after the ReLU operation. $a_{v}$ is also normalized by the average activation level of $v$\u2019s corresponding feature maps *w.r.t.* different images. Thus, in Fig.\u00a0\\[fig:energyCurve\\](left), we computed the ratio of the inferred activation energy as $\\frac{\\sum_{v\\in{\\bf V'}}a_{v}}{\\sum_{v\\in{\\bf V}}a_{v}}$. For each inferred latent pattern $u$, $a_{u}$ denotes the activation score of its selected neural unit[^4]. Fig.\u00a0\\[fig:energyCurve\\](middle) measures the relative magnitude of the inferred activations, which was measured as $\\frac{\\mathbb{E}_{u\\in{\\bf U}}[a_{u}]}{\\mathbb{E}_{v\\in{\\bf V}}[a_{v}]}$. Fig.\u00a0\\[fig:energyCurve\\](right) shows the ratio of the latent patterns being strongly activated. We used a threshold $\\tau=\\mathbb{E}_{v\\in{\\bf V}}[a_{v}]$ to identify strong activations, *i.e.* computing the activation ratio as $\\mathbb{E}_{u\\in{\\bf U}}[{\\bf 1}(a_{u}>\\tau)]$. Curves in Fig.\u00a0\\[fig:energyCurve\\] were reported as the average performance using images in the CUB200-2011 dataset.\n\nFig.\u00a0\\[fig:visualization\\] visualizes latent patterns in the AOG based on the technique of [@FeaVisual]. More specifically, Fig.\u00a0\\[fig:patches\\] lists images patches inferred by different latent patterns in the AOG with high inference scores. It shows that each latent pattern corresponds to a specific part shape through different images.\n\nFig.\u00a0\\[fig:results\\] shows part localization results based on AOGs. Tables\u00a0\\[tab:imgnet\\], \\[tab:VOC\\], and \\[tab:cub200\\] compare the part-localization performance of different baselines on different benchmark datasets using the evaluation metric of the normalized distance. Tables\u00a0\\[tab:VOC\\], and \\[tab:cub200\\] show both the number of part annotations and the number of questions. Fig.\u00a0\\[fig:curve\\] shows the performance of localizing the head part on objects in the PASCAL VOC Part Dataset, when people annotated different numbers of parts for training. Table\u00a0\\[tab:pcp\\] lists part-localization performance, which was evaluated by the PCP metric. In particular, the method of *Ours+fastRCNN* combined our method and the fast-RCNN to refine part-localization results[^5]. Our method learned AOGs with about $1/6$\u2013$1/2$ part annotations, but exhibited superior performance to the second best baseline.\n\n![Part localization performance on the Pascal VOC Part dataset.[]{data-label=\"fig:curve\"}](curve.pdf){width=\"\\linewidth\"}\n\nJustification of the methodology\n--------------------------------\n\nWe have three reasons to explain the good performance of our method. First, **generic information**: the latent patterns in the AOG were pre-fine-tuned using massive object images in a category, instead of being learned from a few part annotations. Thus, these patterns reflected generic part appearances and did not over-fit to a few part annotations.\n\nSecond, **less model drifts:** Instead of learning new CNN parameters, our method just used limited part annotations to mine the related patterns to represent the part concept. In addition, during active QA, Equation\u00a0(\\[eqn:predict\\]) usually selected objects with common poses for QA, *i.e.* choosing objects sharing common latent patterns with many other objects. Thus, the learned AOG suffered less from the model-drift problem.\n\nThird, **high QA efficiency:** Our QA process balanced both the commonness and the accuracy of a part template in Equation\u00a0(\\[eqn:predict\\]). In early steps of QA, our approach was prone to asking about new part templates, because objects with un-modeled part appearance usually had low inference scores. In later QA steps, common part appearances had been modeled, and our method gradually changed to ask about objects belonging to existing part templates to refine the AOG. Our method did not waste much labor of labeling objects that had been well modeled or had strange appearance.\n\nSummary and discussion\n======================\n\nIn this paper, we have proposed a method to bridge and solve the following three crucial issues in computer vision simultaneously.\n\n-   Removing noisy representations in conv-layers of a CNN and using an AOG model to reveal the semantic hierarchy of objects hidden in the CNN.\n\n-   Enabling people to communicate with neural representations in intermediate conv-layers of a CNN directly for model learning, based on the semantic representation of the AOG.\n\n-   Weakly-supervised transferring of object-part representations from a pre-trained CNN to model object parts at the semantic level, which boosts the learning efficiency.\n\nOur method incrementally mines object-part patterns from conv-layers of a pre-trained CNN and uses an AOG to encode the mined semantic hierarchy. The AOG semanticizes neural units in intermediate feature maps of a CNN by associating these units with semantic parts. We have proposed an active QA strategy to learn such an AOG model in a weakly-supervised manner. We have tested the proposed method for a total of 37 categories in three benchmark datasets. Our method has outperformed other baselines in the application of weakly-supervised part localization. For example, our method with 11 part annotations performed better than fast-RCNN with 60 part annotations on the ILSVRC dataset in Fig.\u00a0\\[fig:curve\\].\n\nAcknowledgments {#acknowledgments .unnumbered}\n===============\n\nAcknowledgment {#acknowledgment .unnumbered}\n==============\n\nThis work is supported by ONR MURI project N00014-16-1-2007, DARPA XAI Award N66001-17-2-4029, and NSF IIS 1423305.\n\n[Quanshi Zhang]{} received the B.S. degree in machine intelligence from Peking University, China, in 2009 and M.S. and Ph.D. degrees in center for spatial information science from the University of Tokyo, Japan, in 2011 and 2014, respectively. In 2014, he went to the University of California, Los Angeles, as a post-doctoral associate. Now, he is an associate professor at the Shanghai Jiao Tong University. His research interests include computer vision, machine learning, and robotics.\n\n[Ruiming Cao]{} received the B.S. degree in computer science from the University of California, Los Angeles, in 2017. Now, he is a master student at the University of California, Los Angeles. His research mainly focuses on computer vision.\n\n[Ying Nian Wu]{} received a Ph.D. degree from the Harvard University in 1996. He was an Assistant Professor at the University of Michigan between 1997 and 1999 and an Assistant Professor at the University of California, Los Angeles between 1999 and 2001. He became an Associate Professor at the University of California, Los Angeles in 2001. From 2006 to now, he is a professor at the University of California, Los Angeles. His research interests include statistics, machine learning, and computer vision.\n\n[Song-Chun Zhu]{} Song-Chun Zhu received a Ph.D. degree from Harvard University, and is a professor with the Department of Statistics and the Department of Computer Science at UCLA. His research interests include computer vision, statistical modeling and learning, cognition and AI, and visual arts. He received a number of honors, including the Marr Prize in 2003 with Z. Tu et. al. on image parsing,the Aggarwal prize from the Int\u2019l Association of Pattern Recognition in 2008, twice Marr Prize honorary nominations in 1999 for texture modeling and 2007 for object modeling with Y.N. Wu et al., a Sloan Fellowship in 2001, the US NSF Career Award in 2001, and the US ONR Young Investigator Award in 2001. He is a Fellow of IEEE.\n\nAnd-Or graph representations {#and-or-graph-representations-1 .unnumbered}\n============================\n\nParameters for latent patterns {#parameters-for-latent-patterns .unnumbered}\n------------------------------\n\nWe use the notation of ${\\bf p}_{u}$ to denote the central position of an image region $\\Lambda_{u}$. For simplification, all position variables ${\\bf p}_{u}$ are measured based on the image coordinates by propagating the position of $\\Lambda_{u}$ to the image plane.\n\nEach latent pattern $u$ is defined by its location parameters $\\{L_{u},D_{u},\\overline{\\bf p}_{u},\\Delta{\\bf p}_{u}\\}\\subset{\\boldsymbol\\theta}$, where ${\\boldsymbol\\theta}$ is the set of AOG parameters. It means that a latent pattern $u$ uses a square within the $D_{u}$-th channel of the $L_{u}$-th conv-layer\u2019s feature map as its deformation range. The center position of the square is given as $\\overline{\\bf p}_{u}$. When latent pattern $u$ is extracted from the $k$-th conv-layer, $u$ has a fixed value of $L_{u}=k$.\n\n$\\Delta{\\bf p}_{u}$ denotes the average displacement from $u$ and $u$\u2019s parent part template $v$ among various images, and $\\Delta{\\bf p}_{u}$ is used to compute $S^{\\textrm{inf}}(\\Lambda_{u}|\\Lambda_{v})$. Given parameter $\\overline{\\bf p}_{u}$, the displacement $\\Delta{\\bf p}_{u}$ can be estimated as $$\\Delta{\\bf p}_{u}=\\overline{\\bf p}^{*}_{v}-\\overline{\\bf p}_{u}\\nonumber$$ where $\\overline{\\bf p}^{*}_{v}$ denotes the average position of all ground-truth parts that are annotated for part template $v$. As a result, for each latent pattern $u$, we only need to learn its channel $D_{u}\\in{\\boldsymbol\\theta}$ and central position $\\overline{\\bf p}_{u}\\in{\\boldsymbol\\theta}$.\n\nScores of terminal nodes {#scores-of-terminal-nodes .unnumbered}\n------------------------\n\nThe inference score for each terminal node $v^{\\textrm{unt}}$ under a latent pattern $u$ is formulated as\n\n$$\\begin{aligned}\n&S_{v^{\\textrm{unt}}}=S_{v^{\\textrm{unt}}}^{\\textrm{rsp}}+S_{v^{\\textrm{unt}}}^{\\textrm{loc}}+S_{v^{\\textrm{unt}}}^{\\textrm{pair}}\\nonumber\\\\\n&S_{v^{\\textrm{unt}}}^{\\textrm{rsp}}=\\left\\{\\begin{array}{ll}\\lambda^{\\textrm{rsp}}X(v^{\\textrm{unt}}),& X(v^{\\textrm{unt}})>0\\\\ \\lambda^{\\textrm{rsp}}S_{none},& X(v^{\\textrm{unt}})\\leq0\\end{array}\\right.\\nonumber\\\\\n&S_{v^{\\textrm{unt}}}^{\\textrm{pair}}=-\\lambda^{\\textrm{pair}}\\!\\!\\!\\!\\!\\!\\!\\!\\underset{u_{\\textrm{upper}}\\in\\!\\textrm{Neighbor}(u)}{\\mathbb{E}}\\!\\!\\!\\!\\!\\!\\Vert[{\\bf p}_{v^{\\textrm{unt}}}-{\\bf p}_{u_{\\textrm{upper}}}]-[\\overline{\\bf p}_{u_{\\textrm{upper}}}-\\overline{\\bf p}_{u}]\\Vert\\nonumber\\end{aligned}$$\n\nThe score of $S_{v^{\\textrm{unt}}}$ consists of the following three terms: 1) $S_{v^{\\textrm{unt}}}^{\\textrm{rsp}}$ denotes the response value of the unit $v^{\\textrm{unt}}$, when we input image $I$ into the CNN. $X(v^{\\textrm{unt}})$ denotes the normalized response value of $v^{\\textrm{unt}}$; $S_{none}=-3$ is set for non-activated units. 2) When the parent $u$ selects $v^{\\textrm{unt}}$ as its location inference (*i.e.* $\\Lambda_{u}\\leftarrow\\Lambda_{v^{\\textrm{unt}}}$), $S_{v^{\\textrm{unt}}}^{\\textrm{loc}}$ measures the deformation level between $v^{\\textrm{unt}}$\u2019s location ${\\bf p}_{v^{\\textrm{unt}}}$ and $u$\u2019s ideal location $\\overline{\\bf p}_{u}$. 3) $S_{v^{\\textrm{unt}}}^{\\textrm{pair}}$ indicates the spatial compatibility between neighboring latent patterns: we model the pairwise spatial relationship between latent patterns in the upper conv-layer and those in the current conv-layer. For each $v^{\\textrm{unt}}$ (with its parent $u$) in conv-layer $L_{u}$, we select 15 nearest latent patterns in conv-layer $L_{u}+1$, *w.r.t.* $\\Vert\\overline{\\bf p}_{u}-\\overline{\\bf p}_{u_{\\textrm{upper}}}\\Vert$, as the neighboring latent patterns. We set constant weights $\\lambda^{\\textrm{rsp}}=1.5,\\lambda^{\\textrm{loc}}=1/3,\\lambda^{\\textrm{pair}}=10.0$, $\\lambda^{\\textrm{unant}}=5.0$, and $\\lambda^{\\textrm{close}}=0.4$ for all categories. Based on the above design, we first infer latent patterns corresponding to high conv-layers, and use the inference results to select units in low conv-layers.\n\nDuring the learning of AOGs, we define $S^{\\textrm{unant}}_{u}=S_{\\hat{v}^{\\textrm{unt}}}^{\\textrm{rsp}}+S_{\\hat{v}^{\\textrm{unt}}}^{\\textrm{loc}}$ to measure the latent-pattern-level inference score in Equation\u00a0(5), where $\\hat{v}^{\\textrm{unt}}$ denotes the neural unit assigned to $u$.\n\nScores of AND nodes {#scores-of-and-nodes .unnumbered}\n-------------------\n\n$$S^{\\textrm{inf}}(\\Lambda_{u}|\\Lambda_{v})=-\\lambda^{\\textrm{inf}}\\min\\{\\Vert{\\bf p}(\\Lambda_{u})+\\Delta{\\bf p}_{u}-{\\bf p}(\\Lambda_{v})\\Vert^2,d^2\\}\\nonumber$$\n\nwhere we set $d=37$ pixels and $\\lambda^{\\textrm{inf}}=5.0$.\n\n[^1]: Because the CNN has demonstrated its superior performance in object detection, we assume that the target object can be well detected by the pre-trained CNN. As in [@SemanticPart], we regard object detection and part localization as two separate processes for evaluation. Thus, to simplify the learning scenario, we crop $I$ only to contain the object, resize it to the image size for CNN inputs, and just focus on the part localization task to simplify the scenario of learning for part localization.\n\n[^2]: ${\\bf M}_{ii}\\!\\propto\\!\\exp[\\mathbb{E}_{I\\in{\\bf I}}S_{v^{\\textrm{unt}}_{i}}]$, where $v^{\\textrm{unt}}_{i}$ is the neural unit corresponding to the $i$-th element of ${\\bf f}_{I'}$.\n\n[^3]: It is the \u201cforehead\u201d part for birds in the CUB200-2011 dataset.\n\n[^4]: Two latent patterns may select the same neural unit\n\n[^5]: We used part boxes annotated during the QA process to learn a fast-RCNN for part detection. Given the inference result $\\Lambda_{v}$ of part template $v$ on image $I$, we define a new inference score for localization refinement $S_{v}^{\\textrm{new}}(\\Lambda_{v}^{\\textrm{new}})=S_{v}+\\lambda_1\\Phi(\\Lambda^{\\textrm{new}}_{v})+\\lambda_2\\frac{\\Vert{\\bf p}_{v}-{\\bf p}_{v}^{\\textrm{new}}\\Vert}{2\\sigma^2}$, where $\\sigma=70$ pixels, $\\lambda_1=5$, and $\\lambda_2=10$. $\\Phi(\\Lambda^{\\textrm{new}}_{v})$ denotes the fast-RCNN\u2019s detection score for the patch of $\\Lambda^{\\textrm{new}}_{v}$.\n"
}