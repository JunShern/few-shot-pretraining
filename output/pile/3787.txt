{
    "criteria": [
        {
            "criterion": "AllDocuments",
            "passed": true,
            "reason": "All documents pass."
        },
        {
            "criterion": "Domain",
            "passed": false,
            "reason": "Document source unavailable."
        },
        {
            "criterion": "QuestionAnswerStrings",
            "passed": true,
            "reason": "Text contains Q:."
        },
        {
            "criterion": "QuestionAnswerStringsV2",
            "passed": true,
            "reason": "Text contains ['Q:', 'A:', 'A:', 'A:']."
        },
        {
            "criterion": "FullyStructured",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ListPrefix",
            "passed": false,
            "reason": "Found 0 list prefixes. (Min: 5)"
        },
        {
            "criterion": "ListPrefixV2",
            "passed": false,
            "reason": "Does not meet criterion."
        }
    ],
    "doc_id": "3787",
    "text": "Q:\n\nRead from Kafka and write to hdfs in parquet\n\nI am new to the BigData eco system and kind of getting started.\nI have read several articles about reading a kafka topic using spark streaming but would like to know if it is possible to read from kafka using a spark job instead of streaming ?\nIf yes, could you guys help me in pointing out to some articles or code snippets that can get me started.\nMy second part of the question is writing to hdfs in parquet format.\nOnce i read from Kafka , i assume i will have an rdd.\nConvert this rdd into a dataframe and then write the dataframe as a parquet file.\nIs this the right approach.\nAny help appreciated. \nThanks\n\nA:\n\nYou already have a couple of good answers on the topic. \nJust wanted to stress out - be careful to stream directly into a parquet table.\nParquet's performance shines when parquet row group sizes are large enough (for simplicity, you can say file size should be in order of 64-256Mb for example), to take advantage of dictionary compression, bloom filters etc. (one parquet file can have multiple row chunks in it, and normally does have multiple row chunks in each file; although row chunks can't span multiple parquet files)\nIf you're streaming directly to a parquet table, then you'll end up very likely with a bunch of tiny parquet files (depending on mini-batch size of Spark Streaming, and volume of data). Querying such files can be very slow. Parquet may require reading all files' headers to reconcile schema for example and it's a big overhead. If this is the case, you will need to have a separate process that will, for example, as a workaround, read older files, and writes them \"merged\" (this wouldn't be a simple file-level merge, a process would actually need to read in all parquet data and spill out larger parquet files). \nThis workaround may kill the original purpose of data \"streaming\". You could look at other technologies here too - like Apache Kudu, Apache Kafka, Apache Druid, Kinesis etc that can work here better. \nUpdate: since I posted this answer, there is now a new strong player here - Delta Lake. https://delta.io/ If you're used to parquet, you'll find Delta very attractive (actually, Delta is built on top of parquet layer + metadata). Delta Lake offers:\nACID transactions on Spark: \n\nSerializable isolation levels ensure that readers never see inconsistent data.\nScalable metadata handling: Leverages Spark\u2019s distributed processing power to handle all the metadata for petabyte-scale tables with billions of files at ease.\nStreaming and batch unification: A table in Delta Lake is a batch table as well as a streaming source and sink. Streaming data ingest, batch historic backfill, interactive queries all just work out of the box.\nSchema enforcement: Automatically handles schema variations to prevent insertion of bad records during ingestion.\nTime travel: Data versioning enables rollbacks, full historical audit trails, and reproducible machine learning experiments.\nUpserts and deletes: Supports merge, update and delete operations to enable complex usecases like change-data-capture, slowly-changing-dimension (SCD) operations, streaming upserts, and so on.\n\nA:\n\nFor reading data from Kafka and writing it to HDFS, in Parquet format, using Spark Batch job instead of streaming, you can use Spark Structured Streaming.\nStructured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write Ahead Logs. In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\nIt comes with Kafka as a built in Source, i.e., we can poll data from Kafka. It\u2019s compatible with Kafka broker versions 0.10.0 or higher.\nFor pulling the data from Kafka in batch mode, you can create a Dataset/DataFrame for a defined range of offsets.\n// Subscribe to 1 topic defaults to the earliest and latest offsets\nval df = spark\n  .read\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n  .option(\"subscribe\", \"topic1\")\n  .load()\ndf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n  .as[(String, String)]\n\n// Subscribe to multiple topics, specifying explicit Kafka offsets\nval df = spark\n  .read\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n  .option(\"subscribe\", \"topic1,topic2\")\n  .option(\"startingOffsets\", \"\"\"{\"topic1\":{\"0\":23,\"1\":-2},\"topic2\":{\"0\":-2}}\"\"\")\n  .option(\"endingOffsets\", \"\"\"{\"topic1\":{\"0\":50,\"1\":-1},\"topic2\":{\"0\":-1}}\"\"\")\n  .load()\ndf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n  .as[(String, String)]\n\n// Subscribe to a pattern, at the earliest and latest offsets\nval df = spark\n  .read\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n  .option(\"subscribePattern\", \"topic.*\")\n  .option(\"startingOffsets\", \"earliest\")\n  .option(\"endingOffsets\", \"latest\")\n  .load()\ndf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n  .as[(String, String)]\n\nEach row in the source has the following schema:\n| Column           | Type          |\n|:-----------------|--------------:|\n| key              |        binary |\n| value            |        binary |\n| topic            |        string |\n| partition        |           int |\n| offset           |          long |\n| timestamp        |          long |\n| timestampType    |           int |\n\nNow, to write Data to HDFS in parquet format, following code can be written:\ndf.write.parquet(\"hdfs://data.parquet\")\n\nFor more information on Spark Structured Streaming + Kafka, please refer to following guide - Kafka Integration Guide\nI hope it helps!\n\nA:\n\nUse Kafka Streams.  SparkStreaming is an misnomer (it's mini-batch under the hood, at least up to 2.2).\nhttps://eng.verizondigitalmedia.com/2017/04/28/Kafka-to-Hdfs-ParquetSerializer/\n\n"
}