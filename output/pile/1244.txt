{
    "criteria": [
        {
            "criterion": "AllDocuments",
            "passed": true,
            "reason": "All documents pass."
        },
        {
            "criterion": "Domain",
            "passed": false,
            "reason": "Document source unavailable."
        },
        {
            "criterion": "QuestionAnswerStrings",
            "passed": true,
            "reason": "Text contains Q:."
        },
        {
            "criterion": "QuestionAnswerStringsV2",
            "passed": true,
            "reason": "Text contains ['Q:', 'A:']."
        },
        {
            "criterion": "FullyStructured",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ListPrefix",
            "passed": false,
            "reason": "Found 0 list prefixes. (Min: 5)"
        },
        {
            "criterion": "ListPrefixV2",
            "passed": false,
            "reason": "Does not meet criterion."
        }
    ],
    "doc_id": "1244",
    "text": "Q:\n\nHow to extract all rows from a large postgres table using python efficiently?\n\nI have been able to extract close to 3.5 mil rows from a postgres table using python and write to a file. However the process is extremely slow and I'm sure not the most efficient.\nFollowing is my code:\nimport psycopg2, time,csv\nconn_string = \"host='compute-1.amazonaws.com' dbname='re' user='data' password='reck' port=5433\"\nconn = psycopg2.connect(conn_string)\ncursor = conn.cursor()\nquert = '''select data from table;'''\ncursor.execute(quert)\n\ndef get_data():\n    while True:\n        recs = cursor.fetchmany(10000)\n\n        if not recs:\n            break\n\n        for columns in recs:\n            # do transformation of data here\n            yield(columns) \n\nsolr_input=get_data()\n\nwith open('prc_ind.csv','a') as fh:\n    for i in solr_input:\n        count += 1\n\n        if count % 1000 == 0:\n             print(count)\n\n         a,b,c,d = i['Skills'],i['Id'],i['History'],i['Industry']\n         fh.write(\"{0}|{1}|{2}|{3}\\n\".format(a,b,c,d))\n\nThe table has about 8 mil rows. I want to ask is there is a better, faster and less memory intensive way to accomplish this.\n\nA:\n\nI can see four fields, so I'll assume you are selecting only these.\nBut even then, you are still loading 8 mil x 4 x n Bytes of data from what seems to be another server. So yes it'll take some time.\nThough you are trying to rebuild the wheel, why not use the PostgreSQL client?\npsql -d dbname -t -A -F\",\" -c \"select * from users\" > output.csv\n\n"
}