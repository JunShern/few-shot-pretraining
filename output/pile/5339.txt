{
    "criteria": [
        {
            "criterion": "AllDocuments",
            "passed": true,
            "reason": "All documents pass."
        },
        {
            "criterion": "Domain",
            "passed": false,
            "reason": "Document source unavailable."
        },
        {
            "criterion": "QuestionAnswerStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "QuestionAnswerStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "FullyStructured",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ListPrefix",
            "passed": true,
            "reason": "Found 7 list prefixes. (Min: 5)"
        },
        {
            "criterion": "ListPrefixV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesMinimalEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesSynonymsEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesDiverseEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        }
    ],
    "doc_id": "5339",
    "text": "/****************************************************************************\n * Driver for Solarflare network controllers and boards\n * Copyright 2005-2006 Fen Systems Ltd.\n * Copyright 2005-2013 Solarflare Communications Inc.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License version 2 as published\n * by the Free Software Foundation, incorporated herein by reference.\n */\n\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/slab.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/prefetch.h>\n#include <linux/moduleparam.h>\n#include <linux/iommu.h>\n#include <net/ip.h>\n#include <net/checksum.h>\n#include \"net_driver.h\"\n#include \"efx.h\"\n#include \"filter.h\"\n#include \"nic.h\"\n#include \"selftest.h\"\n#include \"workarounds.h\"\n\n/* Preferred number of descriptors to fill at once */\n#define EFX_RX_PREFERRED_BATCH 8U\n\n/* Number of RX buffers to recycle pages for.  When creating the RX page recycle\n * ring, this number is divided by the number of buffers per page to calculate\n * the number of pages to store in the RX page recycle ring.\n */\n#define EFX_RECYCLE_RING_SIZE_IOMMU 4096\n#define EFX_RECYCLE_RING_SIZE_NOIOMMU (2 * EFX_RX_PREFERRED_BATCH)\n\n/* Size of buffer allocated for skb header area. */\n#define EFX_SKB_HEADERS  128u\n\n/* This is the percentage fill level below which new RX descriptors\n * will be added to the RX descriptor ring.\n */\nstatic unsigned int rx_refill_threshold;\n\n/* Each packet can consume up to ceil(max_frame_len / buffer_size) buffers */\n#define EFX_RX_MAX_FRAGS DIV_ROUND_UP(EFX_MAX_FRAME_LEN(EFX_MAX_MTU), \\\n\t\t\t\t      EFX_RX_USR_BUF_SIZE)\n\n/*\n * RX maximum head room required.\n *\n * This must be at least 1 to prevent overflow, plus one packet-worth\n * to allow pipelined receives.\n */\n#define EFX_RXD_HEAD_ROOM (1 + EFX_RX_MAX_FRAGS)\n\nstatic inline u8 *efx_rx_buf_va(struct efx_rx_buffer *buf)\n{\n\treturn page_address(buf->page) + buf->page_offset;\n}\n\nstatic inline u32 efx_rx_buf_hash(struct efx_nic *efx, const u8 *eh)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)\n\treturn __le32_to_cpup((const __le32 *)(eh + efx->rx_packet_hash_offset));\n#else\n\tconst u8 *data = eh + efx->rx_packet_hash_offset;\n\treturn (u32)data[0]\t  |\n\t       (u32)data[1] << 8  |\n\t       (u32)data[2] << 16 |\n\t       (u32)data[3] << 24;\n#endif\n}\n\nstatic inline struct efx_rx_buffer *\nefx_rx_buf_next(struct efx_rx_queue *rx_queue, struct efx_rx_buffer *rx_buf)\n{\n\tif (unlikely(rx_buf == efx_rx_buffer(rx_queue, rx_queue->ptr_mask)))\n\t\treturn efx_rx_buffer(rx_queue, 0);\n\telse\n\t\treturn rx_buf + 1;\n}\n\nstatic inline void efx_sync_rx_buffer(struct efx_nic *efx,\n\t\t\t\t      struct efx_rx_buffer *rx_buf,\n\t\t\t\t      unsigned int len)\n{\n\tdma_sync_single_for_cpu(&efx->pci_dev->dev, rx_buf->dma_addr, len,\n\t\t\t\tDMA_FROM_DEVICE);\n}\n\nvoid efx_rx_config_page_split(struct efx_nic *efx)\n{\n\tefx->rx_page_buf_step = ALIGN(efx->rx_dma_len + efx->rx_ip_align,\n\t\t\t\t      EFX_RX_BUF_ALIGNMENT);\n\tefx->rx_bufs_per_page = efx->rx_buffer_order ? 1 :\n\t\t((PAGE_SIZE - sizeof(struct efx_rx_page_state)) /\n\t\t efx->rx_page_buf_step);\n\tefx->rx_buffer_truesize = (PAGE_SIZE << efx->rx_buffer_order) /\n\t\tefx->rx_bufs_per_page;\n\tefx->rx_pages_per_batch = DIV_ROUND_UP(EFX_RX_PREFERRED_BATCH,\n\t\t\t\t\t       efx->rx_bufs_per_page);\n}\n\n/* Check the RX page recycle ring for a page that can be reused. */\nstatic struct page *efx_reuse_page(struct efx_rx_queue *rx_queue)\n{\n\tstruct efx_nic *efx = rx_queue->efx;\n\tstruct page *page;\n\tstruct efx_rx_page_state *state;\n\tunsigned index;\n\n\tindex = rx_queue->page_remove & rx_queue->page_ptr_mask;\n\tpage = rx_queue->page_ring[index];\n\tif (page == NULL)\n\t\treturn NULL;\n\n\trx_queue->page_ring[index] = NULL;\n\t/* page_remove cannot exceed page_add. */\n\tif (rx_queue->page_remove != rx_queue->page_add)\n\t\t++rx_queue->page_remove;\n\n\t/* If page_count is 1 then we hold the only reference to this page. */\n\tif (page_count(page) == 1) {\n\t\t++rx_queue->page_recycle_count;\n\t\treturn page;\n\t} else {\n\t\tstate = page_address(page);\n\t\tdma_unmap_page(&efx->pci_dev->dev, state->dma_addr,\n\t\t\t       PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t       DMA_FROM_DEVICE);\n\t\tput_page(page);\n\t\t++rx_queue->page_recycle_failed;\n\t}\n\n\treturn NULL;\n}\n\n/**\n * efx_init_rx_buffers - create EFX_RX_BATCH page-based RX buffers\n *\n * @rx_queue:\t\tEfx RX queue\n *\n * This allocates a batch of pages, maps them for DMA, and populates\n * struct efx_rx_buffers for each one. Return a negative error code or\n * 0 on success. If a single page can be used for multiple buffers,\n * then the page will either be inserted fully, or not at all.\n */\nstatic int efx_init_rx_buffers(struct efx_rx_queue *rx_queue, bool atomic)\n{\n\tstruct efx_nic *efx = rx_queue->efx;\n\tstruct efx_rx_buffer *rx_buf;\n\tstruct page *page;\n\tunsigned int page_offset;\n\tstruct efx_rx_page_state *state;\n\tdma_addr_t dma_addr;\n\tunsigned index, count;\n\n\tcount = 0;\n\tdo {\n\t\tpage = efx_reuse_page(rx_queue);\n\t\tif (page == NULL) {\n\t\t\tpage = alloc_pages(__GFP_COLD | __GFP_COMP |\n\t\t\t\t\t   (atomic ? GFP_ATOMIC : GFP_KERNEL),\n\t\t\t\t\t   efx->rx_buffer_order);\n\t\t\tif (unlikely(page == NULL))\n\t\t\t\treturn -ENOMEM;\n\t\t\tdma_addr =\n\t\t\t\tdma_map_page(&efx->pci_dev->dev, page, 0,\n\t\t\t\t\t     PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t\t\t     DMA_FROM_DEVICE);\n\t\t\tif (unlikely(dma_mapping_error(&efx->pci_dev->dev,\n\t\t\t\t\t\t       dma_addr))) {\n\t\t\t\t__free_pages(page, efx->rx_buffer_order);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t\tstate = page_address(page);\n\t\t\tstate->dma_addr = dma_addr;\n\t\t} else {\n\t\t\tstate = page_address(page);\n\t\t\tdma_addr = state->dma_addr;\n\t\t}\n\n\t\tdma_addr += sizeof(struct efx_rx_page_state);\n\t\tpage_offset = sizeof(struct efx_rx_page_state);\n\n\t\tdo {\n\t\t\tindex = rx_queue->added_count & rx_queue->ptr_mask;\n\t\t\trx_buf = efx_rx_buffer(rx_queue, index);\n\t\t\trx_buf->dma_addr = dma_addr + efx->rx_ip_align;\n\t\t\trx_buf->page = page;\n\t\t\trx_buf->page_offset = page_offset + efx->rx_ip_align;\n\t\t\trx_buf->len = efx->rx_dma_len;\n\t\t\trx_buf->flags = 0;\n\t\t\t++rx_queue->added_count;\n\t\t\tget_page(page);\n\t\t\tdma_addr += efx->rx_page_buf_step;\n\t\t\tpage_offset += efx->rx_page_buf_step;\n\t\t} while (page_offset + efx->rx_page_buf_step <= PAGE_SIZE);\n\n\t\trx_buf->flags = EFX_RX_BUF_LAST_IN_PAGE;\n\t} while (++count < efx->rx_pages_per_batch);\n\n\treturn 0;\n}\n\n/* Unmap a DMA-mapped page.  This function is only called for the final RX\n * buffer in a page.\n */\nstatic void efx_unmap_rx_buffer(struct efx_nic *efx,\n\t\t\t\tstruct efx_rx_buffer *rx_buf)\n{\n\tstruct page *page = rx_buf->page;\n\n\tif (page) {\n\t\tstruct efx_rx_page_state *state = page_address(page);\n\t\tdma_unmap_page(&efx->pci_dev->dev,\n\t\t\t       state->dma_addr,\n\t\t\t       PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t       DMA_FROM_DEVICE);\n\t}\n}\n\nstatic void efx_free_rx_buffers(struct efx_rx_queue *rx_queue,\n\t\t\t\tstruct efx_rx_buffer *rx_buf,\n\t\t\t\tunsigned int num_bufs)\n{\n\tdo {\n\t\tif (rx_buf->page) {\n\t\t\tput_page(rx_buf->page);\n\t\t\trx_buf->page = NULL;\n\t\t}\n\t\trx_buf = efx_rx_buf_next(rx_queue, rx_buf);\n\t} while (--num_bufs);\n}\n\n/* Attempt to recycle the page if there is an RX recycle ring; the page can\n * only be added if this is the final RX buffer, to prevent pages being used in\n * the descriptor ring and appearing in the recycle ring simultaneously.\n */\nstatic void efx_recycle_rx_page(struct efx_channel *channel,\n\t\t\t\tstruct efx_rx_buffer *rx_buf)\n{\n\tstruct page *page = rx_buf->page;\n\tstruct efx_rx_queue *rx_queue = efx_channel_get_rx_queue(channel);\n\tstruct efx_nic *efx = rx_queue->efx;\n\tunsigned index;\n\n\t/* Only recycle the page after processing the final buffer. */\n\tif (!(rx_buf->flags & EFX_RX_BUF_LAST_IN_PAGE))\n\t\treturn;\n\n\tindex = rx_queue->page_add & rx_queue->page_ptr_mask;\n\tif (rx_queue->page_ring[index] == NULL) {\n\t\tunsigned read_index = rx_queue->page_remove &\n\t\t\trx_queue->page_ptr_mask;\n\n\t\t/* The next slot in the recycle ring is available, but\n\t\t * increment page_remove if the read pointer currently\n\t\t * points here.\n\t\t */\n\t\tif (read_index == index)\n\t\t\t++rx_queue->page_remove;\n\t\trx_queue->page_ring[index] = page;\n\t\t++rx_queue->page_add;\n\t\treturn;\n\t}\n\t++rx_queue->page_recycle_full;\n\tefx_unmap_rx_buffer(efx, rx_buf);\n\tput_page(rx_buf->page);\n}\n\nstatic void efx_fini_rx_buffer(struct efx_rx_queue *rx_queue,\n\t\t\t       struct efx_rx_buffer *rx_buf)\n{\n\t/* Release the page reference we hold for the buffer. */\n\tif (rx_buf->page)\n\t\tput_page(rx_buf->page);\n\n\t/* If this is the last buffer in a page, unmap and free it. */\n\tif (rx_buf->flags & EFX_RX_BUF_LAST_IN_PAGE) {\n\t\tefx_unmap_rx_buffer(rx_queue->efx, rx_buf);\n\t\tefx_free_rx_buffers(rx_queue, rx_buf, 1);\n\t}\n\trx_buf->page = NULL;\n}\n\n/* Recycle the pages that are used by buffers that have just been received. */\nstatic void efx_recycle_rx_pages(struct efx_channel *channel,\n\t\t\t\t struct efx_rx_buffer *rx_buf,\n\t\t\t\t unsigned int n_frags)\n{\n\tstruct efx_rx_queue *rx_queue = efx_channel_get_rx_queue(channel);\n\n\tdo {\n\t\tefx_recycle_rx_page(channel, rx_buf);\n\t\trx_buf = efx_rx_buf_next(rx_queue, rx_buf);\n\t} while (--n_frags);\n}\n\nstatic void efx_discard_rx_packet(struct efx_channel *channel,\n\t\t\t\t  struct efx_rx_buffer *rx_buf,\n\t\t\t\t  unsigned int n_frags)\n{\n\tstruct efx_rx_queue *rx_queue = efx_channel_get_rx_queue(channel);\n\n\tefx_recycle_rx_pages(channel, rx_buf, n_frags);\n\n\tefx_free_rx_buffers(rx_queue, rx_buf, n_frags);\n}\n\n/**\n * efx_fast_push_rx_descriptors - push new RX descriptors quickly\n * @rx_queue:\t\tRX descriptor queue\n *\n * This will aim to fill the RX descriptor queue up to\n * @rx_queue->@max_fill. If there is insufficient atomic\n * memory to do so, a slow fill will be scheduled.\n *\n * The caller must provide serialisation (none is used here). In practise,\n * this means this function must run from the NAPI handler, or be called\n * when NAPI is disabled.\n */\nvoid efx_fast_push_rx_descriptors(struct efx_rx_queue *rx_queue, bool atomic)\n{\n\tstruct efx_nic *efx = rx_queue->efx;\n\tunsigned int fill_level, batch_size;\n\tint space, rc = 0;\n\n\tif (!rx_queue->refill_enabled)\n\t\treturn;\n\n\t/* Calculate current fill level, and exit if we don't need to fill */\n\tfill_level = (rx_queue->added_count - rx_queue->removed_count);\n\tEFX_WARN_ON_ONCE_PARANOID(fill_level > rx_queue->efx->rxq_entries);\n\tif (fill_level >= rx_queue->fast_fill_trigger)\n\t\tgoto out;\n\n\t/* Record minimum fill level */\n\tif (unlikely(fill_level < rx_queue->min_fill)) {\n\t\tif (fill_level)\n\t\t\trx_queue->min_fill = fill_level;\n\t}\n\n\tbatch_size = efx->rx_pages_per_batch * efx->rx_bufs_per_page;\n\tspace = rx_queue->max_fill - fill_level;\n\tEFX_WARN_ON_ONCE_PARANOID(space < batch_size);\n\n\tnetif_vdbg(rx_queue->efx, rx_status, rx_queue->efx->net_dev,\n\t\t   \"RX queue %d fast-filling descriptor ring from\"\n\t\t   \" level %d to level %d\\n\",\n\t\t   efx_rx_queue_index(rx_queue), fill_level,\n\t\t   rx_queue->max_fill);\n\n\n\tdo {\n\t\trc = efx_init_rx_buffers(rx_queue, atomic);\n\t\tif (unlikely(rc)) {\n\t\t\t/* Ensure that we don't leave the rx queue empty */\n\t\t\tif (rx_queue->added_count == rx_queue->removed_count)\n\t\t\t\tefx_schedule_slow_fill(rx_queue);\n\t\t\tgoto out;\n\t\t}\n\t} while ((space -= batch_size) >= batch_size);\n\n\tnetif_vdbg(rx_queue->efx, rx_status, rx_queue->efx->net_dev,\n\t\t   \"RX queue %d fast-filled descriptor ring \"\n\t\t   \"to level %d\\n\", efx_rx_queue_index(rx_queue),\n\t\t   rx_queue->added_count - rx_queue->removed_count);\n\n out:\n\tif (rx_queue->notified_count != rx_queue->added_count)\n\t\tefx_nic_notify_rx_desc(rx_queue);\n}\n\nvoid efx_rx_slow_fill(unsigned long context)\n{\n\tstruct efx_rx_queue *rx_queue = (struct efx_rx_queue *)context;\n\n\t/* Post an event to cause NAPI to run and refill the queue */\n\tefx_nic_generate_fill_event(rx_queue);\n\t++rx_queue->slow_fill_count;\n}\n\nstatic void efx_rx_packet__check_len(struct efx_rx_queue *rx_queue,\n\t\t\t\t     struct efx_rx_buffer *rx_buf,\n\t\t\t\t     int len)\n{\n\tstruct efx_nic *efx = rx_queue->efx;\n\tunsigned max_len = rx_buf->len - efx->type->rx_buffer_padding;\n\n\tif (likely(len <= max_len))\n\t\treturn;\n\n\t/* The packet must be discarded, but this is only a fatal error\n\t * if the caller indicated it was\n\t */\n\trx_buf->flags |= EFX_RX_PKT_DISCARD;\n\n\tif (net_ratelimit())\n\t\tnetif_err(efx, rx_err, efx->net_dev,\n\t\t\t  \"RX queue %d overlength RX event (%#x > %#x)\\n\",\n\t\t\t  efx_rx_queue_index(rx_queue), len, max_len);\n\n\tefx_rx_queue_channel(rx_queue)->n_rx_overlength++;\n}\n\n/* Pass a received packet up through GRO.  GRO can handle pages\n * regardless of checksum state and skbs with a good checksum.\n */\nstatic void\nefx_rx_packet_gro(struct efx_channel *channel, struct efx_rx_buffer *rx_buf,\n\t\t  unsigned int n_frags, u8 *eh)\n{\n\tstruct napi_struct *napi = &channel->napi_str;\n\tgro_result_t gro_result;\n\tstruct efx_nic *efx = channel->efx;\n\tstruct sk_buff *skb;\n\n\tskb = napi_get_frags(napi);\n\tif (unlikely(!skb)) {\n\t\tstruct efx_rx_queue *rx_queue;\n\n\t\trx_queue = efx_channel_get_rx_queue(channel);\n\t\tefx_free_rx_buffers(rx_queue, rx_buf, n_frags);\n\t\treturn;\n\t}\n\n\tif (efx->net_dev->features & NETIF_F_RXHASH)\n\t\tskb_set_hash(skb, efx_rx_buf_hash(efx, eh),\n\t\t\t     PKT_HASH_TYPE_L3);\n\tskb->ip_summed = ((rx_buf->flags & EFX_RX_PKT_CSUMMED) ?\n\t\t\t  CHECKSUM_UNNECESSARY : CHECKSUM_NONE);\n\tskb->csum_level = !!(rx_buf->flags & EFX_RX_PKT_CSUM_LEVEL);\n\n\tfor (;;) {\n\t\tskb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\t   rx_buf->page, rx_buf->page_offset,\n\t\t\t\t   rx_buf->len);\n\t\trx_buf->page = NULL;\n\t\tskb->len += rx_buf->len;\n\t\tif (skb_shinfo(skb)->nr_frags == n_frags)\n\t\t\tbreak;\n\n\t\trx_buf = efx_rx_buf_next(&channel->rx_queue, rx_buf);\n\t}\n\n\tskb->data_len = skb->len;\n\tskb->truesize += n_frags * efx->rx_buffer_truesize;\n\n\tskb_record_rx_queue(skb, channel->rx_queue.core_index);\n\n\tgro_result = napi_gro_frags(napi);\n\tif (gro_result != GRO_DROP)\n\t\tchannel->irq_mod_score += 2;\n}\n\n/* Allocate and construct an SKB around page fragments */\nstatic struct sk_buff *efx_rx_mk_skb(struct efx_channel *channel,\n\t\t\t\t     struct efx_rx_buffer *rx_buf,\n\t\t\t\t     unsigned int n_frags,\n\t\t\t\t     u8 *eh, int hdr_len)\n{\n\tstruct efx_nic *efx = channel->efx;\n\tstruct sk_buff *skb;\n\n\t/* Allocate an SKB to store the headers */\n\tskb = netdev_alloc_skb(efx->net_dev,\n\t\t\t       efx->rx_ip_align + efx->rx_prefix_size +\n\t\t\t       hdr_len);\n\tif (unlikely(skb == NULL)) {\n\t\tatomic_inc(&efx->n_rx_noskb_drops);\n\t\treturn NULL;\n\t}\n\n\tEFX_WARN_ON_ONCE_PARANOID(rx_buf->len < hdr_len);\n\n\tmemcpy(skb->data + efx->rx_ip_align, eh - efx->rx_prefix_size,\n\t       efx->rx_prefix_size + hdr_len);\n\tskb_reserve(skb, efx->rx_ip_align + efx->rx_prefix_size);\n\t__skb_put(skb, hdr_len);\n\n\t/* Append the remaining page(s) onto the frag list */\n\tif (rx_buf->len > hdr_len) {\n\t\trx_buf->page_offset += hdr_len;\n\t\trx_buf->len -= hdr_len;\n\n\t\tfor (;;) {\n\t\t\tskb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\t\t   rx_buf->page, rx_buf->page_offset,\n\t\t\t\t\t   rx_buf->len);\n\t\t\trx_buf->page = NULL;\n\t\t\tskb->len += rx_buf->len;\n\t\t\tskb->data_len += rx_buf->len;\n\t\t\tif (skb_shinfo(skb)->nr_frags == n_frags)\n\t\t\t\tbreak;\n\n\t\t\trx_buf = efx_rx_buf_next(&channel->rx_queue, rx_buf);\n\t\t}\n\t} else {\n\t\t__free_pages(rx_buf->page, efx->rx_buffer_order);\n\t\trx_buf->page = NULL;\n\t\tn_frags = 0;\n\t}\n\n\tskb->truesize += n_frags * efx->rx_buffer_truesize;\n\n\t/* Move past the ethernet header */\n\tskb->protocol = eth_type_trans(skb, efx->net_dev);\n\n\tskb_mark_napi_id(skb, &channel->napi_str);\n\n\treturn skb;\n}\n\nvoid efx_rx_packet(struct efx_rx_queue *rx_queue, unsigned int index,\n\t\t   unsigned int n_frags, unsigned int len, u16 flags)\n{\n\tstruct efx_nic *efx = rx_queue->efx;\n\tstruct efx_channel *channel = efx_rx_queue_channel(rx_queue);\n\tstruct efx_rx_buffer *rx_buf;\n\n\trx_queue->rx_packets++;\n\n\trx_buf = efx_rx_buffer(rx_queue, index);\n\trx_buf->flags |= flags;\n\n\t/* Validate the number of fragments and completed length */\n\tif (n_frags == 1) {\n\t\tif (!(flags & EFX_RX_PKT_PREFIX_LEN))\n\t\t\tefx_rx_packet__check_len(rx_queue, rx_buf, len);\n\t} else if (unlikely(n_frags > EFX_RX_MAX_FRAGS) ||\n\t\t   unlikely(len <= (n_frags - 1) * efx->rx_dma_len) ||\n\t\t   unlikely(len > n_frags * efx->rx_dma_len) ||\n\t\t   unlikely(!efx->rx_scatter)) {\n\t\t/* If this isn't an explicit discard request, either\n\t\t * the hardware or the driver is broken.\n\t\t */\n\t\tWARN_ON(!(len == 0 && rx_buf->flags & EFX_RX_PKT_DISCARD));\n\t\trx_buf->flags |= EFX_RX_PKT_DISCARD;\n\t}\n\n\tnetif_vdbg(efx, rx_status, efx->net_dev,\n\t\t   \"RX queue %d received ids %x-%x len %d %s%s\\n\",\n\t\t   efx_rx_queue_index(rx_queue), index,\n\t\t   (index + n_frags - 1) & rx_queue->ptr_mask, len,\n\t\t   (rx_buf->flags & EFX_RX_PKT_CSUMMED) ? \" [SUMMED]\" : \"\",\n\t\t   (rx_buf->flags & EFX_RX_PKT_DISCARD) ? \" [DISCARD]\" : \"\");\n\n\t/* Discard packet, if instructed to do so.  Process the\n\t * previous receive first.\n\t */\n\tif (unlikely(rx_buf->flags & EFX_RX_PKT_DISCARD)) {\n\t\tefx_rx_flush_packet(channel);\n\t\tefx_discard_rx_packet(channel, rx_buf, n_frags);\n\t\treturn;\n\t}\n\n\tif (n_frags == 1 && !(flags & EFX_RX_PKT_PREFIX_LEN))\n\t\trx_buf->len = len;\n\n\t/* Release and/or sync the DMA mapping - assumes all RX buffers\n\t * consumed in-order per RX queue.\n\t */\n\tefx_sync_rx_buffer(efx, rx_buf, rx_buf->len);\n\n\t/* Prefetch nice and early so data will (hopefully) be in cache by\n\t * the time we look at it.\n\t */\n\tprefetch(efx_rx_buf_va(rx_buf));\n\n\trx_buf->page_offset += efx->rx_prefix_size;\n\trx_buf->len -= efx->rx_prefix_size;\n\n\tif (n_frags > 1) {\n\t\t/* Release/sync DMA mapping for additional fragments.\n\t\t * Fix length for last fragment.\n\t\t */\n\t\tunsigned int tail_frags = n_frags - 1;\n\n\t\tfor (;;) {\n\t\t\trx_buf = efx_rx_buf_next(rx_queue, rx_buf);\n\t\t\tif (--tail_frags == 0)\n\t\t\t\tbreak;\n\t\t\tefx_sync_rx_buffer(efx, rx_buf, efx->rx_dma_len);\n\t\t}\n\t\trx_buf->len = len - (n_frags - 1) * efx->rx_dma_len;\n\t\tefx_sync_rx_buffer(efx, rx_buf, rx_buf->len);\n\t}\n\n\t/* All fragments have been DMA-synced, so recycle pages. */\n\trx_buf = efx_rx_buffer(rx_queue, index);\n\tefx_recycle_rx_pages(channel, rx_buf, n_frags);\n\n\t/* Pipeline receives so that we give time for packet headers to be\n\t * prefetched into cache.\n\t */\n\tefx_rx_flush_packet(channel);\n\tchannel->rx_pkt_n_frags = n_frags;\n\tchannel->rx_pkt_index = index;\n}\n\nstatic void efx_rx_deliver(struct efx_channel *channel, u8 *eh,\n\t\t\t   struct efx_rx_buffer *rx_buf,\n\t\t\t   unsigned int n_frags)\n{\n\tstruct sk_buff *skb;\n\tu16 hdr_len = min_t(u16, rx_buf->len, EFX_SKB_HEADERS);\n\n\tskb = efx_rx_mk_skb(channel, rx_buf, n_frags, eh, hdr_len);\n\tif (unlikely(skb == NULL)) {\n\t\tstruct efx_rx_queue *rx_queue;\n\n\t\trx_queue = efx_channel_get_rx_queue(channel);\n\t\tefx_free_rx_buffers(rx_queue, rx_buf, n_frags);\n\t\treturn;\n\t}\n\tskb_record_rx_queue(skb, channel->rx_queue.core_index);\n\n\t/* Set the SKB flags */\n\tskb_checksum_none_assert(skb);\n\tif (likely(rx_buf->flags & EFX_RX_PKT_CSUMMED)) {\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tskb->csum_level = !!(rx_buf->flags & EFX_RX_PKT_CSUM_LEVEL);\n\t}\n\n\tefx_rx_skb_attach_timestamp(channel, skb);\n\n\tif (channel->type->receive_skb)\n\t\tif (channel->type->receive_skb(channel, skb))\n\t\t\treturn;\n\n\t/* Pass the packet up */\n\tnetif_receive_skb(skb);\n}\n\n/* Handle a received packet.  Second half: Touches packet payload. */\nvoid __efx_rx_packet(struct efx_channel *channel)\n{\n\tstruct efx_nic *efx = channel->efx;\n\tstruct efx_rx_buffer *rx_buf =\n\t\tefx_rx_buffer(&channel->rx_queue, channel->rx_pkt_index);\n\tu8 *eh = efx_rx_buf_va(rx_buf);\n\n\t/* Read length from the prefix if necessary.  This already\n\t * excludes the length of the prefix itself.\n\t */\n\tif (rx_buf->flags & EFX_RX_PKT_PREFIX_LEN)\n\t\trx_buf->len = le16_to_cpup((__le16 *)\n\t\t\t\t\t   (eh + efx->rx_packet_len_offset));\n\n\t/* If we're in loopback test, then pass the packet directly to the\n\t * loopback layer, and free the rx_buf here\n\t */\n\tif (unlikely(efx->loopback_selftest)) {\n\t\tstruct efx_rx_queue *rx_queue;\n\n\t\tefx_loopback_rx_packet(efx, eh, rx_buf->len);\n\t\trx_queue = efx_channel_get_rx_queue(channel);\n\t\tefx_free_rx_buffers(rx_queue, rx_buf,\n\t\t\t\t    channel->rx_pkt_n_frags);\n\t\tgoto out;\n\t}\n\n\tif (unlikely(!(efx->net_dev->features & NETIF_F_RXCSUM)))\n\t\trx_buf->flags &= ~EFX_RX_PKT_CSUMMED;\n\n\tif ((rx_buf->flags & EFX_RX_PKT_TCP) && !channel->type->receive_skb)\n\t\tefx_rx_packet_gro(channel, rx_buf, channel->rx_pkt_n_frags, eh);\n\telse\n\t\tefx_rx_deliver(channel, eh, rx_buf, channel->rx_pkt_n_frags);\nout:\n\tchannel->rx_pkt_n_frags = 0;\n}\n\nint efx_probe_rx_queue(struct efx_rx_queue *rx_queue)\n{\n\tstruct efx_nic *efx = rx_queue->efx;\n\tunsigned int entries;\n\tint rc;\n\n\t/* Create the smallest power-of-two aligned ring */\n\tentries = max(roundup_pow_of_two(efx->rxq_entries), EFX_MIN_DMAQ_SIZE);\n\tEFX_WARN_ON_PARANOID(entries > EFX_MAX_DMAQ_SIZE);\n\trx_queue->ptr_mask = entries - 1;\n\n\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t  \"creating RX queue %d size %#x mask %#x\\n\",\n\t\t  efx_rx_queue_index(rx_queue), efx->rxq_entries,\n\t\t  rx_queue->ptr_mask);\n\n\t/* Allocate RX buffers */\n\trx_queue->buffer = kcalloc(entries, sizeof(*rx_queue->buffer),\n\t\t\t\t   GFP_KERNEL);\n\tif (!rx_queue->buffer)\n\t\treturn -ENOMEM;\n\n\trc = efx_nic_probe_rx(rx_queue);\n\tif (rc) {\n\t\tkfree(rx_queue->buffer);\n\t\trx_queue->buffer = NULL;\n\t}\n\n\treturn rc;\n}\n\nstatic void efx_init_rx_recycle_ring(struct efx_nic *efx,\n\t\t\t\t     struct efx_rx_queue *rx_queue)\n{\n\tunsigned int bufs_in_recycle_ring, page_ring_size;\n\n\t/* Set the RX recycle ring size */\n#ifdef CONFIG_PPC64\n\tbufs_in_recycle_ring = EFX_RECYCLE_RING_SIZE_IOMMU;\n#else\n\tif (iommu_present(&pci_bus_type))\n\t\tbufs_in_recycle_ring = EFX_RECYCLE_RING_SIZE_IOMMU;\n\telse\n\t\tbufs_in_recycle_ring = EFX_RECYCLE_RING_SIZE_NOIOMMU;\n#endif /* CONFIG_PPC64 */\n\n\tpage_ring_size = roundup_pow_of_two(bufs_in_recycle_ring /\n\t\t\t\t\t    efx->rx_bufs_per_page);\n\trx_queue->page_ring = kcalloc(page_ring_size,\n\t\t\t\t      sizeof(*rx_queue->page_ring), GFP_KERNEL);\n\trx_queue->page_ptr_mask = page_ring_size - 1;\n}\n\nvoid efx_init_rx_queue(struct efx_rx_queue *rx_queue)\n{\n\tstruct efx_nic *efx = rx_queue->efx;\n\tunsigned int max_fill, trigger, max_trigger;\n\n\tnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\n\t\t  \"initialising RX queue %d\\n\", efx_rx_queue_index(rx_queue));\n\n\t/* Initialise ptr fields */\n\trx_queue->added_count = 0;\n\trx_queue->notified_count = 0;\n\trx_queue->removed_count = 0;\n\trx_queue->min_fill = -1U;\n\tefx_init_rx_recycle_ring(efx, rx_queue);\n\n\trx_queue->page_remove = 0;\n\trx_queue->page_add = rx_queue->page_ptr_mask + 1;\n\trx_queue->page_recycle_count = 0;\n\trx_queue->page_recycle_failed = 0;\n\trx_queue->page_recycle_full = 0;\n\n\t/* Initialise limit fields */\n\tmax_fill = efx->rxq_entries - EFX_RXD_HEAD_ROOM;\n\tmax_trigger =\n\t\tmax_fill - efx->rx_pages_per_batch * efx->rx_bufs_per_page;\n\tif (rx_refill_threshold != 0) {\n\t\ttrigger = max_fill * min(rx_refill_threshold, 100U) / 100U;\n\t\tif (trigger > max_trigger)\n\t\t\ttrigger = max_trigger;\n\t} else {\n\t\ttrigger = max_trigger;\n\t}\n\n\trx_queue->max_fill = max_fill;\n\trx_queue->fast_fill_trigger = trigger;\n\trx_queue->refill_enabled = true;\n\n\t/* Set up RX descriptor ring */\n\tefx_nic_init_rx(rx_queue);\n}\n\nvoid efx_fini_rx_queue(struct efx_rx_queue *rx_queue)\n{\n\tint i;\n\tstruct efx_nic *efx = rx_queue->efx;\n\tstruct efx_rx_buffer *rx_buf;\n\n\tnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\n\t\t  \"shutting down RX queue %d\\n\", efx_rx_queue_index(rx_queue));\n\n\tdel_timer_sync(&rx_queue->slow_fill);\n\n\t/* Release RX buffers from the current read ptr to the write ptr */\n\tif (rx_queue->buffer) {\n\t\tfor (i = rx_queue->removed_count; i < rx_queue->added_count;\n\t\t     i++) {\n\t\t\tunsigned index = i & rx_queue->ptr_mask;\n\t\t\trx_buf = efx_rx_buffer(rx_queue, index);\n\t\t\tefx_fini_rx_buffer(rx_queue, rx_buf);\n\t\t}\n\t}\n\n\t/* Unmap and release the pages in the recycle ring. Remove the ring. */\n\tfor (i = 0; i <= rx_queue->page_ptr_mask; i++) {\n\t\tstruct page *page = rx_queue->page_ring[i];\n\t\tstruct efx_rx_page_state *state;\n\n\t\tif (page == NULL)\n\t\t\tcontinue;\n\n\t\tstate = page_address(page);\n\t\tdma_unmap_page(&efx->pci_dev->dev, state->dma_addr,\n\t\t\t       PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t       DMA_FROM_DEVICE);\n\t\tput_page(page);\n\t}\n\tkfree(rx_queue->page_ring);\n\trx_queue->page_ring = NULL;\n}\n\nvoid efx_remove_rx_queue(struct efx_rx_queue *rx_queue)\n{\n\tnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\n\t\t  \"destroying RX queue %d\\n\", efx_rx_queue_index(rx_queue));\n\n\tefx_nic_remove_rx(rx_queue);\n\n\tkfree(rx_queue->buffer);\n\trx_queue->buffer = NULL;\n}\n\n\nmodule_param(rx_refill_threshold, uint, 0444);\nMODULE_PARM_DESC(rx_refill_threshold,\n\t\t \"RX descriptor ring refill threshold (%)\");\n\n#ifdef CONFIG_RFS_ACCEL\n\nint efx_filter_rfs(struct net_device *net_dev, const struct sk_buff *skb,\n\t\t   u16 rxq_index, u32 flow_id)\n{\n\tstruct efx_nic *efx = netdev_priv(net_dev);\n\tstruct efx_channel *channel;\n\tstruct efx_filter_spec spec;\n\tstruct flow_keys fk;\n\tint rc;\n\n\tif (flow_id == RPS_FLOW_ID_INVALID)\n\t\treturn -EINVAL;\n\n\tif (!skb_flow_dissect_flow_keys(skb, &fk, 0))\n\t\treturn -EPROTONOSUPPORT;\n\n\tif (fk.basic.n_proto != htons(ETH_P_IP) && fk.basic.n_proto != htons(ETH_P_IPV6))\n\t\treturn -EPROTONOSUPPORT;\n\tif (fk.control.flags & FLOW_DIS_IS_FRAGMENT)\n\t\treturn -EPROTONOSUPPORT;\n\n\tefx_filter_init_rx(&spec, EFX_FILTER_PRI_HINT,\n\t\t\t   efx->rx_scatter ? EFX_FILTER_FLAG_RX_SCATTER : 0,\n\t\t\t   rxq_index);\n\tspec.match_flags =\n\t\tEFX_FILTER_MATCH_ETHER_TYPE | EFX_FILTER_MATCH_IP_PROTO |\n\t\tEFX_FILTER_MATCH_LOC_HOST | EFX_FILTER_MATCH_LOC_PORT |\n\t\tEFX_FILTER_MATCH_REM_HOST | EFX_FILTER_MATCH_REM_PORT;\n\tspec.ether_type = fk.basic.n_proto;\n\tspec.ip_proto = fk.basic.ip_proto;\n\n\tif (fk.basic.n_proto == htons(ETH_P_IP)) {\n\t\tspec.rem_host[0] = fk.addrs.v4addrs.src;\n\t\tspec.loc_host[0] = fk.addrs.v4addrs.dst;\n\t} else {\n\t\tmemcpy(spec.rem_host, &fk.addrs.v6addrs.src, sizeof(struct in6_addr));\n\t\tmemcpy(spec.loc_host, &fk.addrs.v6addrs.dst, sizeof(struct in6_addr));\n\t}\n\n\tspec.rem_port = fk.ports.src;\n\tspec.loc_port = fk.ports.dst;\n\n\trc = efx->type->filter_rfs_insert(efx, &spec);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t/* Remember this so we can check whether to expire the filter later */\n\tchannel = efx_get_channel(efx, rxq_index);\n\tchannel->rps_flow_id[rc] = flow_id;\n\t++channel->rfs_filters_added;\n\n\tif (spec.ether_type == htons(ETH_P_IP))\n\t\tnetif_info(efx, rx_status, efx->net_dev,\n\t\t\t   \"steering %s %pI4:%u:%pI4:%u to queue %u [flow %u filter %d]\\n\",\n\t\t\t   (spec.ip_proto == IPPROTO_TCP) ? \"TCP\" : \"UDP\",\n\t\t\t   spec.rem_host, ntohs(spec.rem_port), spec.loc_host,\n\t\t\t   ntohs(spec.loc_port), rxq_index, flow_id, rc);\n\telse\n\t\tnetif_info(efx, rx_status, efx->net_dev,\n\t\t\t   \"steering %s [%pI6]:%u:[%pI6]:%u to queue %u [flow %u filter %d]\\n\",\n\t\t\t   (spec.ip_proto == IPPROTO_TCP) ? \"TCP\" : \"UDP\",\n\t\t\t   spec.rem_host, ntohs(spec.rem_port), spec.loc_host,\n\t\t\t   ntohs(spec.loc_port), rxq_index, flow_id, rc);\n\n\treturn rc;\n}\n\nbool __efx_filter_rfs_expire(struct efx_nic *efx, unsigned int quota)\n{\n\tbool (*expire_one)(struct efx_nic *efx, u32 flow_id, unsigned int index);\n\tunsigned int channel_idx, index, size;\n\tu32 flow_id;\n\n\tif (!spin_trylock_bh(&efx->filter_lock))\n\t\treturn false;\n\n\texpire_one = efx->type->filter_rfs_expire_one;\n\tchannel_idx = efx->rps_expire_channel;\n\tindex = efx->rps_expire_index;\n\tsize = efx->type->max_rx_ip_filters;\n\twhile (quota--) {\n\t\tstruct efx_channel *channel = efx_get_channel(efx, channel_idx);\n\t\tflow_id = channel->rps_flow_id[index];\n\n\t\tif (flow_id != RPS_FLOW_ID_INVALID &&\n\t\t    expire_one(efx, flow_id, index)) {\n\t\t\tnetif_info(efx, rx_status, efx->net_dev,\n\t\t\t\t   \"expired filter %d [queue %u flow %u]\\n\",\n\t\t\t\t   index, channel_idx, flow_id);\n\t\t\tchannel->rps_flow_id[index] = RPS_FLOW_ID_INVALID;\n\t\t}\n\t\tif (++index == size) {\n\t\t\tif (++channel_idx == efx->n_channels)\n\t\t\t\tchannel_idx = 0;\n\t\t\tindex = 0;\n\t\t}\n\t}\n\tefx->rps_expire_channel = channel_idx;\n\tefx->rps_expire_index = index;\n\n\tspin_unlock_bh(&efx->filter_lock);\n\treturn true;\n}\n\n#endif /* CONFIG_RFS_ACCEL */\n\n/**\n * efx_filter_is_mc_recipient - test whether spec is a multicast recipient\n * @spec: Specification to test\n *\n * Return: %true if the specification is a non-drop RX filter that\n * matches a local MAC address I/G bit value of 1 or matches a local\n * IPv4 or IPv6 address value in the respective multicast address\n * range.  Otherwise %false.\n */\nbool efx_filter_is_mc_recipient(const struct efx_filter_spec *spec)\n{\n\tif (!(spec->flags & EFX_FILTER_FLAG_RX) ||\n\t    spec->dmaq_id == EFX_FILTER_RX_DMAQ_ID_DROP)\n\t\treturn false;\n\n\tif (spec->match_flags &\n\t    (EFX_FILTER_MATCH_LOC_MAC | EFX_FILTER_MATCH_LOC_MAC_IG) &&\n\t    is_multicast_ether_addr(spec->loc_mac))\n\t\treturn true;\n\n\tif ((spec->match_flags &\n\t     (EFX_FILTER_MATCH_ETHER_TYPE | EFX_FILTER_MATCH_LOC_HOST)) ==\n\t    (EFX_FILTER_MATCH_ETHER_TYPE | EFX_FILTER_MATCH_LOC_HOST)) {\n\t\tif (spec->ether_type == htons(ETH_P_IP) &&\n\t\t    ipv4_is_multicast(spec->loc_host[0]))\n\t\t\treturn true;\n\t\tif (spec->ether_type == htons(ETH_P_IPV6) &&\n\t\t    ((const u8 *)spec->loc_host)[0] == 0xff)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n"
}