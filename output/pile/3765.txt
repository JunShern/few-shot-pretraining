{
    "criteria": [
        {
            "criterion": "AllDocuments",
            "passed": true,
            "reason": "All documents pass."
        },
        {
            "criterion": "Domain",
            "passed": false,
            "reason": "Document source unavailable."
        },
        {
            "criterion": "QuestionAnswerStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "QuestionAnswerStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "FullyStructured",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ListPrefix",
            "passed": false,
            "reason": "Found 0 list prefixes. (Min: 5)"
        },
        {
            "criterion": "ListPrefixV2",
            "passed": true,
            "reason": "Text contains ['-', '-', '-', '-', '-', '-', '-', '*', '*', '*', '*', '*', '*', '*', '-', '1', '2', '3', '4', '5', '6', '-', '-', '-', '1', '4', '6', '1', '4', '-', '-', '-', '-', '-', '-', '-', '-']."
        }
    ],
    "doc_id": "3765",
    "text": "---\nabstract: 'In this paper, we describe our method for DCASE2019 task\u00a03: Sound Event Localization and Detection (SELD). We use four CRNN SELDnet-like single output models which run in a consecutive manner to recover all possible information of occurring events. We decompose the SELD task into estimating number of active sources, estimating direction of arrival of a single source, estimating direction of arrival of the second source where the direction of the first one is known and a multi-label classification task. We use custom consecutive ensemble to predict events\u2019 onset, offset, direction of arrival and class. The proposed approach is evaluated on the TAU Spatial Sound Events 2019 - Ambisonic and it is compared with other participants\u2019 submissions.'\naddress: |\n    Samsung R&D Institute Poland\\\n    Artificial Intelligence\\\n    Warsaw, 00-844, Poland\\\n    {s.kapka, m.lewandows4}@samsung.com\nbibliography:\n- 'refs.bib'\ntitle: 'Sound source detection, localization and classification using consecutive ensemble of CRNN models'\n---\n\nDCASE 2019, Sound Event Localization and Detection, CRNN, Ambisonics\n\nIntroduction {#sec:intro}\n============\n\nSound Event Localization and Detection (SELD) is a complex task which naturally appears when one wants to develop a system that possesses spatial awareness of the surrounding world using multi-channel audio signals. This year, the task 3 from the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2019) [@dcase2019web] concerned the SELD problem. SELDnet introduced in [@Adavanne2018_JSTSP] is a single system of a good quality designed for the SELD task, and the slight modification of SELDnet was set as the baseline system [@Adavanne2019_DCASE] during the DCASE 2019 Challenge. Solely based on [@Adavanne2018_JSTSP] and [@Adavanne2019_DCASE], we develop a novel system designed for the task 3 from the DCASE 2019 Challenge.\n\nIn our work, we follow the philosophy that if a complex problem can be split into simpler ones, one should do so. Thus we decompose the SELD task with up to 2 active sound sources into the following subtasks:\n\n-   estimating the number of active sources (*noas*),\n\n-   estimating the direction of arrival of a sound event when there is one active sound source (*doa1*),\n\n-   estimating the direction of arrival of a sound event when there are two active sound sources and we posses the knowledge of the direction of arrival of one of these sound events, which we will call an *associated event* (*doa2*),\n\n-   multi-label classification of sound events (*class*).\n\nFor each of this subtasks, we develop a SELDnet-like convolutional recurrent neural network (CRNN) with a single output. We discuss it in detail in section \\[sec:architect\\]. Given such models, we develop a custom consecutive ensemble of these models. This allows us to predict the events\u2019 onset, offset, direction of arrival and class, which we discuss in detail in section \\[sec:ensemble\\]. Due to the sequential nature of generating predictions in our system, errors in models\u2019 predictions may cascade, and thus an overall error may cumulate. Despite this drawback, our system acquire very good results on the TAU Spatial Sound Events 2019 - Ambisonic database. We discuss the results in detail in section \\[sec:results\\].\n\n![An example of the normalised amplitude spectrogram in the decibel scale and the normalised phase spectrogram obtained from the first *foa* channel from some randomly selected recording. The horizontal and vertical axes denote frame numbers and frequencies respectively obtained from the STFT. Note that the values from the legends on the right are dimensionless due to the normalization used in the preprocessing.[]{data-label=\"fig:spectrograms\"}](spectrogram_final.png){width=\"\\columnwidth\"}\n\nFeatures {#sec:features}\n========\n\nThe DCASE 2019 task 3 provides two formats of the TAU Spatial Sound Events 2019 dataset: first order ambisonic (*foa*) and 4 channels from a microphone array (*mic*) [@Adavanne2019_DCASE]. In our method we only use the ambisonic format.\n\nEach recording is approximately 1 minute long with sampling rate of 48k. We use the short time Fourier transform (STFT) with Hann window. We use the window of length 0.4s and hop of length 0.2s in STFT to transform a raw audio associated to each *foa* channel into the complex spectrogram of size 3000x1024. If audio is longer than 1 minute, we truncate spectrograms. If an audio is shorter than 1 minute, we pad them with zeros.\n\nFrom each complex spectrogram we extract its module and phase point-wise, that is amplitude and phase spectrograms, respectively. We transform amplitude spectrograms to the decibel scale. Finally, we standardize all spectrograms frequency-wise to zero mean and unit variance, to obtain spectrograms as in Figure \\[fig:spectrograms\\].\n\nIn summary, from each recording we acquire 4 standardized amplitude spectrograms in the decibel scale and 4 standardized phase spectrograms corresponding to 4 *foa* channels.\n\nArchitecture {#sec:architect}\n============\n\nAs mentioned in the introduction, each of the subtasks (*noas*, *doa1*, *doa2* and *class*) has its own SELDnet-like CRNN. Each of these models is a copy of a single SELDnet node with just minor adjustments so that it fits to the specific subtask and for the regularization purpose.\n\nEach of these models takes as an input a fixed length subsequence of decibel scale amplitude spectrograms (in case of *noas* and *class* subtasks) or both decibel scale amplitude and phase spectrograms (in case of *doa1* and *doa2* subtasks) from all 4 channels.\n\nIn each case, the input layers are followed by 3 convolutional layer blocks. Each block is made of a convolutional layer, batch norm, relu activation, maxpool and dropout. The output from the last convolutional block is reshaped so that it forms a multivariate sequence of a fixed length. In the case of *doa2*, we additionaly concatenate directions of arrivals of associated events with this multivariate sequence. Next, there are two recurrent layers (GRU or LSTM) with 128 units each with dropout and recurrent dropout. Next layer is a time distributed dense layer with dropout and with the number of units depending on subtask.\n\nLastly, depending on a subtask, the model has a different output. For *noas*, the model has just a single time distributed output that corresponds to the number of active sources (0, 1 or 2). For *doa1* and *doa2*, the models have 3 time distributed outputs that corresponds to cartesian xyz coordinates as in [@Adavanne2018_JSTSP]. Cartesian coordinates are advantageous over spherical coordinates in this task due to their continuity. Lastly, for *class*, the model has 11 time distributed outputs corresponding to 11 possible classes. We present the detailed architecture in Table \\[tab:parameteres\\].\n\n[\\*6l]{} **Layer Type** & **Parameters** & *noas* & *doa1* & *doa2* & *class*\\\nInput & Shape & $256 \\times 1024 \\times 4$ & $128 \\times 1024 \\times 8$ & $128 \\times 1024 \\times 8$ & $128 \\times 1024 \\times 4$\\\n*ConvBlock\\** & Pool & 8 & 8 & 8 & 8\\\n*ConvBlock\\** & Pool & 8 & 8 & 8 & 8\\\n*ConvBlock\\** & Pool & 4 & 4 & 4 & 4\\\nReshape & Sequence length $\\times$ features & $256 \\times -1$ & $128 \\times -1$ & $128 \\times -1$ & $128 \\times -1$\\\nDoa2 input & Is used & False & False & True & False\\\nConcatenate & Is used & False & False & True & False\\\n*RecBlock\\*\\** & Unit type & GRU & LSTM & GRU & GRU\\\n*RecBlock\\*\\** & Unit type & GRU & LSTM & GRU & GRU\\\nTD Dense & Number of units & $16$ & $128$ & $128$ & $16$\\\nDropout & Dropout rate & $0.2$ & $0.2$ & $0.2$ & $0.2$\\\nTD Dense & Number of units & $1$ & $3$ & $3$ & $11$\\\nActivation & Function & linear & linear & linear & sigmoid\\\n*\\*ConvBlock$(P)$*&\\\nConv2D &\\\nBatchNorm & \u2014\\\nActivation &\\\nMaxPooling2D &\\\nDropout &\\\n*\\*\\*RecBlock$(U)$*&\\\nRecurrent &\\\nActivation &\\\nDropout &\\\n\n\\[tab:parameteres\\]\n\nDepending on a subtask, we feed the network with the whole recordings or just their parts. For *noas*, we feed all the data. For *doa1*, we extract only those parts of the recordings where there is just one sound source active. For *doa2*, we extract only those parts of the recordings where there are exactly two active sound sources. For *class*, we extract those parts of the recordings where there are at least one active source.\n\nAs for the learning process, we used mean square error loss for the *noas*, *doa1*, *doa2* subtasks and binary cross-entropy loss for the *class* subtask. For all subtasks we initialised learning process using Adam optimizer with default parameters [@Adam]. The *noas* and *class* subtasks were learned for 500 epochs with exponential learning rate decay; every 5 epochs the learning rate were multiplied by 0.95. In *doa1* and *doa2* subtasks, we run learning process for 1000 epochs without changing the initial learning rate.\n\nAs for complexity, the *noas*, *doa1*, *doa2* and *class* have , , and parameters respectively, making total of parameters.\n\nConsecutive ensemble {#sec:ensemble}\n====================\n\nIn this section, we introduce and describe the idea of the consecutive ensemble which is the core of our approach. This custom binding of our four models allows us to predict the events\u2019 onset, offset, direction of arrival and class.\n\nThe algorithm {#ssec:algorithm}\n-------------\n\nWe assume that recordings have at most 2 active sound sources at once and the sound events occur on a 10 degrees resolution grid. In our setting, the audios after feature extraction have exactly 3000 vectors corresponding to the time dimension. Henceforth we will call these vectors as frames. The algorithm itself goes as follows:\n\n1\\. We feed the features to the *noas* network to predict the number of active sources (NOAS) in each frame.\n\n2\\. We transform the predicted NOAS so that each recording starts and ends with no sound sources and the difference of NOAS between each frames is no greater than 1.\n\n3\\. From the predicted NOAS we deduce the number of events, their onsets and the list of possible offsets for each event. If NOAS in two consecutive frames increases, then we predict that a new event happened at the second frame. If in two consecutive frames NOAS decreases, then we append the first frame to all events since last time NOAS was 0 as a possible offset.\n\n4\\. In order to determine which offset corresponds to which event we use the *doa1* network. We extract chunks (intervals of equal NOAS) of audio where the predicted NOAS equals 1 and we feed it to *doa1* network. For each chunk where NOAS was 1 we predict the average azimuth and elevation, and we round it to the closest multiple of 10. If two consecutive chunks have the same azimuth and elevation then we conclude that the first event covered two chunks and the second event started and ended between those chunks. If two consecutive chunks have a different azimuth or elevation, then we conclude that the first event ended when the second chunk started and the second event continued in the second chunk.\n\n5\\. To determine the remaining information about angles we need to predict the direction of arrival (DOA) of events that start and end while the associated event is happening. We feed the chunks where NOAS is 2 to the *doa2* network with the second input being DOA of the associated event in cartesian xyz coordinates. Similarly as in step 4, we average the predicted results from chunks and round it to the closest multiple of 10.\n\n6\\. Lastly, we predict the events\u2019 classes. If an event has chunks where the event is happening in an isolation (NOAS = 1), then all such chunks are feed to the *class* network and the most probable class (using soft voting among frames) is taken as a predicted class. If an event has no such chunks, i.e. the event is only happening with an associated event, then such chunk (NOAS = 2) is fed to the network and two most probable classes are extracted. We choose the first one which does not equal to the class of the associated event.\n\nAn example {#ssec:example}\n----------\n\nThe algorithm itself may seem quite complex at first glance. Hence, we investigate here a concrete example.\n\nGiven a recording constituting of 3000 vectors, we predict its NOAS in each frame as in Figure \\[fig:noas\\]. For the sake of clarity we constrain only to a part of the recording. Consider a block with predicted NOAS as in the top plot from Figure \\[fig:example2\\]. According to the step 3 from the algorithm, we predict that 3 events happened here: $E_1, E_2, E_3$ with 3 corresponding onsets $On_1, On_2, On_3$. Events $E_1$ and $E_2$ may end at $Off_1, Off_2$ or $Off_3$ and event $E_3$ may end at $Off_2$ or $Off_3$ (see the bottom plot from Figure \\[fig:example2\\]). According to the step 4 from the algorithm, we predict DOA using *doa1* in chunks from $On_1$ to $On_2$, from $Off_1$ to $On_3$ and from $Off_2$ to $Off_3$. Based on that we deduce the events\u2019 offsets as in Figure \\[fig:example2\\]. Based on step 5 from the algorithm, we predict the DOA of chunk from $On_3$ to $Off_2$ using *doa2* where the associated DOA is the DOA of $E_2$. Lastly we deduce classes of the events $E_1, E_2$ and $E_3$. According to the step 6 form the algorithm, we predict class of $E_1$ based on the chunk from $On_1$ to $On_2$, predict the class of $E_2$ based on chunks from $Off_1$ to $On_3$ and from $Off_2$ to $Off_3$. Finally, we predict the class of $E_3$ based on the chunk from $On_3$ to $Off_2$. If the predicted class of $E_3$ is the same as the class of $E_2$ then we predict it to be the second most probable class from the *class* network.\n\n![The plot visualising the predicted number of active sources for some randomly selected recording.[]{data-label=\"fig:noas\"}](noas_final.png){width=\"\\columnwidth\"}\n\n![image](example_final.png){width=\"\\textwidth\"}\n\nResults {#sec:results}\n=======\n\nWe evaluate our results on TAU Spatial Sound Events 2019 - Ambisonic dataset. This dataset constitutes of two parts: the development and evaluation sets. The development part consists of 400 recordings with predefined 4-fold cross-validation and the evaluation part consists of 100 recordings. The results from this section relate to our submission `Kapka_SRPOL_task3_2`.\n\nDevelopment phase {#ssec:develop}\n-----------------\n\nAs for the development part, we used 2 splits out of 4 for training for every fold using the suggested cross-validation even though validation splits do not influence the training process.\n\nWe show in Table \\[tab:results\\] the averaged metrics from all folds for our setting and metrics for the baseline [@Adavanne2019_DCASE]. In order to demonstrate the variance among folds, we present in Table \\[tab:test\\_splits\\] the detailed results on the test splits from each fold. The development set provides the distinction for the files where there is up to 1 active sound source at once (ov1) and where there are up to 2 (ov2). In Table \\[tab:ov1\\_ov2\\] we compare metrics for the ov1 and ov2 subsets.\n\n[l\\*5c]{} & Error rate & F-score & DOA error & Frame recall & Seld score\\\nTrain & 0.03 & 0.98 & 2.71 & 0.98 & 0.02\\\nVal. & 0.15 & 0.89 & 4.81 & 0.95 & 0.08\\\nTest & 0.14 & 0.90 & 4.75 & 0.95 & 0.08\\\nBaseline & 0.34 & 0.80 & 28.5 & 0.85 & 0.22\\\n\n\\[tab:results\\]\n\n[l\\*5c]{} & Error rate & F-score & DOA error & Frame recall & Seld score\\\nSplit 1 & 0.13 & 0.91 & 6.01 & 0.95 & 0.07\\\nSplit 2 & 0.16 & 0.88 & 6.01 & 0.95 & 0.09\\\nSplit 3 & 0.11 & 0.93 & 4.93 & 0.96 & 0.06\\\nSplit 4 & 0.17 & 0.86 & 5.89 & 0.96 & 0.10\\\n\n\\[tab:test\\_splits\\]\n\n[l\\*5c]{} & Error rate & F-score & DOA error & Frame recall & Seld score\\\nov1 & 0.07 & 0.94 & 1.28 & 0.99 & 0.04\\\nov2 & 0.18 & 0.87 & 7.96 & 0.93 & 0.11\\\n\n\\[tab:ov1\\_ov2\\]\n\nOfficial results {#ssec:official}\n----------------\n\nFor the evaluation part, we used all 4 splits for training from the development set. We compare our final results with the selected submissions in Table \\[tab:comparison\\].\n\nThe idea of decomposing the SELD task into simpler ones proved to be a very popular idea among contestants. The recent two-stage approach to SELD introduced in [@Cao_oryginal] was used and developed further by many. The best submission using two-step approach `Cao_Surrey_task3_4` [@Cao] obtained results very similar to ours. `He_THU_task3_2` [@He] and `Chang_HYU_task3_3` [@Chang] outperform our submission in SED metrics and DOA error respectively. However, our approach based on estimating NOAS first allows us to outperform all contestants in frame recall.\n\n[\\*2l\\*4c]{} **Rank** & **Submission name** & **Error rate** & **F-score** & **DOA error** & **Frame recall**\\\n1 & `Kapka_SRPOL_task3_2` & 0.08 & 94.7 & 3.7 & **96.8**\\\n4 & `Cao_Surrey_task3_4` & 0.08 & 95.5 & 5.5 & 92.2\\\n6 & `He_THU_task3_2` & **0.06** & **96.7** & 22.4 & 94.1\\\n19 & `Chang_HYU_task3_3` & 0.14 & 91.9 & **2.7** & 90.8\\\n48 & `DCASE2019_FOA_baseline` & 0.28 & 85.4 & 24.6 & 85.7\\\n\n\\[tab:comparison\\]\n\nSubmissions {#sec:submission}\n===========\n\nOverall, we created 4 submissions for the competition:\n\n-   ConseqFOA (`Kapka_SRPOL_task3_2`),\n\n-   ConseqFOA1 (`Kapka_SRPOL_task3_3`),\n\n-   ConseqFOAb (`Kapka_SRPOL_task3_4`),\n\n-   MLDcT32019 (`Lewandowski_SRPOL_task3_1`).\n\nThe first three submissions use the approach described in the above sections. The only difference is that ConseqFOA is trained on all four splits from development dataset. ConseqFOA1 is trained on splits 2,3,4. ConseqFOAb is trained on all splits but the classifier in this version was trained using categorical cross-entropy instead of binary cross-entropy loss.\n\nOur MLDcT32019 submission uses a different approach. It works in the same way as the original SELDnet architecture but with the following differences:\n\n-   We implemented the Squeeze-and-Excitation block [@Hu_2018_CVPR] after the last convolutional block. We pass the output from the last convolutional block through two densely connected neural layers with respectively 1 and 4 neurons, we multiply it with the output of the last convolutional block and we pass it further to recurrent layers.\n\n-   We set all dropout rates to $0.2$.\n\n-   We used SpecAugment [@specAug] as an augmentation technique to double the training dataset.\n\n-   We replaced recurrent layer GRU units with LSTM units.\n\nConclusion {#sec:conclusion}\n==========\n\nWe conclude that decomposing the SELD problem into simpler tasks is instinctive and efficient. However, we are aware that our solution has some serious limitations and it fails when one wants to consider a more general setup. For example when there are more than 2 active sources at once or when the grid resolution is more refined. Thus, we claim that the pursuit for universal and efficient SELD solutions is still open.\n\nAcknowledgement {#sec:acknowledgement}\n===============\n\nWe are most grateful to Zuzanna Kwiatkowska for spending her time on careful reading with a deep understanding the final draft of this paper.\n"
}