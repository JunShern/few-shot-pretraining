{
    "criteria": [
        {
            "criterion": "AllDocuments",
            "passed": true,
            "reason": "All documents pass."
        },
        {
            "criterion": "QuestionAnswerStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "FullyStructured",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ListPrefixV2",
            "passed": true,
            "reason": "Text contains ['2', '-', '6', '2', '2', '-', '-', '-', '-', '6', '-', '-']."
        },
        {
            "criterion": "ExamplesMinimalEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesSynonymsEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesDiverseEmbed",
            "passed": true,
            "reason": "Text contains ['Can you post a few examples of the encoding? (0.179)']."
        }
    ],
    "doc_id": "9881",
    "text": "\nMaze62: a dense and speedy alphanumeric encoding for binary data - DenisM\nhttp://blog.altudov.com/2011/05/16/maze62-a-dense-and-speedy-alphanumeric-encoding-for-binary-data/\n======\nandfarm\nI'm having trouble thinking of any situations where you're free to use any\nalphanumerics, but you absolutely can't use anything else. The closest thing\nthat comes to mind is DNS, and that's a 36-character set, not 62.\n\nAscii85 (<http://en.wikipedia.org/wiki/Base85>) uses a similar concept, but\nusing the majority of the printable ASCII characters. It gets a hard-to-beat\n20% expansion on binary data (4 bytes in 5 chars) without requiring bigint\nmath.\n\n~~~\ndeweller\nI'm pretty sure the canonical use case for this would be for including complex\nbinary data in a URL. A URL shortener might be a good example.\n\n~~~\nDenisM\nAnother thing people don't talk much about is copy-paste. Double click on an\nalphanumeric will select the entire thing, whereas other characters might\ncause selection to end prematurely, and differently so in different text\neditors.\n\n------\nwaterhouse\nI think this has the drawback that if you want to look up, say, bits n through\nm, you'd need to process the whole string up to that point. Lookup would be\nO(n) rather than O(1). Is that a problem? (I don't know, maybe people only use\nbase64-like encoding in places where they'd just read the data in once at the\nbeginning, convert it to a better internal representation, and never touch the\nbase64 version again.)\n\nOn the other hand, if it is a problem, how's this idea: Use strings of 129\nbase-62 digits to represent strings of 128 base-64 digits. I pick 128 because\nit might be convenient, and 129/128 is close to optimal:\n\n    \n    \n      arc> (* 129 (log 62 64))\n      128.01522067331783\n    \n\nOr if decoding 128 digits at a time when you just want a couple is excessive,\nyou could use 17 base-62 digits for 16 base-64 digits (which has a space\ninefficiency of 1/16th, or 6.25%), or 9 for 8 (inefficiency 12.5%), or\nwhatever.\n\n~~~\nDenisM\nRandom access was not a priority, yes. In fact it didn't even occur to me, not\neven for a moment :)\n\nAs to your proposed algorithm, yes, it could work. It's discussed in passing a\nbit earlier in this thread, here's direct link:\n<http://news.ycombinator.com/item?id=2554726>\n\nNot sure about comparative space efficiency of this vs Maze64, but we're\nlikely splitting hairs at this point :), it should be very close. Yes it could\nwork. Converting 128 characters from base64 to base62 would cost N^2 time,\nwhere N is 128. Not sure if it's a big deal. Just another approach.\n\n~~~\nwaterhouse\nHmm, and one could extend this chunking approach to Maze62. Consume 12 bits\n(aka two base64 integers), but if the resulting number is in the range\n[62^2=3844 to 64^2-1=4095] or [0 to 64^2-62^2-1=251], then add that extra\nquotient bit to the next input (from which you'll consume only 11 bits). That\nwould make the worst case only take 12/11ths as long, or 109%, instead of\n6/5ths as long, or 120%, as the best case (which would be the same as before).\n\nOne could extend it upwards as far as desired (probably approaching log_64(62)\nworst-case and average-case space compared to base64[1]), though the integer\narithmetic would become a pain eventually. 12/11 or maybe 18/17 (105.8%) or\n24/23 (104.3%) is probably good enough for most purposes. The relatively\nhardcore might use 60/59 (101.7%), which x86_64 machines can still do with CPU\narithmetic (an integer multiply of two 64-bit integers stores the results in\ntwo 64-bit registers, and you can then do an integer divide on that, which\nstores the quotient and remainder in those registers).\n\n[1] Average case: I analyze it like this. If you encode n base64 integers at a\ntime, then you'll encode 6n bits at a time, but 6n-1 bits if you're unlucky\nand get a number in the ranges [62^n, 64^n - 1] or [0, 64^n - 62^n - 1]. These\ntwo ranges, taken together, make 2 * (64^n - 62^n) numbers out of 64^n\npossible n-digit base64 numbers. Thus, on average, you encode\n\n    \n    \n      6n - 1 * [2 * (64^n - 62^n)]/64^n\n      = 6n - 2 * (1 - (62/64)^n)\n    \n\nbits for every group of n characters, or\n\n    \n    \n      6 - 2/n * (1 - (62/64)^n)\n    \n\nbits per character. (Note that this analysis only applies while 62^n < 64^n <\n2 * 62^n, so taking n -> \u221e gives misleading results.) The original Maze62\nproposal has n = 1, and this yields 5.9375 bits per character on average,\nwhich is pretty good. (Check this: 5 * 4/64 + 6 * 60/64 = 5.9375.) The next\nfew values are:\n\n    \n    \n      1 5.9375\n      2 5.9384765625\n      3 5.939432779947917\n      4 5.940369129180908\n      ...\n      10 5.945595231334425\n    \n      Theoretical limit:\n      6 * log_64(62) = 5.954196310386876\n    \n\nSo I guess there's really not much to be gained for the average case by\nincreasing n. But someone pointed out that all-0 and all-1 sequences happen\nfrequently in real situations, so it may be worth it anyway.\n\n~~~\nDenisM\nThat's a great idea, and it will probably speed up possible native\nimplementations (though likely at the expense of scripting languages).\n\nTo deal with large runs of identical numbers (e.g. all zeroes) someone\nsuggested in my blog comments to XOR the input array with a result of a chosen\npseudo-random function. This will bring any dataset into the realm of average\n(5.9375) except for the dataset that is specifically targeting the chosen\npseudo-random function. :)\n\n------\njerf\nClaiming this is dense, OK. That's just math. But how can you claim this is\n\"speedy\" when you have an unpolished _Python_ implementation in hand? You need\nto bench against best-of-breed Base64 implementations, not some pure-Python\ncomparison you may write up in the future, and for that you're going to need\nto go at least C, and I wouldn't be surprised down to the assembler. Just\nstick with \"dense\" and claim \"speedy\" when you've got some numbers.\n\n(I suspect you won't beat or even tie Base64, but I'm prepared to accept that\nyou only got within X% of base64 but that X% is worth it in some scenario. And\nmaybe if you get clever enough you can prove me wrong.)\n\n~~~\nDenisM\nIt's speedy compared to base62.\n\nSee, the problem is that Base64 is linear is speed, but has large (non-\nalphanumeric) charset, while base62 has small (alphanumeric) charset but is\nquadratic in speed (at least in its straightforward implementation, the only\none I was able to find).\n\nMaze62 is the encoding that is both constrained in charset and liner in speed.\nHence, the title.\n\nDoes it make sense?\n\n~~~\njerf\nOK, that's fair. Thanks.\n\n------\nAcorn\nIf you don't mind using - and _ then the implementation is very simple. I\nposted an example in Python and CoffeeScript to SO recently for doing this\nexact thing.\n\n[http://stackoverflow.com/questions/5940416/compress-a-\nseries...](http://stackoverflow.com/questions/5940416/compress-a-series-\nof-1s-and-0s-into-the-shortest-possible-ascii-string/5941361#5941361)\n\n    \n    \n      >>> bin_string = '000111000010101010101000101001110'\n      >>> encode(bin_string)\n      'hcQOPgd'\n    \n    \n      >>> decode(encode(bin_string))\n      '000111000010101010101000101001110'\n\n~~~\nDenisM\nFYI, it seems to be known as this:\n<http://en.wikipedia.org/wiki/Base64#URL_applications>\n\n~~~\nAcorn\nHmm, looking at python's base64.urlsafe_b64encode(s), padding is still used.\nSo it sounds like it doesn't fully implement the \"modified Base64 for URL\"\nspec.\n\nBase64 encoding also seems to result in quite long ascii strings compared to\nwhat I threw together.\n\nOr is there a way to use Base64 which would give results of a comparable\nlength?\n\n~~~\nDenisM\nIt's a lot longer because it doesn't know your have encoded your input data\nin, essentially base-1. If you to convert it to base-256 (that is, and array\nof bytes, instead or array of bits as you have now) it would produce the same\nlength. Yes, there is base64 implementation in pretty much any language,\nthough they usually use + and / for the two extra characters.\n\nAs wikipedia says, padding can be added or removed as a matter of taste: _From\na theoretical point of view the padding character is not needed, since the\nnumber of missing bytes can be calculated from the number of base64 digits._\n\n<http://en.wikipedia.org/wiki/Base64#Padding>\n\n------\nnkurz\nMy first impression is that this algorithm is going to be much slower than\nBase64 due to all the branching. This probably won't be visible in Python, but\nI don't think there's any way to make it really fly when optimized.\n\nThere are probably cases where this is acceptable, but I'm inclined to think\nthat Base32 is a better choice if larger size is acceptable, and that Base64\nwith domain appropriate characters is better where performance counts.\n\n~~~\ncolomon\nReally depends on what you want to do with it, doesn't it? He mentions\ncharacter limits in URLs as a motivation, and at reasonable lengths for that I\nimagine Maze62 in Python will be plenty fast....\n\n------\npeterbotond\nrecently, i needed to encode/decode similar data, and base64 with a variable\n64 chars seemed to work just as good. say, - or / are problem chars, then make\nanother set of 64 chars which would have a comma and a dot or alike and there\nyou go. keep the sets in a table or somehting and use the first 2 chars as\nindicator which set of 64 to use for that data.\n\n------\njtchang\nCool stuff. Can you post a few examples of the encoding?\n\n------\nandrewcooke\nwhy is base62 quadratic?\n\n~~~\ndfox\nNaive implementation certainly is, as it involves one multiplication (of\nnumber whose size depends linearly depends on number of previously processed\nbytes) for each input byte and one division for each output byte. But on the\nother hand I have this feeling, that more effective implementation is possible\n(and actually used by modern compression algorithms like RAR and LZMA).\n\n~~~\nandrewcooke\nbut couldn't you have a stream algorithm with modular arithmetic?\n\n[edit: i thought i had a good demonstration of this. actually, two. but both\nwere wrong. so it does seem to be non-trivial...]\n\n[edit 2: open stack overflow question:\n[http://stackoverflow.com/questions/3545931/efficient-\nalgorit...](http://stackoverflow.com/questions/3545931/efficient-algorithm-\nfor-conversion-between-numeral-system) ]\n\n~~~\nDenisM\nIf we could, we wouldn't call it base62, as base62 normally means converting\nthe entire \"bigint\" (all bytes of input data as one big number) into base-62\nrepresentation. At least that's how baseXX has been used to far (I searched\nthe web before doing this), and I am in favor of sticking to that naming\nconvention. Agreed?\n\nNow, whether we could come up with such streaming algorithm I'm not sure. I\ntried solving this problem, and this is what I came up with. Maybe I haven't\ntried hard enough to go the other route... I guess you could simply slice the\nincoming stream into a set of chunks, each small enough to make bigin base62\nconversion fast (which is square of the size of the chunk), and yet large\nenough to obscure the occasional loss of space at the chunk boundaries... I\nguess that would be an option...\n\n"
}