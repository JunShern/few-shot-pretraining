{
    "criteria": [
        {
            "criterion": "AllDocuments",
            "passed": true,
            "reason": "All documents pass."
        },
        {
            "criterion": "QuestionAnswerStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "FullyStructured",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStringsV2",
            "passed": true,
            "reason": "Text contains There are many."
        },
        {
            "criterion": "ListPrefixV2",
            "passed": true,
            "reason": "Text contains ['-', '-', '-', '1', '2', '3', '4', '0']."
        },
        {
            "criterion": "ExamplesMinimalEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesSynonymsEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesDiverseEmbed",
            "passed": true,
            "reason": "Text contains ['To give only one example, one can imagine how difficult was to express the $n$th power of a binomial expression in the absence of a symbolic representation, i.e., using only words of the ordinary language. (0.199)', 'In contemporary mathematics we are facing a change of perspective, a change of scenario, replacing the old itinerary definition\u2013theorem\u2013proof by another one (see, for instance, W. Thurston), based on ideas, examples and motivations. (0.199)', 'If there are several five\u2013colour maps, they have chosen one with the smallest number of countries and proved that this map must contain one of 1,936 possible configurations; they also proved that every one of these possible configurations can be reduced into a smaller configuration which also needs five colours. (0.151)', 'The paper [@rsst] concludes with the following interesting comment (p. 24): \u201cWe should mention that both our programs use only integer arithmetic, and so we need not be concerned with round\u2013off errors and similar dangers of floating point arithmetic. (0.197)', 'These have to be taken on faith, and are conceivably a source of error. (0.190)', 'However, from a practical point of view, the chance of a computer error that appears consistently in exactly the same way on all runs of our programs on all the compilers under all the operating systems that our programs run on is infinitesimally small compared to the chance of a human error during the same amount of case\u2013checking. (0.200)', 'For the moment we make the following two observations. (0.184)', 'An example is the proof of the classification of finite simple groups called by Danny Gorenstein the \u201cThirty Years War\" (for the classification battles were fought mostly in the decades 1950\u20131980), a work which comprises about 10,000\u201315,000 pages scattered in 500 journal articles by some 100 authors. (0.175)', ':   Without diminishing in any way the \u201cunderstanding\" component of mathematics we note that the idea of distinguishing between \u201cgood\" and \u201cbad\" proofs on the light they shed on their own truth seems to be, at least to some extent, relative and subjective. (0.197)', 'Taking rigour as something that can be acquired only at the expense of meaning and conversely, taking meaning as something that can be obtained only at the expense of rigour, we oblige mathematical proof to have the status of what was called in physics a \u201cconjugate (complimentary) pair\", i.e., a couple of requirements, each of them being satisfied only at the expense of the other (see [@marcus]). (0.192)', 'There are some genuine obstacles in our attempts to eliminate or at least to diminish the action of various sources of imprecision. (0.180)', 'Since many proofs, if not most of them, are components of a modeling process, we have to add the unavoidable error of approximation involved in any cognitive model. (0.175)', 'In the light of the situations pointed out above, we can understand some ironical comments about what a mathematician could be. (0.190)', 'When various authors (including the famous probabilist J. L. Doob, see [@schmidt]) found some mistakes in the proof of the 4CT, the authors of the proof succeeded in showing that all of them were benign and more than this, [*any other possible mistake, not yet discovered, should be benign*]{}. (0.197)', 'But already here we can observe the weakness of the criterion: how many mathematicians are to check individually and independently the status of an agnogram to give it the status of theorem? (0.191)', 'The proof is extremely intricate, quite long (over 100 printed pages[^7]), and only a handful of people in the entire world can claim to understand it. (0.191)', 'To give only one example in this respect, we recall the famous result obtained by Chomsky [@chomsky], in the late 1950s, stating that context\u2013free grammars are not able to generate the English language. (0.198)', 'There are many reasons which support this prediction. (0.195)']."
        }
    ],
    "doc_id": "4797",
    "text": "---\nauthor:\n- |\n    [**Cristian S. Calude**]{}$^{1}$, \u00a0[**Elena Calude**]{}$^{2}$, [**Solomon Marcus**]{}$^{3}$\\\n    $^{1}$University of Auckland, New Zealand\\\n    [cristian@cs.auckland.ac.nz]{}\\\n    $^{2}$Massey University at Albany, New Zealand\\\n    [e.calude@massey.ac.nz]{}\\\n    $^{3}$Romanian Academy, Mathematics, Bucharest, Romania\\\n    [Solomon.Marcus@imar.ro]{}\ntitle: '**Passages of Proof**'\n---\n\nTo Prove or Not to Prove\u2013That Is the Question!\n==============================================\n\n\\\n\\\n\\\n\\\n\nIn this paper we propose a new perspective on the evolution and history of the idea of mathematical proof. Proofs will be studied at three levels: syntactical, semantical and pragmatical. Computer-assisted proofs will be give a special attention. Finally, in a highly speculative part, we will anticipate the evolution of proofs under the assumption that the quantum computer will materialize. We will argue that there is little \u2018intrinsic\u2019 difference between traditional and \u2018unconventional\u2019 types of proofs.\n\nMathematical Proofs: An Evolution in Eight Stages\n=================================================\n\n[*Reason*]{} and [*experiment*]{} are two ways to acquire knowledge. For a long time mathematical proofs required only reason; this might be no longer true. We can distinguish eight periods in the evolution of the idea of mathematical proof. The first period was that of pre\u2013Greek mathematics, for instance the Babylonian one, dominated by observation, intuition and experience.\n\nThe second period was started by Greeks such as Pythagoras and is characterized by the discovery of deductive mathematics, based on theorems. Pythagoras proved his theorem, but the respective statement was discovered much earlier. Deductive mathematics saw a culminating moment in Euclid\u2019s geometry. The importance of abstract reasoning to ancient Greeks can be illustrated by citing Aristophanes\u2019s comedy [*The Birds*]{} which includes a cameo appearance of Meton, the astronomer, who claims that he had squared the circle. Knuth [@knuth] rhetorically asked: \u201cWhere else on earth would a playwright think of including such a scene?\" Examples would have been difficult to produce in 1985, but today the situation has changed. Take for example, the movie [*Pi*]{} written and directed by Darren Aronofsky Starring Sean Gullette or Auburn\u2019s play [*Proof*]{} [@auburn] originally produced by the Manhattan Theatre Club on 23rd May 2000.\n\nIn a more careful description, we observe that deductive mathematics starts with Thales and Pythagoras, while the axiomatic approach begins with Eudoxus and especially with Aristotle, who shows that a demonstrative science should be based on some non\u2013provable principles, some common to all sciences, others specific to some of them. Aristotle also used the expression \u201ccommon notions\" for axioms (one of them being the famous principle of non\u2013contradiction). Deductive thinking and axiomatic thinking are combined in Euclid\u2019s [*Elements*]{} (who uses, like Aristotle, \u201ccommon notions\" for \u201caxioms\"). The great novelty brought by Euclid is the fact that, for the first time, mathematical proofs (and, through them, science in general) are built on a long distance perspective, in a step by step procedure, where you have to look permanently to previous steps and to fix your aim far away to the hypothetical subsequent steps. Euclid became, for about two thousands years, a term of reference for the axiomatic\u2013deductive thinking, being considered the highest standard of rigour. Archimedes, in his treatise on static equilibrium, the physicists of the Middle Age (such as Jordanus de Nemore, in [*Liber de ratione ponderis*]{}, in the 13th century), B. Spinoza in [*Ethics*]{} (1677) and I. Newton in [*Principia*]{} (1687) follow Euclid\u2019s pattern. This tradition is continued in many more recent works, not only in the field of mathematics, but also in physics, computer science, biology, linguistics, etc.\n\nHowever, some shortcomings of Euclid\u2019s approach were obstacles for the development of mathematical rigour. One of them was the fact that, until Galilei, the mathematical language was essentially the ordinary language, dominated by imprecision resulting from its predominantly spontaneous use, where emotional factors and lack of care have an impact. In order to diminish this imprecision and make the mathematical language capable to face the increasing need of precision and rigour, the ordinary language had to be supplemented by an artificial component of symbols, formulas and equations: with Galilei, Descartes, Newton and Leibniz, the mathematical language became more and more a mixed language, characterized by a balance between its natural and artificial components. In this way, it was possible to pack in a convenient, heuristic way, previous concepts and results, and to refer to them in the subsequent development of mathematical inferences. To give only one example, one can imagine how difficult was to express the $n$th power of a binomial expression in the absence of a symbolic representation, i.e., using only words of the ordinary language. This was the third step in the development of mathematical proofs.\n\nThe fourth step is associated with the so\u2013called epsilon rigour, so important in mathematical analysis; it occurred in the 19th century and it is associated with names such as A. Cauchy and K. Weierstrass. So, it became possible to renounce the predominantly intuitive approach via the infinitely small quantities of various orders, under the form of functions converging in the limit to zero (not to be confused with the Leibnizian infinitely small, elucidated in the second half of the 20th century, by A. Robinson\u2019s non\u2013standard analysis). The epsilon rigour brought by the fourth step created the possibility to cope in a more accurate manner with processes with infinitely many steps such as limit, continuity, differentiability and integrability.\n\nThe fifth period begun with the end of the 19th century, when Aristotle\u2019s logic, underlining mathematical proofs for two thousands years, entered a crisis with the challenge of the principle of non\u2013contradiction. This crisis was already announced by the discovery of non\u2013Euclidean geometries, in the middle of the 19th century. Various therapies were proposed to free the mathematical proof of the dangerous effects of paradoxes (Russell\u2013Whitehead, Hilbert, Brouwer, etc). This period covers the first three decades of the 20th century and is dominated by the optimistic view stating the possibility to arrange the whole mathematics as a formal system and to decide for any possible statement whether it is true or false. However, even during this period mathematicians were divided with respect to the acceptance of non\u2013effective (non\u2013constructive) entities and proofs (for example, Brouwer\u2019s intuitionism rejects the principle of excluded middle in the case of infinite sets). Intuitionism was a signal for the further development of constructive mathematics, culminating with the algorithmic approach leading to computer science.\n\nThe sixth period begins with G\u00f6del\u2019s incompleteness theorem (1931), for many meaning the unavoidable failure of any attempt to formalise the whole of mathematics. Aristotle\u2019s requirement of complete lack of contradiction can be satisfied only by paying the price of incompleteness of the working formal system. Chaitin (1975) has continued this trend of results by proving that from $N$ bits of axioms one cannot prove that a program is the smallest possible if it is more than $N$ bits long; he suggested that complexity is a source of incompleteness because a formal system can capture only a tiny amount of the huge information contained in the world of mathematical truth. This principle has been proved in Calude and J\" urgensen [@cj]. Hence, incompleteness is natural and inevitable rather then mysterious and esoteric. This raises the natural question (see Chaitin [@ch02]): [*How come that in spite of incompleteness, mathematicians are making so much progress?*]{}\n\nThe seventh period belongs to the second half of the 20th century, when algorithmic proofs become acceptable only when their complexities were not too high. Constructiveness is no longer enough, a reasonable high complexity (cost) is mandatory. We are now living in this period. An important event of this period was the 1976 proof of the Four\u2013Colour Problem (4CP): it marked the reconciliation of empirical\u2013experimental mathematics with deductive mathematics, realized by the use of computer programs as pieces of a mathematical proof. Computer refers to classical von Neumann computer. At the horizon we can see the (now hypothetical) quantum computer which may modify radically the relation between empirical\u2013experimental mathematics and deductive mathematics \u2026\n\nWith the eighth stage, proofs are no longer exclusively based on logic and deduction, but also empirical and experimental. On the other hand, in the light of the important changes brought by authors like Hilbert, already at the beginning of the 20th century, primitive terms became to have an explicit status, axioms show their dependence on physical factors and the axiomatic\u2013deductive method displays its ludic dimension, being a play with abstract symbols. Hilbert axiomatization of geometry is essentially different from Euclid\u2019s geometry and this fact is well pointed out by Dijkstra in [@dijkstra] where he considers that, by directing their attention towards provability, formalists circumvented the vague metaphysical notion of \u201ctruth\". Dijkstra qualifies as \u201cphilosophical pollution\" the mentality which pushed Gauss not to publish his ideas related to non\u2013Euclidean geometry. Contrary to appearances, believes Dijkstra, Euclidean geometry is not a prototype of a deductive system, because it is based to a large extent on pictures (so\u2013called definitions of points and lines, for instance) motivated by the need of geometric intuition. For Dijkstra, the claim that the Euclidean geometry is a model of deductive thinking, is a big lie. As a matter of fact, the shortcomings to which Dijkstra refers were well-known, as can be seen in Morris Kline\u2019s book [@kline], pp. 86\u201388. In contemporary mathematics we are facing a change of perspective, a change of scenario, replacing the old itinerary definition\u2013theorem\u2013proof by another one (see, for instance, W. Thurston), based on ideas, examples and motivations. The interesting fact is that the gap created between proof and intuition by Hilbert prepared the way for a new marriage between deduction and experiment, made possible by the computational revolution, as it was shown by the latest step in the evolution of proofs.\n\nProofs, Theorems and Truths\n===========================\n\n\\\n\nWhat is a mathematical proof? At a first glance the answer seems obvious: a proof is a series of logical steps based on some axioms and deduction rules which reaches a desired conclusion. Every step in a proof can be checked for correctness by examining it to ensure that it is logically sound. In David Hilbert\u2019s words: \u201cThe rules should be so clear, that if somebody gives you what they claim is a proof, there is a mechanical procedure that will check whether the proof is correct or not, whether it obeys the rules or not.\" By making sure that every step is correct, one can tell once and for all whether a theorem has been proved. Simple! A moment of reflection shows that the problem may not be so simple. For example, what if the \u201cagent\" (human or computer) checking a proof for correctness makes a mistake (agents are fallible)? Obviously, another agent has to check that the agent doing the checking did not make any mistakes. Some other agent will need to check that agent, and so on. Eventually one runs out of agents who could check the proof and, in principle, they could all have made a mistake!\n\nThe mistake is the neighbour and the brother of proof, it is both an opponent and a stimulus. An interesting analysis, responding to Joseph L. Doob\u2019s challenge, of various possible mistakes in the proof of the 4CT can be found in the work of Schmidt [@schmidt]. In 1976, Kenneth Appel and Wolfgang Haken proved the 4CT. They used some of Alfred Kempe\u2019s ideas, but avoided his mistake.[^1] They showed that if there is a map which needs five colours, then a contradiction follows. If there are several five\u2013colour maps, they have chosen one with the smallest number of countries and proved that this map must contain one of 1,936 possible configurations; they also proved that every one of these possible configurations can be reduced into a smaller configuration which also needs five colours. This is a contradiction because we assumed that we already started with the smallest five\u2013colour map. The reduction step, i.e., the step in which one shows that the 1,936 configurations could be reduced was actually done by brute force computer search through every configuration. No human being could ever actually read the entire proof to check its correctness. For Ron Graham, \u201cThe real question is this: If no human being can ever hope to check a proof, is it really a proof?\"\n\nIn 1996 Robertson, Sanders, Seymour and Thomas [@rsst] offered a simpler proof involving only 633 configurations. The paper [@rsst] concludes with the following interesting comment (p. 24): \u201cWe should mention that both our programs use only integer arithmetic, and so we need not be concerned with round\u2013off errors and similar dangers of floating point arithmetic. However, an argument can be made that our \u201cproof\u201c is not a proof in the traditional sense, because it contains steps that can never be verified by humans. In particular, we have not proved the correctness of the compiler we compiled our programs on, nor have we proved the infallibility of the hardware we ran our programs on. These have to be taken on faith, and are conceivably a source of error. However, from a practical point of view, the chance of a computer error that appears consistently in exactly the same way on all runs of our programs on all the compilers under all the operating systems that our programs run on is infinitesimally small compared to the chance of a human error during the same amount of case\u2013checking. Apart from this hypothetical possibility of a computer consistently giving an incorrect answer, the rest of our proof can be verified in the same way as traditional mathematical proofs. We concede, however, that [*verifying a computer program is much more difficult than checking a mathematical proof of the same length*]{}.\u201d[^2]\n\nAccording to Vladimir Arnold, \u201cProofs are to mathematics what spelling (or even calligraphy) is to poetry. Mathematical works do consist of proofs, just as poems do consist of characters.\" These analogies point out both the necessity and the insufficiency of proofs in the development of mathematics. Indeed, spelling is the way poetry takes expression, but it is equally the tool used by the common everyday language, in most cases devoid of any poetic effect. What should be added to spelling in order to get a piece of poetry remains a mystery. A poem consists of characters, but it is much more than a meaningful concatenation of characters.\n\nMathematics cannot be conceived in the absence of proofs. According to Foia\u015f [@foias], \u201cthe theorem is the brick of mathematics\". Obviously, \u201cproof\" and \u201ctheorem\" go together; the object of a proof is to reach a theorem, while theorems are validated by proofs. Theorems are, for the construction of mathematics, what bricks are for the construction of a building. A building is an articulation of bricks and, analogically, a mathematical work is an articulation of theorems. Motivated by a similar view, Jean Dieudonn\u00e9 [@dieu] defines a mathematician as a person who has proved at least one theorem. In contrast, Arnold\u2019s analogies point out the fact that mathematics is much more than a chain of theorems and proofs, so implicitly a mathematician should be much more than the author of a theorem. Probably the best example is offered by Bernhard Riemann whose lasting fame does not come (in the first instance) from his theorems or proofs, but from his conjectures, definitions, concepts and examples (see for example, the discussion in Hersh [@hersh], pp. 50\u201351). Srinivasa Ramanujan is another famous example of a mathematician who produced more results than proofs. What the mathematical community seems to value most are \u201cideas\". \u201cThe most respected mathematicians are those with strong \u2018intuition\u2019 \" (Harris [@harris], p. 19).\n\nMathematical Proofs: The Syntactic Dimension\n============================================\n\nOf course, the first thing to be discussed is G\u00f6del\u2019s incompleteness theorem (GIT) which says that [*every formal system which is (1) finitely specified, (2) rich enough to include the arithmetic, and (3) consistent, is incomplete.*]{} That is, there exists an arithmetical statement which (A) can be expressed in the formal system, (B) is true, but (C) is unprovable within the formal system. All conditions are necessary. Condition (1) says that there is an algorithm listing all axioms and inference rules (which could be infinite). Taking as axioms all true arithmetical statements will not do, as this set is not finitely listable. But what does it mean to be a \u201ctrue arithmetical statement\"? It is a statement about non-negative integers which cannot be invalidated by finding any combination of non-negative integers that contradicts it. In Alain Connes terminology (see [@cls], p. 6), a true arithmetical statement is a \u201cprimordial mathematical reality\". Condition (2) says that the formal system has all the symbols and axioms used in arithmetic, the symbols for $0$ (zero), $S$ (successor), $+$ (plus), $\\times$ (times), $=$ (equality) and the axioms making them work (as for example, $x +S(y) = S(x+y)$). Condition (2) cannot be satisfied if you do not have individual terms for $0, 1, 2, \\dots $; for example, Tarski proved that Euclidean geometry, which refers to points, circles and lines, is complete. Finally (3) means that the formal system is free of contradictions. The essence of GIT is to distinguish between truth and provability. A closer real life analogy is the distinction between truths and judicial decisions, between what is true and what can be proved in court.[^3] How large is the set of true and unprovable statements? If we fix a formal system satisfying all three conditions in GIT, then the set of true and unprovable statements is topologically \u201clarge\" (constructively, a set of second Baire category, and in some cases even \u201clarger\"), cf. Calude, J\" urgensen, Zimand [@cjz]; because theorems proven in such a system have bounded complexity, the probability that an $n$-bit statement is provable tends to zero when $n$ tends to infinity (see Calude and J\" urgensen [@cj]).\n\nThere is a variety of reactions in interpreting GIT, ranging from pessimism to optimism or simple dismissal (as irrelevant for the practice of mathematics). For pessimists, this result can be interpreted as the final, definite failure of any attempt to formalise the whole of mathematics. For example, Hermann Weyl acknowledged that GIT has exercised a \u201cconstant drain on the enthusiasm\" with which he has engaged himself in mathematics and for Stanley Jaki, GIT is a fundamental barrier in understanding the Universe. In contrast, scientists like Freeman Dyson acknowledge the limit placed by GIT on our ability to discover the truth in mathematics, but interpret this in an optimistic way, as a guarantee that mathematics will go on forever (see Barrow [@barrow], pp. 218\u2013221).\n\nIn modern times a penetrating insight into the incompleteness phenomenon has been obtained by an information\u2013theoretic analysis pioneered by Chaitin in [@ch75]. Striking results have been obtained by studying the Chaitin\u2019s Omega Number, $\\Omega$, the halting probability of a self-delimiting universal Turing machine. This number is not only uncomputable, but also (algorithmically) random. Chaitin has proven the following important theorem: [*If $ZFC$ (Zermelo set theory with the Axiom of Choice) is arithmetically sound, that is, any theorem of arithmetic proved by $ZFC$ is *true*, then, $ZFC$ can determine the value of only finitely many bits of $\\Omega$, and one can give a bound on the number of bits of $\\Omega$ which $ZFC$ can determine.*]{} Robert Solovay [@solovay2k] (see more in [@cc; @crisomega; @cris; @cris2002]) has constructed [*a self-delimiting universal Turing machine such that $ZFC$, if arithmetically sound, cannot determine any single bit of its halting probability*]{} ($\\Omega$). Re\u2013phrased, the most powerful formal axiomatic system is powerless when dealing with the questions of the form \u201cis the $m$th bit of $\\Omega$ 0?\" or \u201cis the $m$th bit of $\\Omega$ 1?\".\n\nChaitin has constructed an exponential Diophantine equation $F(t; x_1,   \\ldots   ,x_n)=0$ with the following property: the infinite binary sequence whose $m$th term is 0 or 1 depending whether the equation $F(m; x_1,   \\ldots   ,x_n)=0$ has finitely or infinitely many solutions is exactly the digits of $\\Omega$, hence it is random; its infinite amount of information is algorithmically incompressible. The importance of exponential Diophantine equations comes from the fact that most problems in mathematics can be formulated in terms of these type of equations; Riemann\u2019s Conjecture is one such example. Manin [@manin1], p. 158, noticed that \u201cThe epistemologically important point is the discovery that randomness can be defined without any recourse to physical reality \u2026 in such a way that the necessity to make an infinite search to solve a parametric series of problems leads to the technically random answers. Some people find it difficult to imagine that a rigidly determined discipline like elementary arithmetic may produce such phenomena\".\n\nLast but not least, is the truth achieved through a formal proof the ultimate expression of knowledge? Many (mathematicians) will give a positive answer, but perhaps not all. For the 13th century Oxford philosopher Roger Bacon, \u201cArgument reaches a conclusion and compels us to admit it, but it neither makes us certain nor so it annihilates doubt that the mind rests calm in the intuition of truth, unless it finds this certitude by way of experience.\" More recently, I. J. Schoenberg[^4] is cited by Epstein ([@hahn]) as saying that Edmund Landau kept in his desk drawer for years a manuscript proving what is now called the two constants theorem: he had the complete proof but could not believe it until his intuition was ready to accept it. Then he published it. A \u201cproof is only one step in the direction of confidence\" argued De Millo, Lipton and Perlis in a classical paper on proofs, theorems and programs [@demillo]. Written in the same spirit is Don Knuth\u2019s warning: \u201cBeware of bugs in the above code: I have only proved it correct, not tried it.\"\n\nMathematical Proofs: The Semantic Dimension\n===========================================\n\n\\\n\nThe above quotation turned slogan as \u201cmore rigour, less meaning\", or better still, \u201cless rigour, more meaning\" (Chaitin [@gregpccris]) points out the necessity to distinguish between the syntactic and the semantic aspects of proofs. Should proofs belong exclusively to logic, according to the tradition started by Greeks such as Pythagoras and Euclid? Or should they also be accepted as a cocktail of logical and empirical\u2013experimental arguments, as in the proof of the 4CT (1976)? Mathematicians are now divided into those giving an affirmative answer to the first question and implicitly a negative answer to the second question and those giving a negative answer to the first question and an affirmative one to the second question. Computationally oriented mathematicians usually belong to the second category, while many other mathematicians (as, for instance, the Fields medalist William Thurston) belong to the first, so for them, the 4CT is not yet proved! Meaning is a key distinction. For mathematicians such as Ren\u00e9 Thom, Daniel Cohen and William Thurston, correctness by itself does not validate a proof; it is also necessary to \u201cunderstand\" it. \u201cThe mission of mathematics is understanding\" says Cohen. Paul Halmos has also insisted on the \u201cconceptual understanding\". For him a \u201cgood\" proof of a theorem is one that sheds light on why it is true. It is just the process of understanding which is in question with proofs like that given to the 4CT. Referring to the proof of the 4CT, Halmos says: \u201cI do not find it easy to say what we learned from all that. \u2026 The present proof relies in effect on an Oracle, and I say down with Oracles! They are not mathematics!\" In contrast with Halmos, who hopes that \u201c100 years from now the map theorem will be \u2026 an exercise in a first\u2013year graduate course, provable in a couple of pages by means of appropriate concepts, which will be completely familiar by then\" (see [@hersh], p. 54), R.\u00a0Hersh thought that the problem itself might be responsible for the way it was solved: he is cited by saying dejectedly \u201cSo it just goes to show, it wasn\u2019t a good problem after all\" (see [@casti] p. 73).\n\nWe will return later to these issues. For the moment we make the following two observations.\n\n A)\n\n:   Not only the hybrid proofs obtained as a combination of logical and empirical\u2013experimental arguments might be hard/impossible to be understood in their \u201cglobality\"; this happens also for some pure deductive proofs. An example is the proof of the classification of finite simple groups called by Danny Gorenstein the \u201cThirty Years War\" (for the classification battles were fought mostly in the decades 1950\u20131980), a work which comprises about 10,000\u201315,000 pages scattered in 500 journal articles by some 100 authors.[^5]\n\n    According to Knuth [@knuth] p. 18, \u201c\u2026 program\u2013writing is substantially more demanding than book\u2013writing\". \u201cWhy is this so? I think the main reason is that a larger attention span is needed when working on a large computer program than when doing other intellectual tasks. \u2026 Another reason is \u2026 that programming demands a significantly higher standard of accuracy. Things don\u2019t simply have to make sense to another human being, they must make sense to a computer.\" Knuth compares his TeX compiler (a document of about 500 pages) with Feit and Thompson [@ft] theorem that all simple groups of odd order are cyclic. He lucidly argues that the program might not incorporate as much creativity and \u201cdaring\" as the proof of the theorem, but they come even when compared on depth of details, length and paradigms involved. What distinguishes the program from the proof is the \u201cverification\": convincing a couple of (human) experts that the proof [*works in principle*]{} seems to be easier than making sure that the program [*really works*]{}. A demonstration that [*there exists a way to compile TeX*]{} is not enough! Another example, which will be discussed later in this section, is the proof of Fermat\u2019s Last Theorem (FLT).\n\nB)\n\n:   Without diminishing in any way the \u201cunderstanding\" component of mathematics we note that the idea of distinguishing between \u201cgood\" and \u201cbad\" proofs on the light they shed on their own truth seems to be, at least to some extent, relative and subjective.\n\nThom\u2019s slogan \u2018more rigour, less meaning\u2019 was the main point in his controversy with Jean Dieudonn\u00e9 (as a representative of the Bourbaki group). Taking rigour as something that can be acquired only at the expense of meaning and conversely, taking meaning as something that can be obtained only at the expense of rigour, we oblige mathematical proof to have the status of what was called in physics a \u201cconjugate (complimentary) pair\", i.e., a couple of requirements, each of them being satisfied only at the expense of the other (see [@marcus]). Famous prototypes of conjugate pairs are (position, momentum) discovered by W. Heisenberg in quantum mechanics and (consistency, completeness) discovered by K. G\" odel in logic. But similar warnings come from other directions. According to Einstein (see, for instance, [@rosen] p. 195), \u201cin so far as the propositions of mathematics are certain, they do not refer to reality, and in so far as they refer to reality, they are not certain\", hence (certainty, reality) is a conjugate pair. Obviously, reality is here understood as an empirical entity, hence mixed with all kinds of imprecision, ranging from obscurity and disorder to randomness, ambiguity and fuzziness [@marcus1]. Pythagoras\u2019 theorem is certain, but its most empirical tests will fail. There are some genuine obstacles in our attempts to eliminate or at least to diminish the action of various sources of imprecision. Einstein implicitly calls our attention on one of them. Proof, to the extent to which it wants to be rigorous, to give us the feeling of certainty, should be mathematical; but satisfying this condition, means failing to reach reality. In other words, the price we have to pay to obtain proofs giving us the feeling of total confidence is to renounce to be directly connected to reality. There is a genuine tension between certainty and reality, they form a conjugate pair, which is the equivalent of what in the field of humanities is an oxymoronic pair. However, there is an essential difference between G\u00f6del\u2019s conjugate pair (consistency, completeness) and Einstein\u2019s conjugate pair (certainty, reality). While consistency and completeness are binary logical predicates, certainty and reality are a matter of degree, exactly like the terms occurring in Thom\u2019s conjugate pair: rigour and meaning. In last two situations there is room for manipulation and compromise.\n\nNear to the above conjugate pairs is a third one: (rigour, reality), attributed to Socrates (see [@renyi]). A price we have to pay in order to reach rigour is the replacement of the real world by a fictional one. There is no point and no line in the real world, if we take them according to their definitions in Euclid\u2019s [*Elements*]{}. Such entities belong to a fictional/virtual universe, in the same way in which the characters of a theatrical play are purely conventional, they don\u2019t exist as real persons. The rules of deduction used in a mathematical proof belong to a game in the style they are described in the scenario of a Hilbert formal system, which is, as a matter of fact, a machine producing demonstrative texts. A convention underlines the production of theorems and again a convention is accepted in a theatrical play. In the first case, the acceptance of the convention is required from both the author of the proof and its readers; in the second case all people involved, the author, the actors and spectators, have to agree the proposed convention. Since many proofs, if not most of them, are components of a modeling process, we have to add the unavoidable error of approximation involved in any cognitive model. The model should satisfy opposite requirements, to be as near as possible to the phenomenon modelled, in order to be relevant; to be as far as possible from the respective phenomenon, in order to useful, to make possible the existence of at least one method or tool that can be applied to the model, but not to the original (see [@marcus2]). Theorems are discovered, models are invented. Their interaction leads to many problems of adequacy, relevance and correctness, i.e., of syntactic, semantic and pragmatic nature.\n\nIn the light of the situations pointed out above, we can understand some ironical comments about what a mathematician could be. It is somebody who can prove theorems, as Dieudonn\u00e9 claimed. But what kind of problems are solved in this way? \u201cAny problem you want, \u2026except those you need\", said an engineer, disenchanted by his collaboration with a mathematician. Again, what is a mathematician? \u201cIt is a guy capable to give, after a long reflection, a precise, but useless answer\", said another mathematician with a deep feeling of self irony. Remember the famous reflection by Goethe: \u201cMathematicians are like French people, they take your question, they translate it in their language and you no longer recognize it\".\n\nBut things are controversial even when they concern syntactic correctness. In this respect, we should distinguish two types of syntactic mistakes: benign and malign. Benign mistakes have only a local, not global effect: they can be always corrected. Malign mistakes, on the contrary, contaminate the whole approach and invalidate the claim formulated by the theorem. When various authors (including the famous probabilist J. L. Doob, see [@schmidt]) found some mistakes in the proof of the 4CT, the authors of the proof succeeded in showing that all of them were benign and more than this, [*any other possible mistake, not yet discovered, should be benign*]{}. How can we accept such arguments, when the process of global understanding of the respective proof is in question? The problem remains open. A convenient, but fragile, solution is to accept Thom\u2019s pragmatic proposal: a theorem is validated if it has been accepted by a general agreement[^6] of the mathematical community (see [@thom1; @thom2]).\n\nThe problems raised by the 4CT were discussed by many authors, starting with Tymoczko [@tymoczko] and Swart [@swart] (more recent publications are D. MacKenzie [@mc], J. Casti [@casti], A.S. Calude [@andreea]). Swart proposed the introduction of a new entity called [*agnogram*]{}, which is \u201ca theorem\u2013like statement that we have verified as best we could, but whose truth is not known with the kind of assurance we attach to theorems and about which we must thus remain, to some extent, agnostic.\" There is however the risk to give the status of agnogram to any property depending on a natural number $n$ and verified only for a large, but finite number of values of $n$. This fact would be in conflict with Swart\u2019s desire to consider an agnogram less than a theorem, but more than a conjecture. Obviously, the 4CT is for Swart an agnogram, not a theorem. What is missing from an agnogram to be a theorem? A theorem is a statement which could be checked individually by a mathematician and confirmed also individually by at least two or three more mathematicians, each of them working independently. But already here we can observe the weakness of the criterion: how many mathematicians are to check individually and independently the status of an agnogram to give it the status of theorem?\n\nThe seriousness of this objection can be appreciated by examining the case of Andrew Wiles\u2019 proof of FLT\u2014a challenge to mathematics since 1637 when Pierre de Fermat wrote it into the margin of one of his books. The proof is extremely intricate, quite long (over 100 printed pages[^7]), and only a handful of people in the entire world can claim to understand it.[^8] To the rest of us, it is utterly incomprehensible, and yet we all feel entitled to say that \u201cthe FLT has been proved\". On which grounds? We say so because [*we believe the experts*]{} and [*we cannot tell for ourselves*]{}. Let us also note that in the first instance the original 1993 proof seemed accepted, then a gap was found, and finally it took Wiles and Richard Taylor another year to fix the error.[^9]\n\nAccording to Hunt [@hunt], \u201cIn no other field of science would this be good enough. If a physicist told us that light rays are bent by gravity, as Einstein did, then we would insist on experiments to back up the theory. If some biologists told us that all living creatures contain DNA in their cells, as Watson and Crick did in 1953, we wouldn\u2019t believe them until lots of other biologists after looking into the idea agreed with them and did experiments to back it up. And if a modern biologist were to tell us that it were definitely possible to clone people, we won\u2019t really believe them until we saw solid evidence in the form of a cloned human being. Mathematics occupies a special place, where we believe anyone who claims to have proved a theorem on the say\u2014so of just a few people\u2014that is, until we hear otherwise.\"\n\nSuppose we loosely define a religion as any discipline whose foundations rest on an element of faith, irrespective of any element of reason which may be present. Quantum mechanics, for example, would qualify as a religion under this definition. Mathematics would hold the unique position of being a branch of theology possessing a \u201cproof\" of the fact that it should be so classified. \u201cWhere else do you have absolute truth? You have it in mathematics and you have it in religion, at least for some people. But in mathematics you can really argue that this is as close to absolute truth as you can get\" says Joel Spencer.\n\nMathematical Proofs: The Pragmatic Dimension\n============================================\n\nIn the second half of the 20th century, theorems together with their proofs occur with increasing frequency as components of some cognitive models, in various areas of knowledge. In such situations we are obliged to question the theorems not only with respect to their truth value, but also in respect to their adequacy and relevance within the framework of the models to which they belong. We have to evaluate the explanatory capacity of a theorem belonging to a model B, concerning the phenomenon A, to which B is referring. This is a very delicate and controversial matter, because adequacy, relevance and explanatory capacity are a matter of degree and quality, which cannot be settled by binary predicates. Moreover, there is no possibility of optimization of a cognitive model. Any model can be improved, no model is the best possible. This happens because, as we have explained before, a cognitive model B of an entity A has simultaneously the tendency to increase its similarity with A and stress its difference from A. To give only one example in this respect, we recall the famous result obtained by Chomsky [@chomsky], in the late 1950s, stating that context\u2013free grammars are not able to generate the English language. This result was accepted by the linguistic and computer science communities until the eighties, when new arguments pointed out the weakness of Chomsky\u2019s argument; but this weakness was not of a logical nature, it was a weakness in the way we consider the entity called \u201cnatural language\". As a matter of fact, the statement \u201cEnglish is a context\u2013free language\" is still controversial.\n\nMathematical proofs are \u201ctheoretical\" and \u201cpractical\". Theoretical proofs (formal, ideal, rigorous) are models for practical proofs (which are informal, imprecise, incomplete). \u201cLogicians don\u2019t tell mathematicians what to do. They make a theory out of what mathematicians actually do\", says Hersh [@hersh], p. 50. According to the same author, logicians study what mathematicians do the way fluid dynamicists study water waves. Fluid dynamicists don\u2019t tell water how to wave, so logicians don\u2019t tell mathematicians what to do. The situation is not as simple as it appears. Logical restrictions and formal models (of proof) can play an important role in the practice of mathematics. For example, the key feature of constructive mathematics is the identification \u201cexistence = computability\" (cf. Bridges [@bridges]) and a whole variety of constructive mathematics, the so\u2013called Bishop constructive mathematics, is mathematics with intuitionistic rather than classical underlying logic.\n\nQuasi\u2013Empirical Proofs: From Classical to Quantum\n=================================================\n\n\\\n\nThe use of large\u2013scale programs, such as Mathematica, Maple or MathLab is now widespread for symbolical and numerical calculations as well as for graphics and simulations. To get a feeling of the extraordinary power of such programs one can visit, for example, the Mathematica website [http://www.wolfram.com]{}. New other systems are produced; \u201cproofs as programs\", \u201cproof animation\" or \u201cproof engineering\" are just a few examples (see [@hayashi]). In some cases an experiment conveys an aesthetic appreciation of mathematics appealing to a much broader audience (cf. [@bb1; @bbg; @crismarcus]). A significant, but simple example of the role an experiment may play in a proof is given by Beyer [@beyer]. He refers to J. North who asked for a computer demonstration that the harmonic series diverges. We quote Beyer: \u201cHis example illustrates the following principle: Suppose that one has a computer algorithm alleged to provide an approximation to some mathematical quantity. Then the algorithm should be accompanied by a theorem giving a measure of the distance between the output of the algorithm and the mathematical quantity being approximated. For the harmonic series, one would soon find that the sum was infinite.\" It is interesting to mention that in 1973 Beyer made together with Mike Waterman a similar attempt to compute Euler\u2019s constant; their experiment failed, but the error was discovered later by Brent [@brent].\n\nNew types of proofs motivated by the experimental \u201cideology\u201d have appeared. For example, rather than being a static object, the [*interactive proof*]{} (see Goldwasser, Micali, Rackoff [@GMR], Blum [@Blum]) is a two\u2013party protocol in which the [*prover*]{} tries to prove a certain fact to the [*verifier*]{}. During the interactive proof the [*prover*]{} and the [*verifier*]{} exchange messages and at the end the [*verifier*]{} produces a verdict \u201caccept\" or \u201creject\". A holographic (or probabilistic checkable) proof (see Babai [@Babai]) is still a static object but it is verified probabilistically. Errors become almost instantly apparent after a small part of the proof was checked.[^10] The transformation of a classical proof (which has to be self-contained and formal) into a holographic one requires super-linear time.\n\nThe blend of logical and empirical\u2013experimental arguments (\u201cquasi\u2013empirical mathematics\" for Tymoczko [@tymoczko], Chaitin [@ch00; @ch02; @gregphil] or \u201cexperimental mathematics\" for Bailey, Borwein [@bb], Borwein, Bailey [@bb1], Borwein, Bailey, Girgensohn [@bbg]) may lead to a new way to understand (and practice) mathematics. For example, Chaitin argued that we should introduce the Riemann hypothesis as an axiom: \u201cI believe that elementary number theory and the rest of mathematics should be pursued more in the spirit of experimental science, and that you should be willing to adopt new principles. I believe that Euclid\u2019s statement that an axiom is a self\u2013evident truth is a big mistake. The Schr\u00f6dinger equation certainly isn\u2019t a self\u2013evident truth! And the Riemann hypothesis isn\u2019t self\u2013evident either, but it\u2019s very useful. A physicist would say that there is ample experimental evidence for the Riemann hypothesis and would go ahead and take it as a working assumption.\" Classically, there are two equivalent ways to look at the mathematical notion of proof: [*logical*]{}, as a finite sequence of sentences strictly obeying some axioms and inference rules, and [*computational*]{}, as a specific type of computation. Indeed, from a proof given as a sequence of sentences one can easily construct a Turing machine producing that sequence as the result of some finite computation and, conversely, given a machine computing a proof we can just print all sentences produced during the computation and arrange them into a sequence.\n\nThis gives mathematics an immense advantage over any science: a proof is an explicit sequence of reasoning steps that can be inspected at [*leisure*]{}. [*In theory*]{}, if followed with care, such a sequence either reveals a gap or mistake, or can convince a sceptic of its conclusion, in which case the theorem [*is considered proven*]{}. The equivalence between the logical and computational proofs has stimulated the construction of programs which play the role of [*\u201cartificial\" mathematicians*]{}. The \u201ctheorem provers\" have been very successful as \u201chelpers\" in proving many results, from simple theorems of Euclidean geometry to the computation of a few digits of a Chaitin Omega Number [@crisds]. \u201cArtificial\" mathematicians are far less ingenious and subtle than human mathematicians, but they surpass their human counterparts by being infinitely more patient and diligent.\n\nIf a conventional proof is replaced by an \u201cunconventional\" one (that is a proof consisting of a sequence of reasoning steps obeying axioms and inference rules which depend not only on some logic, but also on the external physical medium), then the conversion from a computation to a sequence of sentences may be impossible, e.g. due to the size of the computation. An extreme, and for the time being hypothetical example, is the proof obtained as a result of a quantum computation (see Calude and P\u0103un [@cp]). The quantum automaton would say \u201cyour conjecture is true\", but (due to quantum interference) there will be no way to exhibit all trajectories followed by the quantum automaton in reaching that conclusion. The quantum automaton has the ability to check a proof, but it may fail to reveal any \u201ctrace\" of the proof for the human being operating the quantum automaton. Even worse, any attempt to [*watch*]{} the inner working of the quantum automaton (e.g. by \u201clooking\" inside at any information concerning the state of the ongoing proof) may compromise forever the proof itself! We seem to go back to Bertrand Russell who said that \u201cmathematics may be defined as the subject in which we never know what we are talking about, nor whether what we are saying is true\", and even beyond by adding [*and even when it\u2019s true we might not know why.*]{}\n\nSpeculations about quantum proofs [*may not affect*]{} the essence of mathematical objects and constructions (which, many believe, have an autonomous reality quite independent of the physical reality), but they seem to [*have an impact*]{} on how we [*learn/understand mathematics,*]{} which is through the physical world. Indeed, our glimpses of mathematics are revealed only through physical objects, human brains, silicon computers, quantum automata, etc., hence, according to Deutsch [@deutsch-97], they have to obey not only the axioms and the inference rules of the theory, but the [*laws of physics*]{} as well. To complete the picture we need to take into account also the [*biological*]{} dimension. No matter how precise the rules (logical and physical) are, we need human consciousness to apply the rules and to understand them and their consequences. Mathematics is a human activity.\n\nKnowledge Versus Proof\n======================\n\n\\\n\nAre there intrinsic differences between traditional and \u2018unconventional\u2019 types of proofs? To answer this question we will consider the following interrelated questions:\\\n\n1.  Do \u2018unconventional\u2019 methods supply us with a proof in some formal language?\\\n\n2.  Do \u2018unconventional\u2019 methods supply us with a mathematical proof?\\\n\n3.  Do \u2018unconventional\u2019 methods supply us with knowledge?\\\n\n4.  Does mathematics require knowledge or proof?\\\n\nA blend of mathematical reasoning supported by some silicon or quantum computation or a classical proof of excessive length and complexity (for example, the classification of finite simple groups) are examples of \u201cunconventional\u201d proofs. The ultimate goal of the mathematical activity is the [*advance human understanding of mathematics*]{} (whatever this means!).\n\nThe answer to the first two question is affirmative. Indeed, computations are represented in the programming language used by the computer (the \u2018unconventional\u2019 computer too), even if the whole proof cannot be globally \u2018visualized\u2019 by a human being. The proof can be checked by any other mathematician having the equipment used in the \u2019unconventional\u2019 proof. A proof provides knowledge only to the extent that its syntactic dimension is balanced by the semantic one; any gap between them makes the proof devoid of knowledge and paves the way for the proof to become a ritual without meaning. Proofs generating knowledge, quite often produce much more, for example, \u2019insight\u2019 (think of the insight provided by understanding the algorithm used in the proof).\n\nA misleading analogy would be to replace, in the above questions, [*\u2018unconventional\u2019 methods*]{} with [*\u201ctestimony from a respected and (relevantly) competent mathematician\u201d*]{}. Certainly, such testimony provides knowledge; it does not qualify as a mathematical proof (even less as a formalized proof), but the result is a \u201cmathematical activity\u201d because it advances our knowledge of mathematics. The difference between \u2018unconventional\u2019 methods and \u2018relevant testimony\u2019 can be found in the mechanisms used to produce outputs: a \u2018relevant testimony\u2019 is the gut feeling of a respected, relevant, competent mathematician, by and large based on a considerable mathematical experience, while an \u2018unconventional\u2019 method produces an objective argument.\n\nThere is little \u2018intrinsic\u2019 difference between traditional and \u2018unconventional\u2019 types of proofs as i) first and foremost, [*mathematical truth*]{} cannot always be certified by proof, ii) correctness is not absolute, but almost certain, as mathematics advances by making mistakes and correcting and re\u2013correcting them (mathematics fallibility was argued by Lakatos), iii) non\u2013deterministic and probabilistic proofs do not allow mistakes in the applications of rules, they are just indirect forms of checking (see Pollack [@pollack], p. 210) which correspond to various degrees of rigour, iv) the explanatory component, the understanding \u2018emerging\u2019 from proofs, while extremely important from a cognitive point of view, is subjective and has no bearing on formal correctness. As Hersh noticed, mathematics like music exists by some logical, physical and biological manifestation, but \u201cit makes sense only as a mental and a cultural activity\" ([@hersh], p. 22).\n\nHow do we continue to produce rigorous mathematics when more research will be performed in large computational environments where we might or might not be able to determine what the system has done or why[^11] is an open question. The blend of logical and empirical\u2013experimental arguments are here to stay and develop. Of course, some will continue to reject this trend, but, we believe, they will have as much effect as King Canute\u2019s royal order to the tide. There are many reasons which support this prediction. They range from economical ones (powerful computers will be more and more accessible to more and more people), social ones (skeptical oldsters are replaced naturally by youngsters born with the new technology, results and success inspire emulation) to pure mathematical (new challenging problems, wider perspective) and philosophical ones (note that incompleteness is based on the analysis of the computer\u2019s behaviour). The picture holds marvelous promises and challenges; it does not eliminate the continued importance of extended personal interactions in training and research.\n\nAcknowledgements {#acknowledgements .unnumbered}\n================\n\nThis paper is based on a talk presented at the Workshop [*Truths and Proofs*]{}, a satellite meeting of the [*Annual Conference of the Australasian Association of Philosophy (New Zealand Division)*]{}, Auckland, New Zealand, December 2001. We are most grateful to Andreea Calude, Greg Chaitin, Sergiu Rudeanu, Karl Svozil, Garry Tee, and Moshe Vardi for inspired comments and suggestions.\n\n[999]{} M. Aschbacher. The status of the classification of finite simple groups, [*Notices of the Amer. Math. Soc.*]{}51, 7 (2004), 736\u2013740.\n\nD. Auburn. [*Proof. A Play*]{}, Faber and Faber, New York, 2001.\n\nK. Appel, W. Haken. [*Every Planar Graph is Four Colorable*]{}, Contemporary Mathematics 98, AMS, Providence, 1989.\n\nL. Babai. Probably true theorems, cry wolf? [*Notices of the Amer. Math. Soc.*]{} 41 (5) (1994), 453\u2013454.\n\nD. H. Bailey, J. M. Borwein. Experimental mathematics: Recent developments and future outlook, in B. Engquist, W. Schmid (eds.). [*World Mathematical Year Mathematics Unlimited\u20142001 and Beyond*]{}, Springer-Verlag, Berlin, 2001, 51\u201366.\n\nJ. Barrow. [*Impossibility. The Limits of Science and the Science of Limits*]{}, Oxford University Press, Oxford, 1998.\n\nJ. M. Borwein, D. H. Bailey, [*The Experimental Mathematician. Plausible Reasoning in the 21st Century,*]{} A. K. Peters, Natick, Ma., 2003.\n\nJ. M. Borwein, D. H. Bailey, R. Girgensohn. [*Experimentation in Mathematics. Computational Paths to Discovery*]{}, A. K. Peters, Natick, Ma., 2004.\n\nW. A. Beyer. The computer and mathematics. [*Notices of the Amer. Math. Soc.*]{} 48, 11 (2001), 1302.\n\nM. Blum. How to prove a theorem so no one else can claim it, [*Proceedings of the International Congress of Mathematicians,*]{} Berkeley, California, USA, 1986, 1444\u20131451.\n\nR. P. Brent. Computation of the regular continued fraction for Euler\u2019s constant, [*Math. of Computation*]{} 31, 139 (1977), 771\u2013777.\n\nD. S. Bridges. Constructive truth in practice, in H.\u00a0G.\u00a0Dales and G.\u00a0Oliveri (eds.). [*Truth in Mathematics*]{}, Clarendon Press, Oxford, 1998, 53\u201369.\n\nA. S. Calude. The journey of the four colour theorem through time, [*The NZ Mathematics Magazine*]{} 38, 3 (2001), 27\u201335. C. S. Calude. [*Information and Randomness: An Algorithmic Perspective*]{}, 2nd Edition, Revised and Extended, Springer Verlag, Berlin, 2002.\n\nC.\u00a0S.\u00a0Calude. Chaitin $\\Omega$ numbers, Solovay machines and incompleteness, [*Theoret. Comput. Sci.*]{}, 284 (2002), 269\u2013277.\n\nC.\u00a0S. Calude. Incompleteness, complexity, randomness and beyond, [*Minds and Machines: Journal for Artificial Intelligence, Philosophy and Cognitive Science*]{}, 12, 4 (2002), 503\u2013517.\n\nC. S. Calude, G. J. Chaitin. Randomness everywhere, [*Nature*]{}, 400, 22 July (1999), 319\u2013320.\n\nC.\u00a0S. Calude, M.\u00a0J. Dinneen and C.-K. Shu. Computing a glimpse of randomness, [*Experimental Mathematics*]{} 11, 2 (2002), 369\u2013378.\n\nC. S. Calude, H. J[\u00fc]{}rgensen. [*Is Complexity a Source of Incompleteness?*]{}, [*CDMTCS Research Report*]{} 241, 2004, 15 pp.\n\nC.\u00a0Calude, H.\u00a0J\u00fcrgensen, M.\u00a0Zimand. Is independence an exception?, [*Appl. Math. Comput.*]{} 66 (1994), 63\u201376.\n\nC.\u00a0S. Calude, S. Marcus. Mathematical proofs at a crossroad? in J. Karhum\" aki, H. Maurer, G. P\u0103un, G. Rozenberg (eds.). [*Theory Is Forever*]{}, Lectures Notes in Comput. Sci. 3113, Springer Verlag, Berlin, 2004, 15\u201328.\n\nC. S. Calude, G. P\u0103un. [*Computing with Cells and Atoms*]{}, Taylor & Francis Publishers, London, 2001.\n\nJ. L. Casti. [*Mathematical Mountaintops*]{}, Oxford University Press, Oxford, 2001.\n\nG. J. Chaitin. Randomness and mathematical proof, [*Scientific American*]{}, 232 (5) (1975), 47\u201352.\n\nG. J. Chaitin. [*Exploring Randomness*]{}, Springer Verlag, London, 2001.\n\nG. J. Chaitin. Computers, paradoxes and the foundations of mathematics, [*American Scientist*]{}, 90 March\u2013April (2002), 164\u2013171.\n\nG. J. Chaitin. Personal communication to C. S. Calude, 5 March, 2002.\n\nG. J. Chaitin. On the intelligibility of the universe and the notions of simplicity, complexity and irreducibility, [http://www.cs.auckland.ac.nz/CDMTCS/ chaitin/bonn.html]{}, September 2002.\n\nN. Chomsky. [*Syntactic Structures*]{}, Mouton, The Hague, 1957.\n\nA. Connes, A. Linchnerowicz, M. P. Sch\" utzenberger. [*Triangle of Thoughts*]{}, AMS, Providence, 2001.\n\nE. W. Dijkstra. Real mathematicians don\u2019t prove, [*EWD1012*]{}, University of Texas at Austin, 1988, [http://www.cs.utexas.edu/users/EWD/EWD1012.pdf]{}.\n\nD. Deutsch. [*The Fabric of Reality*]{}, Allen Lane, Penguin Press, 1997.\n\nR. A. De Millo, R. J. Lipton, A. J. Perlis. Social processes and proofs of theorems and programs, [*Comm. ACM*]{} 22, 5 (1979), 271\u2013280.\n\nJ. Dieudonn\u00e9. [*Pour L\u2019honneur de l\u2019Esprit Humain*]{}, Gallimard, Paris, 1986.\n\nW. Feit, J. G. Thomson. Solvability of groups of odd order, [*Pacific J. Math.* ]{} 13 (1963), 775\u20131029.\n\n01 J. E. Fenstad. Is mathematical still the science of paper, pencils and proofs?, Conference on [*Electronic Communication and Research in Europe*]{}, Darmstadt/Seeheim, 15\u201317 April 1998, [//academia.darmstadt.gmd.de/seeheim/thebook/finals/fenstad.html]{}.\n\nS. Goldwasser, S. Micali, C. Rackoff. The knowledge complexity of interactive proof\u2013systems, [*SIAM Journal of Computing,*]{} 18(1) (1989), 186\u2013208.\n\nC. Foia\u015f. Personal communication to S. Marcus (about 20 years ago).\n\nL. E. Hahn, B. Epstein. [*Classical Complex Analysis*]{}, Sudury, Mass., Jones and Barlettt, 1996.\n\nM. Harris. Contexts of justification, [*The Mathematical Intelligencer*]{} 23, 1 (2001), 10\u201322.\n\nS. Hayashi, R. Sumitomo, K. Shii. Towards the animation of proofs\u2013testing proofs by examples, [*Theoret. Comput. Sci.*]{} 272 (2002), 177\u2013195. R. Hunt. The philosophy of proof, [http://plus.maths.org/issue10/ features/proof4/]{}. R. Hersh. [*What Is Mathematics, Really?*]{}, Vintage, London, 1997.\n\nM. Kline. [*Mathematical Thought from Ancient to Modern Times*]{}, Oxford University Press, Oxford, Vol. 1, 1972. D. E. Knuth. Theory and practice, [*EATCS Bull.*]{} 27 (1985), 14\u201321.\n\nD. MacKenzie. Slaying the kraken. The sociohistory of a mathematical proof, [*Social Studies of Science*]{} 29, 2 (1999), 7\u201360.\n\nYu. I. Manin. Truth, rigour, and common sense, in H.\u00a0G.\u00a0Dales and G.\u00a0Oliveri (eds.). [*Truth in Mathematics*]{}, Clarendon Press, Oxford, 1998, 147\u2013159.\n\nS. Marcus. No system can be improved in all respects, in G. Altmann and W. A. Koch (eds.). [*Systems. New Paradigms for the Human Sciences*]{}, Walter de Gruyter, Berlin, 1998, 143\u2013164.\n\nS. Marcus. Imprecision, between variety and uniformity: the conjugate pairs, in J. J. Jadacki and W. Strawinski (eds.). [*The World of Signs*]{}, Poznan Studies in the Philosophy of Sciences and the Humanities 62 Rodopi, Amsterdam, 1998 59\u201372.\n\nS. Marcus. Metaphor as dictatorship, in J. Bernard, J. Wallmannsberger and G. Withalm (eds.). [*World of Signs. World of Things*]{}, Angewandte Semiotik 15, OGS, Wien, 1997, 87\u2013108.\n\nR. Pollack. How to believe a machine\u2013checked proof, in G. Sambin and J. M. Smith (eds.). [*Twenty\u2013five Years of Constructive Type Theory*]{}, Clarendon Press, Oxford, 1998, 205\u2013220.\n\nA. R' enyi. [*Dialogues on Mathematics*]{}, Holden Day, San Francisco, 1967.\n\nN. Robertson, D. Sanders, P. Seymour, R. Thomas. A new proof of the four\u2013colour theorem, [*Electronic Research Announcements of the AMS*]{} 2,1 (1996), 17\u201325.\n\nR. R. Rosen. Complementarity in social structures, [*Journal of Social and Biological Structures*]{} 1 (1978), 191\u2013200.\n\nR. Thom. Modern mathematics: does it exist?, in A.\u00a0G.\u00a0Howson (ed.). [*Developments in Mathematical Education*]{}, Cambridge University Press, 1973, 194\u2013209.\n\nR. Thom. Topologie et linguistique, in A.\u00a0Haefliger and R.\u00a0Nerasimham (eds.). [*Essays on Topology and Related Topics. Memoires D\u00e9di\u00e9s \u00e0 Georges de Rham*]{}, Springer Verlag, New York, 1970, 226\u2013248.\n\nR. M. Solovay. A version of $\\Omega$ for which $ZFC$ can not predict a single bit, in C. S. Calude and G. P\u0103un (eds.). [*Finite Versus Infinite. Contributions to an Eternal Dilemma*]{}, Springer Verlag, London, 2000, 323\u2013334.\n\nT. Tymoczko. The four\u2013colour problem and its philosophical significance, [*J. Philosophy*]{} 2,2 (1979), 57\u201383.\n\nU. Schmidt. [*\u00dcberpr\u00fcfung des Beweises f\u00fcr den Vierfarben Satz*]{}, Diplomarbeit Technische Hochschule, Aachen, 1982.\n\nE. R. Swart. The philosophical implications of the four\u2013colour problem, [*American Mathematical Monthly*]{} 87, 9 (1980), 697\u2013702.\n\n[^1]: In 1879 Kempe announced his \u2018proof\u2019 of the 4CT in both the magazine [*Nature*]{} and the [*American Journal of Mathematics*]{}. Eleven years later, Percy Heawood found an error in the proof which nobody had spotted, despite careful checking.\n\n[^2]: Our Italics.\n\n[^3]: The Scottish judicial system which admits three forms of verdicts, guilty, not\u2013guilty and not\u2013proven, comes closer to the picture described by GIT.\n\n[^4]: Landau\u2019s son-in-law.\n\n[^5]: Still, there is a controversy in the mathematical community on whether these articles provide a complete and correct proof. For a recent account see Aschbacher [@ma].\n\n[^6]: Perhaps \u201cgeneral\" should be replaced here by \u201cquasi\u2013general\".\n\n[^7]: Probabilists would argue that very long proofs can at best be viewed as only probably correct, cf. [@demillo], p. 273. In view of [@cj], the longer the statement, the lesser its chance is to be proved.\n\n[^8]: Harris [@harris] believes that no more than 5% of mathematicians have made the effort to work through the proof. Does this have anything to do with what George Hardy has noted in his famous [*Apology*]{}: \u201cAll physicists and a good many quite respectable mathematicians are contemptuous about proof.\"?\n\n[^9]: According to Wiles, \u201cIt was an error in a crucial part of the argument, but it was something so subtle that I\u2019d missed it completely until that point. The error is so abstract that it can\u2019t really be described in simple terms. Even explaining it to a mathematician would require the mathematician to spend two or three months studying that part of the manuscript in great detail.\"\n\n[^10]: More precisely, a traditional proof of length $l$ is checked in time a constant power of $l$ while a holographic proof requires only constant power of $\\log_{2}l$. To appreciate the difference, the binary logarithm of the number of atoms in the known Universe is smaller than 300.\n\n[^11]: Metaphorically described as \u201crelying on proof by \u2018Von Neumann says\u2019\".\n"
}