{
    "criteria": [
        {
            "criterion": "AllDocuments",
            "passed": true,
            "reason": "All documents pass."
        },
        {
            "criterion": "Domain",
            "passed": false,
            "reason": "Document source unavailable."
        },
        {
            "criterion": "QuestionAnswerStrings",
            "passed": true,
            "reason": "Text contains Q&A."
        },
        {
            "criterion": "QuestionAnswerStringsV2",
            "passed": true,
            "reason": "Text contains ['Q:', 'A:', 'Q&A', 'A:', 'A:']."
        },
        {
            "criterion": "FullyStructured",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ListPrefix",
            "passed": false,
            "reason": "Found 0 list prefixes. (Min: 5)"
        },
        {
            "criterion": "ListPrefixV2",
            "passed": false,
            "reason": "Does not meet criterion."
        }
    ],
    "doc_id": "5226",
    "text": "Q:\n\nI estimate 10% of the links posted here are dead. How do we deal with them?\n\nTL;DR: Approximately 10% of 1.5M randomly selected unique links in the March 2015 data dump are unavailable. To be more precise, that is approximately 150K dead links.\n\nMotivation\nI've been running into more and more links that are dead on Stack Overflow and it's bothering me. In some cases, I've spent the time hunting down a replacement, in others I've notified the owner of the post that a link is dead, and (more shamefully), in others I've simply ignored it and left just a down vote. Obviously that's not good.\nBefore making sweeping generalizations that there are dead links everywhere, though, I wanted to make sure I wasn't just finding bad posts because I was wandering through the review queues. Utilizing the March 2015 data dump, I randomly selected about 25% of the posts (both questions and answers) and then parsed out the links. This works out to 5.6M posts out of 21.7M total.\nOf these 5.6M posts, 2.3M contained links and 1.5M of these were unique links. I sent each unique URL a HEAD request, with a user agent mimicking Firefox1. I then retested everything that didn't return a successful response a week later. Finally, anything that failed from that batch, I resent a final test a week later. If a site was down in all three tests, I considered it down for this test.\n\nResults2\nBy status code\nGood news/Bad News: A majority of the links returned a valid response, but there are still roughly 10% that failed.\n\n(This image is showing the top status codes returned)\nThe three largest slices of the pie are the status 200s (site working!), status 404 (page not found, but server responded saying the page isn't found) and Connection Errors. Connection errors are sites that had no proper server response. The request to access the page timed out. I was generous in the time out and allowed a request to live for 20 seconds before failing a link with this status. The 4xx and 5xx errors are status codes that fall in the 400 and 500 range of HTTP responses. These are client and server error ranges, thus counted as a failure. 2xx errors (of which was are in the low triple) are pages that responded with a success message in the 200 range, but it wasn't a 200 code. Finally, there were just over a hundred sites that hit a redirect loop that didn't seem to end. These are the 3xx errors. I failed a site with this range if it redirected more than 30 times. There are a negligible number of sites that returned status codes in the 600 and 700 range4\nBy most common\nThere are, as expected, many URLs that failed that appeared frequently in the sample set. Below is a list of the top 503 URLs that are in posts most often, but failed three times over the course of three weeks.\nhttp://docs.jquery.com/Plugins/validation\nhttp://www.eclipse.org/eclipselink/moxy.php\nhttp://jackson.codehaus.org/\nhttp://xstream.codehaus.org/\nhttp://opencv.willowgarage.com/wiki/\nhttp://developer.android.com/resources/articles/painless-threading.html\nhttp://valums.com/ajax-upload/\nhttp://sqlite.phxsoftware.com/\nhttp://qt.nokia.com/\nhttp://www.oracle.com/technetwork/java/codeconv-138413.html\nhttp://download.java.net/jdk8/docs/api/java/time/package-summary.html\nhttp://docs.oracle.com/javase/1.4.2/docs/api/java/text/SimpleDateFormat.html\nhttp://watin.sourceforge.net/\nhttp://leandrovieira.com/projects/jquery/lightbox/\nhttps://graph.facebook.com/\nhttps://ccrma.stanford.edu/courses/422/projects/WaveFormat/\nhttp://www.postsharp.org/\nhttp://www.erichynds.com/jquery/jquery-ui-multiselect-widget/\nhttp://ha.ckers.org/xss.html\nhttp://jetty.codehaus.org/jetty/\nhttp://cpp-next.com/archive/2009/08/want-speed-pass-by-value/\nhttp://codespeak.net/lxml/\nhttp://www.hpl.hp.com/personal/Hans_Boehm/gc/\nhttp://jquery.com/demo/thickbox/\nhttp://book.git-scm.com/5_submodules.html\nhttp://monotouch.net/\nhttp://developer.android.com/resources/articles/timed-ui-updates.html\nhttp://jquery.bassistance.de/validate/demo/\nhttp://codeigniter.com/user_guide/database/active_record.html\nhttp://www.phantomjs.org/\nhttp://watin.org/\nhttp://www.db4o.com/\nhttp://qt.nokia.com/products/\nhttp://referencesource.microsoft.com/netframework.aspx\nhttps://github.com/facebook/php-sdk/\nhttp://java.decompiler.free.fr/\nhttp://pivotal.github.com/jasmine/\nhttp://api.jquery.com/category/plugins/templates/\nhttp://code.google.com/closure/library\nhttp://www.w3schools.com/tags/ref_entities.asp\nhttp://xstream.codehaus.org/tutorial.html\nhttps://github.com/facebook/php-sdk\nhttp://download.java.net/maven/1/jstl/jars/jstl-1.2.jar\nhttps://developers.facebook.com/docs/offline-access-deprecation/\nhttp://www.parashift.com/c++-faq-lite/pointers-to-members.html\nhttps://developers.facebook.com/docs/mobile/ios/build/\nhttp://downloads.php.net/pierre/\nhttp://fluentnhibernate.org/\nhttp://net.tutsplus.com/tutorials/javascript-ajax/5-ways-to-make-ajax-calls-with-jquery/\nhttp://dev.iceburg.net/jquery/jqModal/\n\nBy post score\nCount of posts by score (top 10)  (Covers 94% of all broken links):\n| Score | Percentage of Total Broken |\n|-------|----------------------------|\n| 0     | 36.4087%                   |\n| 1     | 25.1674%                   |\n| 2     | 13.4089%                   |\n| 3     | 7.2806%                    | \n| 4     | 4.2971%                    |\n| 5     | 2.7065%                    |\n| 6     | 1.8068%                    |\n| 7     | 1.2854%                    |\n| -1    | 1.1935%                    |\n| 8     | 0.9415%                    |\n\nBy number of views\nNote, this is number of views at the time the data dump was created, not as of today\nCount of posts by number of views (top 10):\n| Views        | Total Views |\n|--------------|-------------|\n| (0, 200]     | 24.4709%    |\n| (200, 400]   | 14.2186%    |\n| (400, 600]   | 9.5045%     |\n| (600, 800]   | 6.9793%     | \n| (800, 1000]  | 5.2574%     |\n| (1000, 1200] | 4.1864%     |\n| (1200, 1400] | 3.3699%     |\n| (1400, 1600] | 2.7766%     |\n| (1600, 1800] | 2.3477%     |\n| (1800, 2000] | 1.9550%     |\n\nBy days since post created\nNote: This is number of days since creation at the time the data dump was created, not from today\nCount of posts by days since creation (top 10) (Covers 64% of broken links):\n| Days since Creation | Percentage of Total Broken |\n|---------------------|----------------------------|\n| (1110, 1140]        | 7.2938%                    |\n| (1140, 1170]        | 6.7648%                    |\n| (1470, 1500]        | 6.6579%                    |\n| (1080, 1110]        | 6.6535%                    | \n| (750, 780]          | 6.5535%                    |\n| (720, 750]          | 6.5516%                    |\n| (1500, 1530]        | 6.3978%                    |\n| (390, 420]          | 5.8508%                    |\n| (360, 390]          | 5.8258%                    |\n| (780, 810]          | 5.5175%                    |\n\nBy Ratio of Views:Days\nRatio Views:Days (top 20) (Covers 90% of broken links):\n| Views:Days Ratio | Percentage of Total Broken |\n|------------------|-------------|\n| (0, 0.25]        | 27.2369%    |\n| (0.25, 0.5]      | 18.8496%    |\n| (0.5, 0.75]      | 11.4321%    |\n| (0.75, 1]        | 7.2481%     | \n| (1, 1.25]        | 5.1668%     |\n| (1.25, 1.5]      | 3.7907%     |\n| (1.5, 1.75]      | 2.9310%     |\n| (1.75, 2]        | 2.4033%     |\n| (2, 2.25]        | 1.9788%     |\n| (2.25, 2.5]      | 1.6850%     |\n| (2.5, 2.75]      | 1.4080%     |\n| (2.75, 3]        | 1.1879%     |\n| (3, 3.25]        | 1.0654%     |\n| (3.25, 3.5]      | 0.9391%     |\n| (3.5, 3.75]      | 0.8334%     |\n| (3.75, 4]        | 0.7165%     |\n| (4, 4.25]        | 0.6634%     |\n| (4.25, 4.5]      | 0.5789%     |\n| (4.5, 4.75]      | 0.5508%     |\n| (4.75, 5]        | 0.4833%     |\n\nDiscussion\nWhat can we do with all of this? How do we, as a community, solve the issue of 10% of our outbound links pointing to places on the internet that no longer exist? Assuming that my sample was indicative of the entire data dump, there are close to 600K (150K broken unique links x 4, because I took 1/4 of the data dump as a sample) broken links posted in questions and answers on Stack Overflow. I assume a large number of links posted in comments would be broken as well, but that's an activity for another month.\nWe encourage posters to provide snippets from their links just in case a link dies. That definitely helps, but the resources behind the links and the (presumably) expanded explanation behind the links are still gone. How can we properly deal with this? \nIt looks like there have been a few previous discussions:\n\nUtilize the Wayback API to automatically fix broken links. Development appeared to stall on this due to the large number of edits the Community user would be making. This would also hide posts that depended on said link from being surfaced for the community to fix it.\nLink review queue. It was in alpha, but disappeared in early 2014. \nBadge proposal for fixing broken links\n\nFootnotes\n\nThis is how it ultimately played out. Originally I sent HEAD requests, in an effort to save bandwidth. This turned out to waste a whole bunch of time because there are a whole bunch of sites around the internet that return a 405 Method Not Allowed when sending a HEAD request. The next step was to sent GET requests, but utilize the default Python requests user-agent. A lot of sites were returning 401 or 404 responses to this user agent. \nLinks to Stack Exchange sites were not counted in the above results. The failures seen are almost 100% due to a question/answer/comment being deleted. The process ran as an anonymous user, thus didn't have any reputation and was served a 404. A user with appropriate permissions can still visit the link. I verified a number of 404'd links to Stack Overflow posts and this was the case.\nThe 4th most common failure was to localhost. The 16th and 17th most common were localhost on ports other than 80. I removed these from the result table with the knowledge that these shouldn't be accessible from the internet.\nThere where 7 total URLs that returned status codes in the 600 and 700 range. One such site was code.org with a status code of 752. Sadly, this is not even defined the joke RFC. \n\nA:\n\nI really think that, at least at this point, there isn't a problem. To the extent it is a problem, it is difficult to fix.\nStack Overflow is meant to be a Q&A site, not a repository of links. Encountering a dead link is an annoyance, but it doesn't instantly invalidate the answer, and often barely has any impact at all. This site has a policy of encouraging answers consisting of more than links exactly for this reason: so even if the link dies, the answer still survives and remains meaningful. If an answer consists of just a link, then this is the problem, not the dead links. I'd go as far as to say the question hasn't really been answered.\nMany of the links are dead simply because the resource they pointed to has been moved to a slightly different location that any user could discover with a tiny bit of effort (for example, typing the name into Google). Take the link http://www.eclipse.org/eclipselink/moxy.php for example. Even though I don't trust casual users to actually fix the link, I do trust them not to be total idiots and just google eclipse moxy and follow one of the top three results to the new location.\nIn other cases, it's simply impossible to fix a link at all, except by a person who is familiar with the subject. This is a more significant problem, but unfortunately not one that is fixable automatically. \nFor example, take the link http://www.db4o.com, to the object database db4o. db4o hasn't existed for a while now and is no longer supported by the developer. You might be able to find the source code or the binaries, but I would not fix the link to point to them, because I would not recommend it to anyone (since it's dead). The problem is not really that the link is dead, but rather that the product has ceased to exist, and the answer that recommends it is no longer valid. It can only be fixed by posting a new answer, voting, and comments. These things might already exist on the questions you looked at.\nAlso, a major problem with any automatic scheme to fix dead links is the potential for error. A link that points to something else, or to something that is no longer a valid answer, is a lot worse than a dead link, in the same way that misinformation is a lot worse than a lack of information. It really might confuse users, or have them using outdated software.\nIf the bulk of the dead links continues to grow, and if popular answers get hit as well, I really would like to do something about it, largely because it makes the site looks dated and unprofessional. As it stands, an attempt at fixing it would be nice, but not something I think is important. Personally, I have encountered very few dead links as a casual user.\n\nA:\n\nThe world wide web's sole purpose was to link relevant documents together. With no (working) links, there's no web.\nSo I think every effort that can be undertaken to fix broken links, is a good effort.\nWe shouldn't rely on users fixing their own post. We have way more inactive than active users. \nPerhaps there could be something like a \"broken link queue\", where users can report a broken link (A) and suggest a replacement (B). Then when agreed upon by reviewers and/or moderators, the system (Community user) could replace all instances in all posts of link A with link B.\nOf course this is very spam-sensitive, so the actual implementation details needs to be worked out pretty tight.\n\nA:\n\nI propose another hybrid of the previous broken link queue (as was mentioned above in comments and other answers) and an automated process to fix broken links with an archived version (which has also been suggested). \nThe broken link queue should focus on editing and fixing the links in a post (as opposed to closing it). It'd be similar to the suggested edits queue, but with the focus intended to correct links not spelling and grammar. This could be done by only allowing a user to edit the links.\nOne possibility, I envision is presenting the user with the links in the post and a status on whether or not the link is available. If it's not available, give the user a way to change that specific link. Utilizing this post, I have a quick mock up of what I propose such a review task looks like:\n\nAll the links that appear in the post are on the right hand side of the screen. The links that are accessible have a green check mark. The ones that are broken (and the reason for being in this queue) have a red X. When a user elects to fix a post, they are presented with a modal showing only the broken URLs.\n\nWith this queue, though, I think an automated process would be helpful as well. The idea is that this would operate similarly to the Low Quality queue, where the system can automatically add a post to the queue if certain criteria are met or a user can flag a post as having broken links. I've based my idea on what Tim Post outlined in the comments to a previous post.\n\nAutomated process performs a \"Today in History\" type check. This keeps the fixes limited to a small subset of posts per day. It also focuses on older posts, which were more likely to have a broken link than something posted recently. Example: On July 31, 2015, the only posts being checked for bad links would be anything posted on July 31 in any year 2008 through current year - 1. \nUtilizing the Wayback Machine API, or similar service, the system attempts to change broken links into an archived version of the URL. This archived version should probably be from \"close\" to the time the post was originally made. If the automated process isn't able to find an archived version of the link, the post should be tossed into the Broken Link queue\nWhen the Community edits a post to fix a link, a new Post History event is utilized to show that a link was changed. This would allow anyone looking at revision history to easily see that a specific change was only to fix links. \nActions performed in the previous bullets are exposed to 10K users in the moderator tools. Much like recent close/delete posts show up, these do as well. This allows higher rep users to spot check (if they so desire). I think this portion is important when the automated process fixes a link. For community edits in the queue, the history tab in /review seems sufficient. \nIf a post consists of a large percentage of a link (or links) and these links were changed by Community, the post should have further action taken on it in some queue.\nExample:\n\nA post where X+% of the text is hyperlinks is very dependant on the links being active. If one or more of the links are broken, the post may no longer be relevant (or may be a link only post). One example I found while doing this was this answer.\n\nI don't think that this type of edit from the Community user should bump a post to the front page. Edits done in the broken link queue, though, should bump the post just like a suggested edit does today. By preventing the automated Community posts from being bumped, we prevent the the front page from being flooded, daily, with old posts and these edits. I think that the exposure in the 10K tools and the broken link queue will provide the visibility needed to check the process is working correctly. \nQueue Flow:\n\nAutomated process flow:\n\nThe automated link checking will likely run into several of the problems I did. Mainly:\n\nSites modify the HEAD request to send a 404 instead of a 405. My solution to this was to issue GET requests for everything.\nSites don't like certain user agents. My solution to this was to mimic the Firefox user agent. To be a good internet citizen, Stack Exchange probably shouldn't go that far, but providing a unique user agent that is easily identifiable as \"StackExchangeBot\" (think \"GoogleBot\"), should be helpful in identifying where traffic is coming from. \nSites that are down one week and up another. I solved this by spreading my tests over a period of 3 weeks. With the queue and automatic linking to an archived version of the site, this may not be necessary. However, immediately converting a link to an archived copy should be discussed by the community. Do we convert the broken link immediately? Or do we try again in X days. If it's still down then convert it? It was suggested in another answer that we first offer the poster the chance to make changes before an automatic process takes place.\nThe need to throttle requests so that you don't flood a site with requests. I solved this by only querying unique URLs. This still issues a lot of requests to certain, popular, domains. This could be solved by staggering the checks over a period of minutes/hours versus spewing 100s - 1000s of GET requests at midnight daily. \n\nWith the broken link queue, I feel the first two would be acceptable. Much like posts in the Low Quality queue appear because of a heuristic, despite not being low quality, links will be the same way. The system will flag them as broken and the queue will determine if that is true (if an archived version of the site can't be found by the automated process). The bullet about throttling requests is an implementation detail that I'm sure the developers would be able to figure out.\n\n"
}