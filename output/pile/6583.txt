{
    "criteria": [
        {
            "criterion": "AllDocuments",
            "passed": true,
            "reason": "All documents pass."
        },
        {
            "criterion": "QuestionAnswerStringsV2",
            "passed": true,
            "reason": "Text contains ['Q:', 'A:']."
        },
        {
            "criterion": "FullyStructured",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ListPrefixV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesMinimalEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesSynonymsEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesDiverseEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        }
    ],
    "doc_id": "6583",
    "text": "Q:\n\nShrinking a large transaction log on a full drive\n\nSomeone fired off an update statement as part of some maintenance which did a cross join update on two tables with 200,000 records in each.  That's 40 trillion statements, which would explain part of how the log grew to 200GB.  I also did not have the log file capped, which is another problem I will be taking care of server wide - where we have almost 200 databases residing.\nThe 'solution' I used was to backup the database, backup the log with truncate_only, and then backup the database again.  I then shrunk the log file and set a cap on the log.  \nSeeing as there were other databases using the log drive, I was in a bit of a rush to clean it out.  I might have been able to back the log file up to our backup drive, hoping that no other databases needed to grow their log file.  \nPaul Randal from http://technet.microsoft.com/en-us/magazine/2009.02.logging.aspx\n\nUnder no circumstances should you\n  delete the transaction log, try to\n  rebuild it using undocumented\n  commands, or simply truncate it using\n  the NO_LOG or TRUNCATE_ONLY options of\n  BACKUP LOG (which have been removed in\n  SQL Server 2008). These options will\n  either cause transactional\n  inconsistency (and more than likely\n  corruption) or remove the possibility\n  of being able to properly recover the\n  database.\n\nWere there any other options I'm not aware of?  \n\nA:\n\nYou could've put the database into Simple recovery model (then use the CHECKPOINT command to make sure that the log is as truncated as possible) and then back into the Full recovery model. Then take a full database backup so that the system realises that it's in Full.\nThen you can shrink the log at your convenience. Shrinking a log isn't nearly as awful as shrinking the database, which you should almost never do.\nYou haven't deleted the log, you've just truncated it - which kills your option to restore to a point in time. So do a full backup of your database as soon as possible.\nAlso bear in mind that if your log can't grow, your database will stop. So leaving autogrow on isn't a bad option at all... but perhaps set it up to send you an alert when it's filling up.\n\n"
}