{
    "criteria": [
        {
            "criterion": "AllDocuments",
            "passed": true,
            "reason": "All documents pass."
        },
        {
            "criterion": "Domain",
            "passed": false,
            "reason": "Document source unavailable."
        },
        {
            "criterion": "QuestionAnswerStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "QuestionAnswerStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "FullyStructured",
            "passed": true,
            "reason": "Text contains Markdown."
        },
        {
            "criterion": "ExamStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ListPrefix",
            "passed": false,
            "reason": "Found 0 list prefixes. (Min: 5)"
        },
        {
            "criterion": "ListPrefixV2",
            "passed": true,
            "reason": "Text contains ['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '1', '2', '5', '5', '-', '-', '3', '-']."
        },
        {
            "criterion": "ExamplesMinimalEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesSynonymsEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesDiverseEmbed",
            "passed": false,
            "reason": "Does not meet criterion."
        }
    ],
    "doc_id": "1501",
    "text": "\nWhich is better on Android: divide by 2 or shift by 1? - zdw\nhttps://jakewharton.com/which-is-better-on-android-divide-by-two-or-shift-by-one/\n======\nanyfoo\nThorough work. Going back to the premise, I want to offer an alternative\nviewpoint by asking whether, in the case of binary trees implemented by\narrays, \u201cinteger division by 2\u201d of the array index is necessarily the best\ninterpretation of what you are trying to do here?\n\nInstead, you can also see the array index as a bit string, where every bit\ntells you which path to go down, left or right. In that case, \u201cshifting right\nby one bit\u201d moves you up to the parent. \u201cShifting left\u201d moves you down to the\nleft child. Flipping one bit flips you over to the other child. Bit wise\noperations indeed seem more natural with that interpretation.\n\nA lot of \u201cpower of 2\u201d multiplication/division has similar interpretations. For\nexample, when walking page tables, you could see walking down the levels as\n\u201cdividing by the size of the granule\u201d, or simply as \u201cshifting right to select\nthe index on that level\u201d.\n\nNo contest on anything where the power of 2 is coincidence, i.e. for non\n\u201ccomputery\u201d things where there is no such underlying structure.\n\n~~~\nyiyus\nI had a slightly similar experience at work. We deal a lot with angles and\nbinary angles are often the most efficient representation. Many colleagues\nfind it annoying because they insist on converting every binary angle to\nradian or degrees, but if you actually interpret the bits as successive\ndivisions of a circumference, I actually find the binary representation way\nmore intuitive than a floating point number.\n\n~~~\nslavik81\nThat method is exactly equivalent to using revolutions as the unit with a\nfixed-point numeric representation. Maybe your skeptical colleagues would find\nthat perspective more palatable?\n\n~~~\nyiyus\nThat's some good advice. Unfortunately, most of the people I work with do not\nhave a computer science background (they are materials scientists and\nmechanical engineers), so they are not familiarized with fixed-point\narithmetic neither.\n\n------\ntwoodfin\nI appreciate the thorough exploration and resulting detail in this article.\nStill, I find it a little sad that either the state of our toolchains or the\nperception thereof prevent it from being self-evident that basic strength\nreduction will always happen, and developers need not worry about the cost of\nexpressing simple arithmetic operations in the clearest way.\n\n~~~\nbsder\nI would go further in that many programmers don't understand the difference\nbetween logical and arithmetic operations and why they exist.\n\nI have had to puzzle over far too much code doing\nadds/subtracts/multiplies/divides instead of and/or/xor/shift _FAR_ too often.\n\nI blame Java not having an unsigned type. There are apparently some weird\ntricks you can do with arithmetic in Java that operate on things like an\nunsigned type without having to go up to the next higher integer width.\n\n~~~\nvbezhenar\nWhat exactly do you miss in Java? It's possible to treat int as unsigned for\nall the necessary operations.\n\n~~~\nblibble\nplus Java's had various unsigned integer operations on Integer for years\n\n------\nlgessler\nQuestion from a perf noob: it seems like this in principle only shows that\nthere's no difference for a Pixel 3 because other Android machines could have\nprocessors that have/lack an optimization for shift or divide. Couldn't a\ndifferent Android phone have different performance characteristics?\n\n~~~\nlgg\nIt is not that the processor that contains an optimization per se. The thing\nto understand is that shift is fundamentally a simpler operation than\nmultiply... a shift can be implemented with a few transistors per bit and done\nin a single cycle trivially. A multiply unit takes tons of transistors, and\noften takes multiple cycles (this is a trade off you make when you design a\nmultiply unit, you can save space by making it work on smaller integers and\nreusing it multiple times over several cycles to do multiplies of larger\nintegers, just you like you iteratively multiply digits one you do it on paper\nby hand). Even on processors that have single cycle multipliers it takes a lot\nmore power to do a multiply than a shift because of all the extra hardware you\nneed to engage.\n\nSince shifts are fundamentally simpler than multiplies it always makes sense\nto do this transform. This is one of a number of transforms that are generally\ncalled \"strength reductions\"\n<[https://en.wikipedia.org/wiki/Strength_reduction>](https://en.wikipedia.org/wiki/Strength_reduction>),\nconverting for a more general expensive operation into a more constrained\ncheaper operation. In this case it is the equivalent to knowing that if you\nwant to multiply a number by 10 you can just add a 0 at the beginning instead\nof having to write all the work by hand.\n\nThe only reason not to do this transform would be if you had a CPU that\nliterally does not have a shift operation, but I cannot think of any such\npart. Even if you did have such a part, the odds are you could emulate a shift\nusing other other instructions and still outperform the multiply.\n\nThis has been a standard optimization for half a century. The original C\ncompiler for the PDP-11 did these transforms even when you turned off\noptimizations\n<[http://c-faq.com/misc/shifts.html>](http://c-faq.com/misc/shifts.html>).\n\n~~~\ncogman10\n> This has been a standard optimization for half a century. The original C\n> compiler for the PDP-11 did these transforms even when you turned off\n> optimizations\n\nConsider this, a common easily applied optimization that compilers have been\ndoing for half a century MAY have made it's way into modern CPUs.\n\nTransistors aren't nearly as power hungry as you paint them and CPUs aren't\nnearly as bad at optimization. There is no reason to switch a multiply or\ndivide for a shift. The ONLY reason to make that switch is if you are dealing\nwith the simplest of processors (Such as a microwave processors). If you are\nusing anything developed in the last 10 years that consumes more than 1W of\npower, chances are really high that the you aren't saving any power by using\nshifts instead of multiples. It is the sort of micro-optimization that\nfundamentally misunderstands how modern CPUs actually work and over estimates\nhow much power or space transistors actually need.\n\n~~~\nreitzensteinm\nSince it's what I'm typing this on, let's look at Skylake.\n\nMultiply: Latency 3, Throughput 1 Shift: Latency 1, Throughput 2\n\nIf the ALU contained an early out or fast path for simpler multiplies, the\nlatency would read 1-3. You can verify this by looking at div, which does\nearly out and has a latency of 35-88.\n\nAny compiler that doesn't swap a multiply to a shift when it can is negligent.\n\n[https://www.agner.org/optimize/instruction_tables.pdf](https://www.agner.org/optimize/instruction_tables.pdf)\n\n------\nnecovek\nI am really struggling to understand the benchmark output quoted.\n\nCan anyone elaborate what does benchmark=3/4 ns mean, and the count? Is the\nset-up part of the benchmark (test structure suggest not, but just to make\nsure)?\n\nThe only way I can read it is that 4000 divisions takes 4ns, and 4000 shift-\nrights takes 3ns, but that only has 1 digit of precision, which makes it\nunusable for comparison, but even then suggests a 25%/33% difference, which is\nnot insignificant.\n\nAlso, the VM seems to optimise multiply out, so it must be doing it for a\nreason.\n\n~~~\nthechao\nIt's definitely not 4000 divisions per 4ns \u2014 that'd imply a terahertz\ncomputer. I think it's saying that the amortized cost of 4000 divisions is 4ns\nper division. Small integer division is an \"easy win\" for a dedicated HW path,\nso it doesn't surprise me that it's only a little slower than a shift-right.\nVariable length right shifts aren't that fast.\n\n~~~\nbonzini\nIt's not small integer division that is being benchmarked, the JIT compiler\nhas reduced it to an addition, a conditional move and a right shift. This\nsequence is then benchmarked against the right shift.\n\n~~~\nsaagarjha\nART is an AOT complier, is it not?\n\n~~~\nmonocasa\nIt's both AOT and JIT.\n\n[https://source.android.com/devices/tech/dalvik/jit-\ncompiler](https://source.android.com/devices/tech/dalvik/jit-compiler)\n\n~~~\npjmlp\nWith a PGO cache updated across execution runs and devices (since Android 10\nPGO data is shared across the Play Store).\n\n~~~\nignoramous\nFor anyone like me wondering what a PGO is:\n[https://source.android.com/devices/tech/perf/pgo](https://source.android.com/devices/tech/perf/pgo)\n\n------\nMithrilTuxedo\nNow I'm wondering if there's a power usage difference between the two.\n\nIt stands to reason that if two operations take the same amount of time, but\none requires more transistors to compute, power usage should diverge.\n\n~~~\nwmf\nFor scalar instructions, most of the energy is consumed in scheduling not\nexecuting them. This means that cycles is a good proxy for total energy.\n\n------\nsaurik\nEveryone is talking about this microoptimization on the math of the access but\nall I can think about is how the data structure you are building using this is\nprobably memory bandwidth limited and powers-of-two storage is almost\ndefinitely going to cause some kind of cache line aliasing, so maybe you\nshould try something non-obvious like \"division by 3\" (after doing three key\ncomparisons instead of one) and see if it makes your algorithm much much\nfaster than messing around with a division by 2; there was even some good\nanalysis of this effect a while back I can reference.\n\n[https://pvk.ca/Blog/2012/07/30/binary-search-is-a-\npathologic...](https://pvk.ca/Blog/2012/07/30/binary-search-is-a-pathological-\ncase-for-caches/)\n\n------\nsambe\nAm I mis-reading this? The article keeps claiming there is no difference but\nthe way I read it the compiler(s) are transforming mul/div to shifts. i.e. it\nvery likely _is_ faster on the hardware but it won't matter for this\nparticular toolchain because of the conversion.\n\n~~~\nchrisseaton\n> Am I mis-reading this? The article keeps claiming there is no difference but\n> the way I read it the compiler(s) are transforming mul/div to shifts. i.e.\n> it very likely is faster on the hardware but it won't matter for this\n> particular toolchain because of the conversion.\n\nThere is no difference... because of the conversion.\n\n~~~\nsambe\nRight: for now, in certain situations, on the tested toolchain.\n\nEven ignoring those caveats, several commentators seem to have got the\nimpression that this applies to the CPU.\n\n~~~\nuluyol\nThese types of transformations are simple to detect, well known, and applied\nby ~every compiler.\n\nUnless you have evidence otherwise (ASM differences or benchmarks), there is\nno use in manually transforming your arithmetic into something more complex\nbut faster. The compiler will do it for you.\n\n~~~\nsambe\nI think that's a point which is bordering on religious - many people would\ndebate trusting the compiler, especially over time and more complex\nsituations.\n\nI'd certainly tend to agree with you in general but more for the reason that\nthe compiler can abstract over hardware changes across time. I'd take that\nbenefit over the risk of the optimisation not being applied for most code I\nwrite - non-optimisations would be considered bugs and probably/eventually\nfixed.\n\nI'd strongly disagree the code is more complex (in this case).\n\n~~~\nmerlincorey\nIt especially seems religious to me because it's saying that somehow \"/ 2\" is\nsimpler than \">> 1\" because it has one less character for the symbol, and\nbecause division is a more commonly known operator to most people than bitwise\nshifting.\n\nIt seems to me that they are equally simple if we assume that programmers\ndealing with low level or performance intensive code know what a bitwise shift\nis and ignore the extra character, then they are literally equivalently\ncomplicated expressions with 1 symbol and 1 value applied to the symbol.\n\n~~~\nkadoban\nCode does not happen in a vacuum. Which is more understandable/simple depends\non the domain of the code in question. Usually that's going to be the multiply\nor the divide.\n\n~~~\nmerlincorey\nRight, but my statement was that the domain would be low level or performance\nintensive code -- do you disagree that in that domain they are equally simple?\n\n------\nremcob\nAnother fast way to double a number is to add it to itself.\n\n~~~\nfyp\nIsn't that the wrong direction for the optimization? I would assume you would\nwant to compile adding two numbers into shifting by one, not the other way\naround.\n\n(I know nothing about hardware, it just intuitively seems like moving a bunch\nof bits over by 1 should be faster than dealing with xor and carries)\n\n~~~\njcranmer\nIn hardware terms, adders are simpler than shifters. You can usually do both\nin a single cycle, but it's going to be lower power to do the add instead of\nthe shift.\n\nTo put this in more concrete terms: an N-bit adder involves N 1-bit stages to\nadd each bit, and then a 1-bit carry network on top of that, which has N\nstages in it. So overall, it's O(N) in terms of hardware. An N-bit shift unit\nis going to use lg N N-bit muxes--or O(N lg N) in terms of hardware. Total\ngate delay in both cases is O(lg N), but adders have O(N) hardware (and thus\nenergy consumption) while shifters have O(N lg N).\n\nA secondary consequence of being larger area is that a superscalar\narchitecture may choose to have one execution unit that has an adder and a\nshifter and a second that only has the adder. So an addition may schedule\nbetter than a shift, since there are more things it can execute on.\n\n~~~\nTuna-Fish\n> To put this in more concrete terms: an N-bit adder involves N 1-bit stages\n> to add each bit, and then a 1-bit carry network on top of that, which has N\n> stages in it. So overall, it's O(N) in terms of hardware.\n\nO(N) adders cannot meet the latency demands of modern high-frequency CPUs. The\nactual complexity of adders in real CPUs is usually O(N\u00b2).\n\n------\nAnimats\nHe did this in Java? In one case, running on an emulator? That's removed too\nfar from the hardware for this kind of benchmarking. Try in a hard-compiled\nlanguage.\n\nUsing shifts for constant divide has been a compiler code generator\noptimization for decades. This is not something programmers have needed to\nworry about in source code for a long time, unless targeting some small\nmicrocontroller that lacks fast divide hardware.\n\n~~~\npjmlp\nJava is a hard-compiled language on Android since version 5.0.\n\nART, which replaced Dalvik on 5.0 (available as experimental on 4.4), was AOT\nonly up to version 7.0.\n\nAs it was proven that Android users lack the patience of a C++ developer when\nupdating their apps, Google adopted another approach with version 7.0.\n\nA multi-tier compiler infrastructure, composed by a very fast interpreter hand\nwritten in Assembly for fast startup, a JIT compiler for the first\noptimization level, with gathering of PGO data, then the AOT compiler runs in\nthe background and when the device is idle gets that PGO data and just like a\nC++ compiler with PGO data, outputs a clean AOT compiled binary for the usual\nuser workflow.\n\nIn case of an update or changes in the workflow that trigger the execution of\ncode that wasn't AOT compiled, the process restarts.\n\nAs means to reduce this kind of de-optimizations, since Android 10 those PGO\nfiles are uploaded into the Play Store and when a user installs an application\nthat already has PGO data available, it is downloaded alongside the APK and\nthe AOT compiler can do its job right from the start.\n\nIn any case, he used _dex2aot_ which is the AOT compiler daemon on Android.\n\nMicrosoft has gone through similar process with .NET for UWP, with the main\ndifference that the AOT compiler lives on the Microsoft store and what gets\ndownloaded is already straight binary code.\n\nApparently mixing language capabilities with toolchains keeps being an issue.\n\n------\nrenewiltord\nHow come the division is almost the same as the shifting? Is the CPU\npipelining the operations between iterations of the loop or something? There\nis a direct data-dependency in those operations but not between iterations so\nperhaps that's it?\n\nAFAIK there's no fused add-shift op that could be used.\n\n------\nesnellman\nGiven the number of execution loops. The profiler applied an optimization.\nDon't expect this optimization during initial executions or seldom used code\nor cases where properties of the method do not allow it to be optimized; be it\non Android VMs or JVMs.\n\n------\npacman83\nApart from the fact that compilers are really good and generally will choose\nthe best option for you, it seems like it boils down to what processor is\nused. On Android aren't ARM processors the most common?\n\n~~~\npjmlp\nYes, followed by some Intel and MIPS survivors.\n\n------\njejones3141\nIf it's signed int, unless you know the value is positive you can't just shift\nright 1--if the result of the shift is negative, you have to add 1.\n\n~~~\nchrisseaton\nCan't you do an arithmetic shift-right? That takes the sign into account.\n\n~~~\njcranmer\nNo. An arithmetic shift right does a division that rounds down; an integer\ndivision operation instead truncates (rounds to zero). The easiest example is\n-1 / 2: -1 / 2 is 0, but -1 ashr 1 is -1.\n\nTo replace a signed division with ashr, you have to know that for all negative\ninputs, the value of the bits shifted out are all 0.\n\n------\nToo\nI thought this discussion was settled 30 years ago?\n\nWrite what you want to do, not how to do it. There is no difference.\n\n------\nnicetryguy\n...So the Dalvik VM sucks?\n\n~~~\npjmlp\nYes it sucks, that is why it was replaced by ART on Android 5.0.\n\n~~~\nnicetryguy\nAh, i haven't kept up. Anyway, a right bit shift should absolutely be quicker\nthan floating point or even integer division. If it isn't, that is an\nimplementation problem.\n\n~~~\npjmlp\nART as of Android 10, combines an hand written interpreter in Assembly, a JIT\ncompiler that generates PGO data as well, and an AOT PGO based optimizing\ncompiler capable of doing bounds check elision, de-virtualization, auto-\nvectorization, escape analysis and a couple of other traditional\noptimizations.\n\nThe PGO metadata files also get shared across devices via the Play Store as\nmeans to steer the AOT compiler into the optimal level of optimization across\nall users of the application.\n\nI assume that at the current level of ongoing ART optimizations, the team\nwould consider that a compiler bug.\n\n~~~\nnicetryguy\nAwesome info! Thanks!\n\nWhat would you recommend for an IDE? I used Eclipse some years ago. Is that\nstill common?\n\nI may want to experiment with some Android flavored Java again.\n\n~~~\npjmlp\nAndroid Studio is the official IDE, it is a merge of InteliJ with Clion and\nGoogle specific plugins.\n\n------\nmadhato\nIs there an advantage of using multiply by .5 versus divide by 2?\n\n~~~\nfox8091\nMultiply by 0.5 would be slower, as it's a floating point operation rather\nthan simple arithmetic.\n\n------\nnipxx\nif these kinds of optimizations make a difference for your applications write\nit in native code, dammitl\n\n~~~\npjmlp\nWhich pretty much means Assembly, given that Java and Koltin go through JVM\nand DEX bytecodes to machine code, and C and C++ on Android go through LLVM\nbitcode to machine code.\n\n------\ntemac\nTLDR: it is the same, like it should.\n\nWe are in 2020. Don't shift by 1 instead of /2 if you mean to /2.\n\n~~~\nOrgNet\nlol... if you care about optimizing, you should care about all possible\noptimizations... (even if most of today's platforms don't)\n\n~~~\ntemac\nYeah but you should also then understand what will yield real results, and >>1\ninstead of /2 has become useless to write manually a long time ago, whereas\ne.g. continuous memory is more important than ever. You will not optimize a\nlot by attempting micro techniques from 30 years ago.\n\nOther random example: in some edge cases, integer division replacement by a\nmultiplication _can_ still be relevant today (depends on if its a constant,\nthe compiler, and if nothing optimized also on the exact processor, though,\nbecause last models are already ultra-fast with the real integer divide\ninstructions), but I suspect in 15 years (maybe even 10) this will be\ncompletely irrelevant, at least for high perf targets.\n\n------\ndrivebycomment\nNews at 11 - someone learned strength reduction exists, which have been in use\nfor the past 4 decades.\n\n~~~\nanyfoo\nHad you read the article, you would have known that the question answered here\nwas whether, and where in the process, strength reduction was actually\napplied.\n\n------\nfefe23\nA few points.\n\n1\\. Looking at the Java bytecode is practically meaningless, you would have to\nlook at the machine code the JIT is creating.\n\n2\\. A division by 2 is identical to a shift right by 1 only if the integer is\nunsigned. Java integers are signed. Try this program in C to see for yourself:\n\nint foo(int a) { return a/2; } int bar(int a) { return a>>1; }\n\nRun gcc -S -O2 to get assembly output in text form.\n\nBasically, the problem is this:\n\n5/2 -> 2 (ok, rounds down)\n\n5>>1 -> 2 (ok, same)\n\n-5/2 -> -2 (ok, rounds down)\n\n-5>>1 -> -3 (oops!)\n\n3\\. The question is really about the JIT backend for the target platform,\nwhich means CPU platform, not OS platform. So \"on Android\" does not make much\nsense here, as Android exists for x86 and ARM and those JIT backends might\nbehave differently.\n\n~~~\nanyfoo\nAll things directly addressed by the article.\n\n------\ndevit\nHonestly, it's hard to read this article and not question the author's mental\nstate or intelligence.\n\nHe literally presents x86 and ARM assembly dumps where shift right generates\none instruction, and divide generates that same instruction plus several\nothers.\n\nThen, he feels the need to run an unnecessary benchmark (most likely screwing\nit up somehow) and concludes there is no difference!\n\nBut how can there possibly be no performance difference, in general, between\nthe CPU running an ALU instruction and running that same instruction plus\nseveral other ALU instructions?!?\n\nIt's almost unbelievable.\n\nAs to how he screwed up the benchmark, my guesses are that either he failed to\ninline the function (and the CPU is really bad), or failed to prevent the\noptimizer from optimizing the whole loop, or didn't run enough iterations, or\nperhaps he ran the benchmark on a different VM than what produced the assembly\n(or maybe somehow the CPU can extract instruction level parallelism in this\nmicrobenchmark, but obviously that doesn't generalize to arbitrary code).\n\n~~~\nbrianyu8\nWhile I think that there is merit to your argument, I feel like it could have\nbeen presented without questioning the author's intelligence or mental state.\n\n"
}