{
    "criteria": [
        {
            "criterion": "AllDocuments",
            "passed": true,
            "reason": "All documents pass."
        },
        {
            "criterion": "Domain",
            "passed": false,
            "reason": "Document source unavailable."
        },
        {
            "criterion": "QuestionAnswerStrings",
            "passed": true,
            "reason": "Text contains Q:."
        },
        {
            "criterion": "QuestionAnswerStringsV2",
            "passed": true,
            "reason": "Text contains ['Q:', 'A:']."
        },
        {
            "criterion": "FullyStructured",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamStringsV2",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ExamplesStrings",
            "passed": false,
            "reason": "Does not meet criterion."
        },
        {
            "criterion": "ListPrefix",
            "passed": false,
            "reason": "Found 0 list prefixes. (Min: 5)"
        },
        {
            "criterion": "ListPrefixV2",
            "passed": false,
            "reason": "Does not meet criterion."
        }
    ],
    "doc_id": "2892",
    "text": "Q:\n\nBatch processing in array using PHP\n\nI got thousands of data inside the array that was parsed from xml.. My concern is the processing time of my script, Does it affect the processing time of my script since I have a hundred thousand records to be inserted in the database? I there a way that I process the insertion of the data to the database in batch?\n\nA:\n\nThis is for SQL files - but you can follow it's model ( if not just use it ) - \nIt splits the file up into parts that you can specify, say 3000 lines and then inserts them on a timed interval < 1 second to 1 minute or more.\nThis way a large file is broken into smaller inserts etc.\nThis will help bypass editing the php server configuration and worrying about memory limits etc.  Such as script execution time and the like.\nNew Users can't insert links so Google Search \"sql big dump\" or if this works goto:\nwww [dot] ozerov [dot] de [ slash ] bigdump [ dot ] php\nSo you could even theoretically modify the above script to accept your array as the data source instead of the SQl file.  It would take some modification obviously.\nHope it helps.\n-R\n\n"
}